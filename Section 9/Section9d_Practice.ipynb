{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf1f2bc-f975-4b3b-ae16-8af3a8567828",
   "metadata": {},
   "source": [
    "# Section 9. Text Analysis Practice\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n",
    "\n",
    "The purpose of this notebook is to give you opportunities and challenge to practice applying the skills developed in the other notebooks. \n",
    "\n",
    "The content of this notebook is taken from UC Berkeley D-Lab's Python Text Analysis [course](https://github.com/dlab-berkeley/Python-Text-Analysis).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199519e3-896e-4085-a3a0-83ba137409bb",
   "metadata": {},
   "source": [
    "## Challenge 1: Extracting and Counting Substrings\n",
    "\n",
    "Adapting the code to extract twitter handle mentions from the twitter data, write code to extract all hashtags. Keep the results as lists. Then, using this information, calculate the count of mentions for each hashtag across all tweets. Plot a bar chart of mentions for the 10 most common hashtags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396e8189-65e7-4a36-84c6-28fd657847b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the separator to be comma\n",
    "tweets = pd.read_csv('Data/airline_tweets.csv', sep=',')\n",
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb457d-4b53-4e84-8185-ac6c3481cc13",
   "metadata": {},
   "source": [
    "## Challenge 2: Preprocessing with Multiple Steps\n",
    "\n",
    "So far we've learned a few preprocessing operations, let's put them together in a function! This function would be a handy one to refer to if you happen to work with some messy English text data, and you want to preprocess it with a single function. \n",
    "\n",
    "The below code reads in example text data for this challenge. Write a function to:\n",
    "- Lowercase the text\n",
    "- Remove punctuation marks\n",
    "- Remove extra whitespace characters\n",
    "\n",
    "Feel free to recycle the code we used in the other notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f36bd0b-579d-44f8-9fa2-d5a9dc5b250e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "challenge1_path = 'Data/example1.txt'\n",
    "\n",
    "with open(challenge1_path, 'r') as file:\n",
    "    challenge1 = file.read()\n",
    "    \n",
    "print(challenge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e20c3-2030-4e2f-aea4-40c29731135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    '''Remove punctuation marks in input text'''\n",
    "    \n",
    "    # Select characters not in puncutaion\n",
    "    no_punct = []\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            no_punct.append(char)\n",
    "\n",
    "    # Join the characters into a string\n",
    "    text_no_punct = ''.join(no_punct)   \n",
    "    \n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8380a327-1a75-4181-9915-7ee1700aea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # Step 1: Lowercase\n",
    "    text = ...\n",
    "\n",
    "    # Step 2: Use remove_punct to remove punctuation marks\n",
    "    text = ...\n",
    "\n",
    "    # Step 3: Remove extra whitespace characters\n",
    "    text = ...\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df33994f-b24a-4b12-a63a-2ef8057edfeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(challenge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25616f6a-2c3d-4004-9d78-73b1cb4c3e6d",
   "metadata": {},
   "source": [
    "## Challenge 3: Remove Stop Words\n",
    "\n",
    "We have known how `nltk` and `spaCy` work as NLP packages. We've also demostrated how to identify stop words with each package. \n",
    "\n",
    "Let's write **two** functions to remove stop words from our text data. \n",
    "\n",
    "- Complete the function for stop words removal using `nltk`\n",
    "    - The starter code requires two arguments: the raw text input and a list of predefined stop words\n",
    "- Complete the function for stop words removal using `spaCy`\n",
    "    - The starter code requires one argument: the raw text input\n",
    " \n",
    "A little reminder before we dive in: both functions take raw text as input, so that's a signal to perform tokenization on the raw text first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b22c991-bd26-499c-bdf7-cdec2043ae14",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2095795780.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    # YOUR CODE here\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def remove_stopword_nltk(raw_text, stopword):\n",
    "    \n",
    "    # Step 1: Tokenization with nltk\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Step 2: Filter out tokens in the stop word list\n",
    "    # YOUR CODE here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148dab5-64aa-4585-bcd8-4e4f1f65fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword_spacy(raw_text):\n",
    "\n",
    "    # Step 1: Apply the nlp pipeline\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Step 2: Filter out tokens that are stop words\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7a9c53-7bcc-49b7-9ad4-6061eef94fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tweets['text'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c182f-9091-42f5-8a2a-0b38322a0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopword_nltk(text, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e4a8b-bfee-4ea2-bb7d-00e31d870e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopword_spacy(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beff891a-2815-40fc-96e8-c4b292c2b956",
   "metadata": {},
   "source": [
    "## Challenge 4: Find the Word Boundary\n",
    "\n",
    "Now we know that tokenization in BERT often returns subwords. Let's try a few more examples! \n",
    "\n",
    "Do the results make sense to you? What do you think is the correct word boundary to split the following words into subwords? \n",
    "\n",
    "Also feel free to read more about limitations of the WordPiece algorithm. For instance, [this blog post](https://medium.com/@rickbattle/weaknesses-of-wordpiece-tokenization-eb20e37fec99) dives into reasons why does it fail, and [this one](https://tinkerd.net/blog/machine-learning/bert-tokenization/#demo-bert-tokenizer) introduces the mechanism underlying the algoritm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43380209-25b7-4593-a60d-1be58eac528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "# Initialize the tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_tokens(string):\n",
    "    '''Tokenize the input string with BERT'''\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    return print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e605c1-fb19-4899-a3f9-acce0db680b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Abbreviations\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClermont-Ferrand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Prefix\u001b[39;00m\n\u001b[1;32m      5\u001b[0m get_tokens(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munstoppable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mget_tokens\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_tokens\u001b[39m(string):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Tokenzie the input string with BERT'''\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize(string)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Abbreviations\n",
    "get_tokens('Clermont-Ferrand')\n",
    "\n",
    "# Prefix\n",
    "get_tokens('unstoppable')\n",
    "\n",
    "# Digits\n",
    "get_tokens('378')\n",
    "\n",
    "# YOUR EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32551dc-1077-445d-aace-f82fcb552066",
   "metadata": {},
   "source": [
    "## Challenge 5: Words with Highest Mean TF-IDF scores\n",
    "\n",
    "In notebook 9b, we got tf-idf values for each term in each document. Does that inform us anything about our data? Instead of focusing on the tf-idf value of any particular word, let's take a step back. Are there any words that are particularly informative for tweets that have been classified as positive/negative? \n",
    "\n",
    "Let's gather the indices to all positive/negative tweets, and calculate the mean tf-idf scores of words appear in positive/negative tweets. \n",
    "\n",
    "We've provided the following starter codes to scaffold:\n",
    "- Use boolean masks to select tweets that have positive/negative sentiments, retrieve the indices, and assign them to `positive_index`/`negative_index`\n",
    "- Select positive/negative tweets in the tfidf dataframe, and take the mean tf-idf values across the documents, sort the mean values in the descedning order, and get the top 10 terms. \n",
    "\n",
    "After you've completed the following two cells, you can plot the words having the highest mean tf-idf scores for each subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687401d9-6a8a-4832-a064-3fa285279bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the code to generate the tf-idf dataframe\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa612ae-00c6-4ebb-8feb-d7f97c62c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the boolean masks identifying tweets by 'airline_sentiment'\n",
    "positive_index = tweets[...].index\n",
    "negative_index = tweets[...].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e03dc-f46d-449c-9087-3b8f551debf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following two lines\n",
    "pos = tfidf.loc[...].mean().sort_values(...).head(...)\n",
    "neg = tfidf.loc[...].mean().sort_values(...).head(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a02ed4-e279-4a8d-8097-e07b4f9a1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.plot(kind='barh', \n",
    "         xlim=(0, 0.18),\n",
    "         color='cornflowerblue',\n",
    "         title='Top 10 terms with the highest mean tf-idf values for positive tweets');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4551b9a-8cb3-4ead-abc2-42d8cf13289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg.plot(kind='barh', \n",
    "         xlim=(0, 0.18),\n",
    "         color='darksalmon',\n",
    "         title='Top 10 terms with the highest mean tf-idf values for negative tweets');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569bb3c-97f5-4ea5-b4a1-41e27967d8fc",
   "metadata": {},
   "source": [
    "How do you interpret these two plots? Are there any words that don't really make sense to you? Do the results suggest a need for any additional preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08e625-3ffa-486c-b3d0-a1a7a8b6bd23",
   "metadata": {},
   "source": [
    "## Challenge 6: Doesn't Match\n",
    "\n",
    "We have a list of tuples for coffee-noun pairs. Let's find out which coffee drink is most commonly associated with the word \"coffee,\" and which one is not. Complete the for loop to calculate the cosine similarity between each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba9893-5bf8-400c-9432-5d6fb1d72d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load_word2vec_format('Data/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d82762-f314-4f0a-a248-c1ff9caefd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_nouns = [\n",
    "    ('coffee', 'espresso'),\n",
    "    ('coffee', 'cappuccino'),\n",
    "    ('coffee', 'latte'),\n",
    "    ('coffee', 'americano'),\n",
    "    ('coffee', 'irish'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95448c-eb1a-4bda-bac4-745b6c55af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1, w2 in coffee_nouns:\n",
    "    similarity = # YOUR CODE HERE\n",
    "    print(f\"{w1}, {w2}, {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e50a4f-6980-44ea-9d2d-35229fa68c64",
   "metadata": {},
   "source": [
    "Next, look up the documentation for the [`doesnt_match`](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#word2vec-demo) function. We will use it to identify the verb in the following list (one cell below) that does not seem to belong.\n",
    "\n",
    "Use `doesnt_match` to find the verb that is unlikely to fit within the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61134261-9596-4ec1-be7d-0d6bfe195528",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_verbs = ['brew', 'drip', 'pour', 'make', 'grind', 'roast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56f67a-20c5-4002-baf8-160921d7a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_doesnt_match = # YOUR CODE HERE\n",
    "verb_doesnt_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448ce9c-501d-460b-8e5e-5ade889a511f",
   "metadata": {},
   "source": [
    "## Challenge 7: Gender bias in word embeddings\n",
    "\n",
    "[Bolukbasi et al. (2016)](https://arxiv.org/pdf/1607.06520) is a thought-provoking investigation of gender bias in word embeddings. They primarily focus on word analogies, especially those that reveal gender stereotyping. Let run a couple examples discussed in the paper, using the `most_similiar` function we've just learned. \n",
    "\n",
    "The following code block contains a few examples we can pass to the `positive` argument: we want the output to be similar to, for example, `woman` and `chairman`, and in the meantime, we are also specificying that it should be dissimilar to `man`. We'll print the top result by indexing to the 0th item. \n",
    "\n",
    "Let's complete the following for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26910669-83a3-438c-bea5-89b2150db6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pair = [['woman', 'chairman'],\n",
    "                 ['woman', 'doctor'], \n",
    "                 ['woman', 'computer_programmer'],\n",
    "                 ['woman', 'pilot']]\n",
    "negative_word = 'man'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7883e-b9e6-4aa3-82fa-a0b479bc9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in positive_pair:\n",
    "    # YOUR CODE HERE\n",
    "    result = ...\n",
    "    print(f\"man is to {example[1]} as woman to {result[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81e848-3f42-4c42-a841-2359de9a767f",
   "metadata": {},
   "source": [
    "**Question**: What do you find? Are these results surprising?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
