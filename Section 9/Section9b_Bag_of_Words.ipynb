{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffcdc86-76ef-453f-a162-7741c9c23b8c",
   "metadata": {},
   "source": [
    "# Section 9. Bag of Words Text Analysis\n",
    "\n",
    "#### Instructor: Pierre Biscaye \n",
    "\n",
    "This is the second of three notebooks covering the foundations for performing **text analysis** in Python. In the previous part, we learned how to perform text preprocessing. However, we didn't move beyond the text data itself. If we're interested in doing any computational analysis on the text data, we still need approaches to convert the text into a **numeric representation**.\n",
    "\n",
    "In Part 2 of this series, we'll explore one of the most straightforward ways to generate a numeric representation from text: the **bag-of-words** (BoW). We will implement the BoW representation to transform the airline tweets data into numerical representation, and then build a classifier to explore what we can learn about the sentiment of the tweets. At the heart of the bag-of-words approach lies the assumption that the frequency of specific tokens is informative about the semantics and sentiment underlying the text. We'll make heavy use of the `scikit-learn` package to do so, as it provides a nice framework for constructing the numeric representations.\n",
    "\n",
    "The content of this notebook is taken from UC Berkeley D-Lab's Python Text Analysis [course](https://github.com/dlab-berkeley/Python-Text-Analysis).\n",
    "    \n",
    "### Sections\n",
    "1. Exploratory Data Analysis and Preprocessing\n",
    "2. The Bag-of-Words Representation: Learn how to convert text data into a numerical representation through a Bag-of-Words approach.\n",
    "3. Term Frequency-Inverse Document Frequency: Understand the TF-IDF algorithm and how it complements the Bag-of-Words representation. \n",
    "4. Sentiment Classification: Use the numerical representations of text data to perform sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a3a0d-66f4-44e5-8dd6-5f441146014d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to install the following packages\n",
    "# %pip install NLTK\n",
    "# %pip install spaCy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3862ffd-918f-4184-8c90-8a39a8a2a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cee159-c592-475b-abd4-a331c2250d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import re\n",
    "from string import punctuation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ea4a5-7c28-4557-acdd-afe8a97b7235",
   "metadata": {},
   "source": [
    "# 1. Exploratory Data Analysis and Preprocessing\n",
    "\n",
    "Before we do any preprocessing or modeling, we always should do some exploratory data analysis to get a feel for the dataset.\n",
    "\n",
    "First, let's take a look again at the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190e351-97b7-4c5b-866e-07aa6cbd42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset in\n",
    "tweets_path = 'Data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acbaf2-6625-4abb-b50f-97ea54ba0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80232c78-ac41-4d74-a581-76c9dac3b8f6",
   "metadata": {},
   "source": [
    "As a refresher, each row in this dataframe correponds to a tweet. The following columns are of main interests to us. There are other columns containing metadata of the tweet, such as the author of the tweet, when it was created, the timezone of the user, and others, which we will set aside for now. \n",
    "\n",
    "- `text` (`str`): the text of the tweet.\n",
    "- `airline_sentiment` (`str`): the sentiment of the tweet, labeled as \"neutral\", \"positive\", or \"negative\". \n",
    "- `airline` (`str`): the airline that is tweeted about.\n",
    "- `retweet count` (`int`): how many times the tweet was retweeted.\n",
    "\n",
    "To prepare us for sentiment classification, we'll partition the dataset so as to focus on the \"positive\" and \"negative\" tweets for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1faaf90-8c01-4d25-9468-90c01823f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['airline_sentiment'] != 'neutral'].reset_index(drop=True)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6b039-53e7-4afe-a9e0-b3522c12b2d7",
   "metadata": {},
   "source": [
    "Let's take a look at the text of a few of these tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438830e6-1064-47fe-b578-a1ca693a0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first five tweets\n",
    "for idx in range(5):\n",
    "    print(tweets['text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6746f8-b29c-40d4-bef6-b4afd4cd6cc1",
   "metadata": {},
   "source": [
    "We can already see that some of these tweets contain negative sentiment—how can we tell this is the case? \n",
    "\n",
    "Next, let's take a look at the distribution of sentiment labels in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01955158-6954-447a-acb6-2989d02a49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a bar plot showing the count of tweet sentiments\n",
    "sns.countplot(data=tweets,\n",
    "              x='airline_sentiment', \n",
    "              color='cornflowerblue',\n",
    "              order=['positive', 'negative']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab45abf-adf4-4f5e-ae09-75f6c4fd50d1",
   "metadata": {},
   "source": [
    "It looks like the majority of the tweets we have in this dataset have been classified as expressing negative sentiment.\n",
    "\n",
    "Let's take a look at what gets more retweeted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ddde7-af73-4eb6-92c9-041a1791ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean retweet count for each sentiment\n",
    "tweets.groupby('airline_sentiment')['retweet_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31f3bc-257c-48a8-86a0-fd0d7c3e8cb3",
   "metadata": {},
   "source": [
    "Negative tweets are clearly retweeted more often than tweets having positive sentiments.\n",
    "\n",
    "Let's see which airline receives most negative tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa9f2d-d655-494a-bb72-08ad973518f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the proportion of negative tweets by airline\n",
    "proportions = tweets.groupby(['airline', 'airline_sentiment']).size() / tweets.groupby('airline').size()\n",
    "proportions.unstack().sort_values('negative', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042419e-9c41-40e7-8dbf-47bd1e2ad45a",
   "metadata": {},
   "source": [
    "It looks like people are most dissatified with US Airways, followed by American Airlines, both having over 85\\% negative tweets!\n",
    "\n",
    "That's enough data exploration for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e513930-2dc7-489c-bc5a-22eb09add5bf",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before conducting our own sentiment analysis, we need to preprocess the text data so that they are in a standard format.\n",
    "\n",
    "We spent much of the last workshop learning how to preprocess data. Let's apply what we learned! Looking at some of the tweets above, we can see that while they are in pretty good shape, we can do some additional processing on them.\n",
    "\n",
    "In our pipeline, we'll omit the tokenization process, since we will perform it in a later step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a83ece-f3b2-4200-9d22-0788fbc07fa4",
   "metadata": {},
   "source": [
    "Let's put together a text cleaning pipeline. \n",
    "\n",
    "We'll accomplish this by writing a function called `preprocess()` that performs the following steps on a text input:\n",
    "* Step 1: Lowercase text.\n",
    "* Step 2: Replace the following patterns with placeholders:\n",
    "    * URLs &rarr; ` URL `\n",
    "    * Digits &rarr; ` DIGIT `\n",
    "    * Hashtags &rarr; ` HASHTAG `\n",
    "    * Tweet handles &rarr; ` USER `\n",
    "* Step 3: Remove extra blankspaces.\n",
    "\n",
    "**Question**: Why might some of these steps make sense for a sentiment analysis task? What step(s) might we reconsider?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03569f0d-34ba-492d-aa1d-1dce9d34f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "blankspace_pattern = r'\\s+'\n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "handle_pattern = r'@\\w+'\n",
    "digit_pattern = r'\\d+'\n",
    "hashtag_pattern = r'[＃#]'\n",
    "\n",
    "def preprocess(text):\n",
    "    '''Create a preprocess pipeline that cleans the tweet data.'''\n",
    "\n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Replace patterns with placeholders\n",
    "    text = re.sub(url_pattern, ' URL ', text)\n",
    "    text = re.sub(handle_pattern, ' HANDLE ', text)\n",
    "    text = re.sub(digit_pattern, ' DIGITS ', text)\n",
    "    text = re.sub(r'[＃#]', ' ', text)\n",
    "\n",
    "    # Step 3: Remove extra whitespace characters\n",
    "    text = re.sub(blankspace_pattern, ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a448fb-e180-455f-a296-686a7d7ad9ef",
   "metadata": {},
   "source": [
    "Let's test the `preprocess()` function on an example tweet to see how it's working. Then we can apply it to the entire `text` column in the tweets DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990cefd-5d04-46ba-ada2-29978c28cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweet = 'congrats @Beyonce #Finally and @kendricklamar #NotLikeUs for big wins at the 2025 #Grammys https://abcnews.go.com/GMA/Culture/2025-grammys-winners-list/story?id=118247847'\n",
    "\n",
    "# Print the example tweet\n",
    "print(example_tweet)\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Print the preprocessed tweet\n",
    "print(preprocess(example_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7bb6a-f064-48cc-b650-12c4ef2fbb88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to text column and assign the preprocessed tweets to a new column\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "tweets['text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acc6-b305-492a-8fde-65b343cb779c",
   "metadata": {},
   "source": [
    "Congratulations! Preprocessing is complete. Let's dive into the bag-of-words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53282330-54da-4e1c-bfe5-e77cb8fa3add",
   "metadata": {},
   "source": [
    "# 2. The Bag-of-Words Representation\n",
    "\n",
    "The idea of bag-of-words (BoW), as the name suggests, is quite intuitive: we take a text (or all its words) and toss it in a bag. The action of \"throwing\" the document in a bag disregards relative position between words, so what is \"in the bag\" is essentially an unsorted set of words [(Jurafsky & Martin, 2024, p.62)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf). We then sort these into a list of unique words and the frequency with which they appear. \n",
    "\n",
    "For example, as shown in the following illustration, the word \"coffee\" appears twice. \n",
    "\n",
    "<img src='Images/bow-illustration-1.png' alt=\"BoW-Part2\" width=\"600\">\n",
    "\n",
    "Analysis based on a bag-of-words representation primarily focuses on word frequency while discarding consideratinos around word order. \n",
    "\n",
    "In the context of sentiment analysis, the sentiment of a tweet is conveyed more strongly by specific words. For example, if a tweet contains the word \"happy\", it likely conveys positive sentiment, but not always (e.g., \"not happy\" denotes the opposite sentiment). When these words come up more often, they probably more strongly convey the sentiment. Clearly, a bag-of-words approach can help with sentiment analysis, but it has serious limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9bdbd-406d-469b-a8f6-41d1b3687c37",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "\n",
    "Now let's implement the idea of bag-of-words. Before we go deep into that, let's step back for a moment. In practice, text analysis often involves handling many documents. From now on, we use the term **document** to indicate a piece of text that we perform analysis on. It could be a news article, a book chapter, a phrase, a sentence, a tweet, etc.. As long as it can be represented by a string of text, the length dosen't really matter. \n",
    "\n",
    "Imagine we have four documents (i.e., the four coffee-related phrases shown above) and toss them all in the bag. Instead of a word-frequency list, we can create a **document-term matrix** (DTM), which preserves information about each document rather than simply aggregating across all documents. In a DTM, the word list is the **vocabulary** (V) that holds all unique words occuring across the documents. For each **document** (D), we count the number of occurences of each word in the vocabulary, and then plug the number into the matrix. In other words, the DTM we construct is a $D \\times V$ matrix, where each row corresponds to a document, and each column corresponds to a token (or \"term\").\n",
    "\n",
    "In the following example, the unique tokens (in this case individual words) in this set of documents, in alphabetical order, are in columns. For each document, we mark the occurence of each word showing up in the document. The numerical representation for each document is a row in the matrix. For example, \"the coffee roaster\" or the first document has numerical representation $[0, 1, 0, 0, 0, 1, 1, 0]$.\n",
    "\n",
    "Note that the left index column now displays these documents as texts, but typically we would just assign an index to each of them. \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccccccccccc}\n",
    " & \\text{americano} & \\text{coffee} & \\text{iced} & \\text{light} & \\text{roast} & \\text{roaster} & \\text{the} & \\text{time} \\\\\\hline\n",
    "\\text{the coffee roaster} &0 &1\t&0\t&0\t&0\t&1\t&1\t&0 \\\\ \n",
    "\\text{light roast} &0 &0\t&0\t&1\t&1\t&0\t&0\t&0 \\\\\n",
    "\\text{iced americano} &1 &0\t&1\t&0\t&0\t&0\t&0\t&0 \\\\\n",
    "\\text{coffee time} &0 &1\t&0\t&0\t&0\t&0\t&0\t&1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To create a DTM, we will use `CountVectorizer` from the package `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2adf56-ba93-459d-8cfa-16ce8dc9284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989781d-6b40-417a-be70-eeba05cd8a50",
   "metadata": {},
   "source": [
    "The image below summarizes the general workflow of `CountVectorizer`:\n",
    "\n",
    "<img src='Images/CountVectorizer1.png' alt=\"CountVectorizer\" width=\"500\">\n",
    "\n",
    "Let's walk through these steps with the example of coffee phrases shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34174034-46b9-43e2-a511-5972d378cb00",
   "metadata": {},
   "source": [
    "### A Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2bd3d-0460-4b5f-9b9e-02940db0d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy example containing four documents (phrases)\n",
    "test = ['the coffee roaster',\n",
    "        'light roast',\n",
    "        'iced americano',\n",
    "        'coffee time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7c1d3-fcee-4e20-b9a7-17306ebd5fc2",
   "metadata": {},
   "source": [
    "The first step is to initialize a `CountVectorizer` object. Within the round paratheses are the parameter settings we may choose to specify. You can take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and see what options are available.  \n",
    "\n",
    "For now we can just leave it blank for the default settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3fe6a-9abf-4e11-aad1-e54c891567bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a7d0d-0bfc-4fb9-8e5f-e91e39797fb5",
   "metadata": {},
   "source": [
    "The second step is to `fit` this `CountVectorizer` object to the data, which means creating a vocabulary of tokens from the set of documents. Thirdly, we `transform` our data according to the \"fitted\" `CountVectorizer` object, which means taking each of the document and transforming it into a DTM according to the vocabulary established by the \"fitting\" step.\n",
    "\n",
    "It may sound a bit complex but steps 2 and 3 can actually be done in one swoop using a `fit_transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1bbad4-bb1a-4b92-9096-6e17558b4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform to create DTM\n",
    "test_count = vectorizer.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d3b65-4e98-48bf-87d2-399457f4939c",
   "metadata": {},
   "source": [
    "Let's take a look at the resulting DTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb044001-8eb2-4489-b025-2d8e2d4bfee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9817b09-a806-42c4-9436-822cc27a38b9",
   "metadata": {},
   "source": [
    "Apparently the return is a \"sparse matrix\"—a matrix that contains a lot zeros. It actually makes sense. For each document we definitely have words that don't occur at all, which are counted zero in the DTM. This sparse matrix is stored in a \"Compressed Sparse Row\" format, which is a memory-saving format that is designed to deal with sparse matrix. \n",
    "\n",
    "Let's convert it to a dense matrix, where those zeros are organized as in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03a238-87d8-40c9-b20e-66e7c9b6576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DTM to a dense matrix \n",
    "test_count.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b58a63-d7f6-4b9f-aadf-4d4fc7341336",
   "metadata": {},
   "source": [
    "So this is our DTM. It is the same as shown above, but to make it more reader-friendly, let's convert it to a dataframe. The column names should be tokens in the vocabulary, which we can access with `get_feature_names_out()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714de5d3-e37d-4a19-9ade-3c6629e38d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the vocabulary\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7729a2-ca2e-4de7-8795-74dfedb7a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DTM dataframe\n",
    "test_dtm = pd.DataFrame(data=test_count.todense(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a03b4-94fa-4fe7-8f5d-7280e31b9bc4",
   "metadata": {},
   "source": [
    "Here it is! The DTM of our toy data is now a dataframe. The index of `test_dtm` corresponds to the position of each document in the `test` list. \n",
    "\n",
    "Now let's apply this process to the preprocessed tweet data we set up above.\n",
    "\n",
    "### DTM for Tweets\n",
    "\n",
    "We'll still begin with initializing a `CountVectorizer` object. In the following cell, we have included a few parameters that people often adjust. These parameters are currently set to their default values.\n",
    "\n",
    "As shown below, when we construct a DTM, the default is to lowercase the input text. If nothing is provided for `stop_words`, the default is to keep them. The next three parameters are used to control the size of the vocabulary, which we'll return to in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783e44a4-4a22-4290-b222-282b02c080dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             stop_words=None,\n",
    "                             min_df=1,\n",
    "                             max_df=1.0, \n",
    "                             max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e76ea-bc54-4775-bcda-432a03d2c96f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit and transform to create DTM\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87119057-c78c-4eb2-a9d6-3e9f44e4c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run if you have limited memory\n",
    "np.array(counts.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99322b85-1a15-46a5-bb80-bb5eaa6eeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens\n",
    "tokens = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43620587-3795-4434-8f1f-145c81b93706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DTM\n",
    "first_dtm = pd.DataFrame(data=counts.todense(),\n",
    "                         index=tweets.index,\n",
    "                         columns=tokens)\n",
    "\n",
    "# Print the shape of DTM\n",
    "print(first_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd257d5-4244-436c-afe7-5688232caf8f",
   "metadata": {},
   "source": [
    "If we leave the `CountVectorizer` to the default setting, in total we have a vocabulary size of 9817. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3604ec-d909-4238-9a3f-67e7d4ae2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d34e2-52f8-4419-b4c7-ed20dbd5df89",
   "metadata": {},
   "source": [
    "Most of the tokens have zero occurences at least in the first five tweets. This is not surprising when most tweets have relatively few words and we are indexing over nearly 10,000 vocabulary terms. \n",
    "\n",
    "Let's take a closer look at the DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432154a-eae0-4723-a797-55f3cfdd71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent tokens\n",
    "first_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7f1c9-dd66-49f2-b337-01253da551d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least frequent tokens\n",
    "first_dtm.sum().sort_values(ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d230f79-e752-4e32-93db-4f013287f8e2",
   "metadata": {},
   "source": [
    "It is not surprising to see \"handle\" and \"digit\" to be among the most frequent tokens as we replaced each idiosyncratic one to these placeholders. The rest of the most frequent list are mostly stop words, except for \"flight\".\n",
    "\n",
    "Perhaps a more interesting pattern is to look for which token appears most in any given tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8f4d8-4c88-4155-a6c5-c72a5b4e8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame()\n",
    "\n",
    "# Retrieve the index to the tweet where each token appears most frequently\n",
    "counts['token'] = first_dtm.idxmax(axis=1)\n",
    "\n",
    "# Retrieve the number of occurence \n",
    "counts['number'] = first_dtm.max(axis=1)\n",
    "\n",
    "# Filter out placeholders\n",
    "counts[(counts['token'] != 'digits')\n",
    "         & (counts['token'] != 'handle')].sort_values('number', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdac4ef-6b9d-4aad-9b24-c70f6c2eb8f0",
   "metadata": {},
   "source": [
    "It looks like among all tweets, at most a token appears 6 times, and it is either the word \"It\" or the word \"worst\". \n",
    "\n",
    "Let's go back to our tweets dataframe and get the 918th tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cacd8-1fb3-4f0d-a744-4ee0994a089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at index 918: \"worst\"\n",
    "tweets.iloc[918]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba8e37-4880-4565-b6fc-7e7c96958f0f",
   "metadata": {},
   "source": [
    "## Customizing the `CountVectorizer`\n",
    "\n",
    "So far we used to the default parameter settings to create our DTMs, but in many cases we may want to customize the `CountVectorizer` object. The purpose of doing so is to further filter out unnecessary tokens. In the example below, we tweak the following parameters:\n",
    "\n",
    "- `stop_words = 'english'`: exclude English stop words \n",
    "- `min_df = 2`: exclude words that don't occur at least twice across all documents\n",
    "- `max_df = 0.95`: exclude words if they occur in more than 95\\% of the documents \n",
    "\n",
    "**Question**: Does it seem reasonable to set these parameters? Keep in mind the objective of this text analysis task: sentiment analysis.\n",
    "\n",
    "Typically we are not interested in words whose frequencies are either too low or too high, so we use the `min_df` and `max_df` parameters to trim them out. Alternatively, we can also define our vocabulary size to be $N$ using the `max_feature` parameter—this tells `CountVectorizer` to only consider the top $N$ most frequent tokens when construct the DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0a93e-9dd8-43dc-a82c-06a24bf02bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the parameter setting\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             stop_words='english',\n",
    "                             min_df=2,\n",
    "                             max_df=0.95,\n",
    "                             max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e5ecf-7be3-4915-9d11-fd3edb913400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create the second DTM\n",
    "second_dtm = pd.DataFrame(data=counts.todense(),\n",
    "                          index=tweets.index,\n",
    "                          columns=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fb598-fa81-4111-9e36-7172d8034713",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_dtm.shape)\n",
    "print(second_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e66bc-2eaa-4642-8848-74459948084b",
   "metadata": {},
   "source": [
    "Our second DTM has a substantially smaller vocabulary, compared to the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7bf4e-640b-49bc-b64b-721140f67f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fe2c3-ec90-4027-8c7f-417327a33a27",
   "metadata": {},
   "source": [
    "The most frequent token list now includes words that make more sense to us, for example, \"cancelled\", \"service\", etc. Note that it no longer includes \"handle,\" as this was likely included in every tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b5145-d505-4e36-9a39-a40d25d8ec6f",
   "metadata": {},
   "source": [
    "## Lemmatize the Text Input\n",
    "\n",
    "Recall from notebook 9a that we introduced using `spaCy` to perform **lemmatization**, i.e., removing morphological affixes on words. With lemmatization, we keep only word stems in texts, which presumbaly should capture the core meaning of the text. \n",
    "\n",
    "Now let's implement lemmatization on our tweet data, and pass the lemmatized text to create a third DTM. We'll write a function `lemmatize_text`. It requires a text input, and the output is the same text except all tokens are lemmatized. We will use the `nlp()` pipeline for this, as lemmatization is one of the linguistic annotations that the `nlp` pipeline automatically does. We can use `token.lemma_` to access the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da610560-62c3-48ab-a1b2-25e0b589bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy (can take a few seconds)\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ead266-30f3-48ad-bc51-c1685487f000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    '''Lemmatize the text input with spaCy annotations.'''\n",
    "\n",
    "    # Step 1: Initialize an empty list to hold lemmas\n",
    "    lemma = []\n",
    "\n",
    "    # Step 2: Apply the nlp pipeline to input text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Step 3: Iterate over tokens in the text to get the token lemma\n",
    "    for token in doc:\n",
    "        lemma.append(token.lemma_)\n",
    "\n",
    "    # Step 4: Join lemmas together into a single string\n",
    "    text_lemma = ' '.join(lemma)\n",
    "    \n",
    "    return text_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36aab6-35dd-42a2-9b38-b7c432f021c6",
   "metadata": {},
   "source": [
    "Let's apply the function to an example tweet first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e82bb-5c42-4fa8-9101-5a0ea908db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to an example tweet\n",
    "print(tweets.iloc[101][\"text_processed\"])\n",
    "print(f\"{'='*50}\")\n",
    "print(lemmatize_text(tweets.iloc[101]['text_processed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeda987-dc32-4979-b158-c24be7d1a420",
   "metadata": {},
   "source": [
    "Now let's lemmatize the tweet data, and save the output to a new column `text_lemmatized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac128d2-1be5-4ef5-bb50-5b8d44ef8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while!\n",
    "tweets['text_lemmatized'] = tweets['text_processed'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fc862-a4cb-4af6-8708-557cab3e041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save this dataset with the cleaned tweets for future use\n",
    "tweets.to_csv(\"Data/tweets_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02aad6-4e71-4afc-80cf-31d4f39498b2",
   "metadata": {},
   "source": [
    "Now with the `text_lemmatized` column, let's create a third DTM. The parameter settings are the same as the second DTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49d790-3c9d-4dc1-a5c9-72c306630412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer (the same param setting as previous)\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             stop_words='english',\n",
    "                             min_df=2,\n",
    "                             max_df=0.95,\n",
    "                             max_features=None)\n",
    "\n",
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_lemmatized'])\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create the third DTM\n",
    "third_dtm = pd.DataFrame(data=counts.todense(),\n",
    "                   index=tweets.index,\n",
    "                   columns=tokens)\n",
    "third_dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859eb04-dbd2-4fa0-9798-65ed7496c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of three DTMs\n",
    "print(first_dtm.shape)\n",
    "print(second_dtm.shape)\n",
    "print(third_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94c8ac-e4f4-4b76-afdb-1d4af54a3eee",
   "metadata": {},
   "source": [
    "Let's print the top 10 most frequent tokens as usual. These tokens are now word stems, and the counts also change after lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745ca29-97ed-4fe1-81db-7e402c8da674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most frequent tokens in the third DTM\n",
    "third_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c63e6a-50c3-448a-9a56-a1d193cd6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to the most frequent tokens in the second DTM\n",
    "second_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38363398-fdf5-456b-ae3d-cae9d5294140",
   "metadata": {},
   "source": [
    "# 3. Term Frequency-Inverse Document Frequency \n",
    "\n",
    "So far, we've been relying on word frequencies to give us information about a document. This assumes that if a word appears more often in a document, it's more informative. However, this may not always be the case. For example, we've already removed stop words because they are not informative, despite the fact that they appear many times in a document. We also know the word \"flight\" is among the most frequent words, but it is also not that informative, because it appears in many documents. Since we're looking at airline tweets, we shouldn't be surprised to see the word \"flight\"!\n",
    "\n",
    "To remedy this, we use a weighting scheme called **tf-idf (term frequency-inverse document frequency)**. The big idea behind tf-idf is to weight a word not just by its frequency within a document, but also by its across documents. The idea is that words are more informative if there is variation in their appearance across documents, and when they appear frequently within a given document. So, when we construct the DTM, we will be assigning each term a **tf-idf score**. Specifically, term $t$ in document $d$ is assigned a tf-idf score as follows:\n",
    "\n",
    "<img src='Images/tf-idf.png' alt=\"TF-IDF\" width=\"1200\">\n",
    "\n",
    "In essence, the tf-idf score of a word in a document is the product of two components: term frequency (tf) and inverse document frequency (idf). The idf acts as a scaling factor. If a word occurs in all documents, then idf equals to 1 and no scaling will happen. But idf is typically greater than 1, which is the weight we assign to the word to make the tf-idf score higher, so as to highlight that the word is informative. In practice, we add 1 to both the denominator and nominator (\"add-1 smooth\"), to prevent any issues with zero occurrences.\n",
    "\n",
    "We will create a tf-idf DTM using `sklearn`'s `TfidfVectorizer` function. It takes mainly the same parameters as `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e32d8a-c42d-475f-aab4-21eca8b1aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23916c1-5693-456c-b71d-6d9d78d1e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                             stop_words='english',\n",
    "                             min_df=2,\n",
    "                             max_df=0.95,\n",
    "                             max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5b342-ab18-4766-9561-e38e50cd1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform \n",
    "tf_dtm = vectorizer.fit_transform(tweets['text_lemmatized'])\n",
    "tf_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e509c8-5402-4be0-9143-0e448fff7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf-idf dataframe\n",
    "tfidf = pd.DataFrame(tf_dtm.todense(),\n",
    "                     columns=vectorizer.get_feature_names_out(),\n",
    "                     index=tweets.index)\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba13ea-c429-4ff1-a9a2-abf27c4d0888",
   "metadata": {},
   "source": [
    "You may have noticed that we still have the same vocabulary size as above. This is because we used the same parameter settings when creating the vectorizer. But the values in the matrix are different now—they are tf-idf scores instead of raw counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58c360-5c55-4fa0-8c55-1f00e68baa9a",
   "metadata": {},
   "source": [
    "## Interpret TF-IDF Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad233d-ebc1-420f-9b67-c227c48f3e60",
   "metadata": {},
   "source": [
    "Let's take a look at the documents where each term has the highest tf-idf value.\n",
    "\n",
    "We'll use `idxmax()` to find the index to these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b511a-d448-4cfb-a6a0-22a465efd8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the index to the document\n",
    "tfidf.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc0249-7c68-42ee-8290-ff41715e346b",
   "metadata": {},
   "source": [
    "For example, the term \"worst\" occurs distinctively in the 918th tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b222fb-ad8c-4767-a974-dd261370a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.idxmax()['worst']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a48bc-dc93-481b-ba49-29876fc577fb",
   "metadata": {},
   "source": [
    "Recall from previous part that it is the tweet where the word \"worst\" appears 6 times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ee0e0-476f-4236-ba8a-615ba7a0efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text'].iloc[918]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd06bbc-e2fc-49e4-9354-efdaca5cfbd3",
   "metadata": {},
   "source": [
    "How about \"delay\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809df1a-1178-4272-a415-42edb20173b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.idxmax()['delay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093b6a7-54ca-468a-9376-b3c0be0b6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text_processed'].iloc[5740]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b4a60-f816-4f76-917c-93f81c8ea931",
   "metadata": {},
   "source": [
    "It seems there is no instance where 'delay' appears more than once.\n",
    "\n",
    "In the practice notebook, you will be asked to use the tf-idf dataframe to plot the 10 most informative words in tweets that have been classified as positive and negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da410cb3-a452-441b-a94d-8f751d59d7a6",
   "metadata": {},
   "source": [
    "# 4. Introduction to Sentiment Classification\n",
    "\n",
    "Now that we have a numerical representation of the text, we are ready operate on it for our sentiment classification task. We'll construct a DTM from the text data, and use that to predict the sentiment labels using a logistic regression model, as covered in Section 7 of this course. \n",
    "\n",
    "We will split the tweets data into traning and test samples, then train the model on the training sample. The target is the airline sentiment being positive or negative. The list of features we'll pass to the model is exactly the vocabulary of the DTM. The coefficients from the model will tell us whether a feature contributes positively or negatively to the predicted value. The predicted value will then inform the predicted label, either positive (when $p>=0.5$) or negative (when $p<0.5$). \n",
    "\n",
    "We will then evaluate the performance of the model on the test data. \n",
    "\n",
    "Now that we have the tf-idf dataframe, the feature set is ready. Let's dive into model specification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33413d63-87eb-489f-b374-3cfeaa51cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87ff74-3fbb-472a-b795-6f4d18fab215",
   "metadata": {},
   "source": [
    "We'll use the `train_test_split` function from `sklearn` to separate our data into two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cec8b9-14d9-4897-9c02-cc89fcf7b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X = tfidf\n",
    "y = tweets['airline_sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066771d8-2f31-4646-9a1b-6d2b1b9b208c",
   "metadata": {},
   "source": [
    "Now let's specify and fit a logistic regression model.\n",
    "\n",
    "Recall that the positive and negative classes are not balanced, so we can tell the classifier to automatically assign weights (through the `class_weight` parameter) proportional to our data in order to have it pay more attention to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773963bd-6603-4fad-884b-09ce60afab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model\n",
    "lr = LogisticRegressionCV(Cs=10, # how many different values of the inverse regularization strength (C) should be tested, 10 is default\n",
    "                                 cv=5, # 5-fold cross-validation\n",
    "                                 solver='liblinear', # suitable for smaller datasets, supports lasso and ridge regularization\n",
    "                                 class_weight='balanced', # automatically adjusts weights to handle imbalanced datasets\n",
    "                                 random_state=5, # for reproducibility\n",
    "                                 refit=True) # retrains full dataset on the best C value\n",
    "# Fit the logistic regression model\n",
    "model = lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124aa7ea-1bc1-43e2-beeb-0ba2da9b2df9",
   "metadata": {},
   "source": [
    "How does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d06c1-d884-45d4-a03d-dd5d40bf70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test accuracy\n",
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e186c5-1719-4deb-bdb4-614a9980f058",
   "metadata": {},
   "source": [
    "The model got ~95% accuracy on the training set, and ~91% on the test set - that's pretty good! The similarity between the two performances is also a good sign—it means we were able to generalize pretty well.\n",
    "\n",
    "Let's also take a look at the fitted coefficients to see if what we see makes sense! \n",
    "\n",
    "We can access them using `coef_`, and we can match each coefficient to the tokens from the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb6ef1-13b3-437e-813c-7118911847a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefs of all features\n",
    "coefs = model.coef_.ravel()\n",
    "\n",
    "# Get all tokens\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a token-coef dataframe\n",
    "importance = pd.DataFrame()\n",
    "importance['token'] = tokens\n",
    "importance['coefs'] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63814e-9c0d-4f7a-a5e0-72cca2758d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 tokens with lowest coefs\n",
    "neg_coef = importance.sort_values('coefs').head(10)\n",
    "neg_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e00c6-8a9f-484f-aea2-853fd5512083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 10 tokens that have the lowest coefs\n",
    "neg_coef.plot(kind='barh', \n",
    "              xlim=(0, -8),\n",
    "              x='token',\n",
    "              color='darksalmon',\n",
    "              title='Top 10 tokens with lowest coeffient values');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d596bf7-753c-40cd-ac52-4a37163650ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 tokens with highest coefs\n",
    "pos_coef = importance.sort_values('coefs').tail(10)\n",
    "pos_coef "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b1223b-e5c1-4992-bb7e-0a99651c3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 10 tokens that have the highest coefs\n",
    "pos_coef.sort_values('coefs', ascending=False).plot(kind='barh', \n",
    "                                                    xlim=(0, 14),\n",
    "                                                    x='token',\n",
    "                                                    color='cornflowerblue',\n",
    "                                                    title='Top 10 tokens with highest coeffient values');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed48ea-fd35-4585-9b98-90456aaee447",
   "metadata": {},
   "source": [
    "**Question:** Do these make sense? Could an analysis based on a set of such keywords have been used for the sentiment classification in the tweets dataset we are using? What would a modeler need to predict sentiment based on tweet text without preexisting classification as positive or negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4430fbd-108f-4a02-ab64-ef36c5949e56",
   "metadata": {},
   "source": [
    "\n",
    "## Key Points\n",
    "\n",
    "* A Bag-of-Words representation is a simple method to transform our text data to numbers. It focuses on word frequency but not word order. \n",
    "* A TF-IDF representation is a step further; it also considers if a certain word distinctively appears in one document or occurs uniformally across all documents. \n",
    "* With a numerical representation, we can perform a range of text classification task, such as sentiment analysis. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
