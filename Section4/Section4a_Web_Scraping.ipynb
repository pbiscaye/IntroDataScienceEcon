{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Python Web Scraping with Beautiful Soup\n",
    "\n",
    "#### Instructor: Pierre Biscaye \n",
    "\n",
    "The objective of this notebook is to introduce you to how to extract data from the web using Python web scraping tools.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Extracting and parsing HTML using Beautiful Soup\n",
    "2. Understand difference between tags, attributes, and attribute values\n",
    "3. Apply these tools in the context of scraping information about 2025 development job market paper blog posts\n",
    "4. Practice scraping downloadable files from a website\n",
    "\n",
    "### Libraries loaded\n",
    "* beautifulsoup4\n",
    "* datetime\n",
    "* requests\n",
    "* time\n",
    "* requests\n",
    "* lxml\n",
    "* pandas\n",
    "* re\n",
    "* os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "When we'd like to access data from the web, we first have to make sure if the website we are interested in offers a Web API. Platforms like Twitter, Reddit, and the New York Times offer APIs. In the next notebook we provide an introduction to using APIs using the NY Times API as a case study.\n",
    "\n",
    "However, there are often cases when a Web API does not exist. In these cases, we may have to resort to web scraping, where we extract the underlying HTML from a web page, and directly obtain the information we want. There are several packages in Python we can use to accomplish these tasks. We'll focus two packages: Requests and Beautiful Soup.\n",
    "\n",
    "Our first case study will be scraping information on the [World Bank Development Impact Job Market Blogs](https://blogs.worldbank.org/en/impactevaluations/wrap-up-of-job-market-series-2025). \n",
    "\n",
    "Before we get started, let's peruse the link and view the page source to take a look at the structure of the blogs. \n",
    "\n",
    "**Question**: What do you observe, both about the structure of the web pages and about the structure of the URLs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extracting and Parsing HTML \n",
    "\n",
    "We will use two main packages: [Requests](http://docs.python-requests.org/en/latest/user/quickstart/) and [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/). We'll also need the `lxml` package, which helps support some of the parsing that Beautiful Soup performs, but do not need to load it specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to succesfully scrape and analyse HTML, we'll be going through the following 4 steps:\n",
    "1. Make a GET request\n",
    "2. Parse the page with Beautiful Soup\n",
    "3. Search for HTML elements\n",
    "4. Get attributes and text of these elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Make a GET Request to Obtain a Page's HTML\n",
    "\n",
    "We can use the Requests library to:\n",
    "\n",
    "1. Make a GET request to the page, and\n",
    "2. Read in the webpage's HTML code.\n",
    "\n",
    "The process of making a request and obtaining a result resembles that of the Web API workflow. But here we are making a request directly to the website, and we are going to have to parse the HTML ourselves. This is in contrast to being provided data organized into a more straightforward `JSON` or `XML` output via an API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify URL\n",
    "url = \"https://blogs.worldbank.org/en/impactevaluations/wrap-up-of-job-market-series-2025\"\n",
    "# Make a GET request\n",
    "req = requests.get(url)\n",
    "req.raise_for_status()  # Ensures the request was successful/not blocked\n",
    "# Read the content of the server’s response\n",
    "src = req.text\n",
    "# View some output\n",
    "print(src[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Parse the Page with Beautiful Soup\n",
    "\n",
    "Now, we use the `BeautifulSoup` function to parse the reponse into an HTML tree. This returns an object (called a **soup object**) which contains all of the HTML in the original document.\n",
    "\n",
    "In order to parse the HTML, we need to choose a **parser**. This is the 'engine' that decides how to read, break down, and structure the HTML code.\n",
    "\n",
    "Two common ones are `html.parser` and `lxml`. `html.parser` is built-in to Python and has moderate speed but strict flexibility. In contrast `lxml` must be installed but is very fast and extremely lenient. `lxml` is the current inductry standard. \n",
    "\n",
    "In terms of speed, we probably won't notice a speed difference with the World Bank blog posts, but it would save significant time if scraping thousands of pages.\n",
    "\n",
    "In terms of leniency/flexibility, it is important to understand that real-world websites rarely have perfect HTML. For example, there might be a `<div>` that opens but never closes. More lenient/flexible parsers will guess where tags should close and build a logical tree anyway. More strict parsers might break and return incomplete content.\n",
    "\n",
    "If you run into an error about a parser library, make sure you've installed the `lxml` package to provide Beautiful Soup with the necessary parsing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the response into an HTML tree\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "# Take a look\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks pretty similar to the above, but now it's organized in a `soup` object which allows us to more easily traverse the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Search for HTML Elements\n",
    "\n",
    "Beautiful Soup has a number of functions to find useful components on a page. Beautiful Soup lets you find elements by their:\n",
    "\n",
    "1. HTML tags\n",
    "2. HTML Attributes\n",
    "3. CSS Selectors\n",
    "\n",
    "Let's search first for **HTML tags**. \n",
    "\n",
    "The function `find_all` searches the `soup` tree to find all the elements with a particular HTML tag, and returns all of those elements.\n",
    "\n",
    "What does the example below do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all elements with a certain tag\n",
    "a_tags = soup.find_all(\"a\")\n",
    "print(a_tags[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `find_all()` is the most popular method in the Beautiful Soup search API, you can use a shortcut for it. If you treat the BeautifulSoup object as though it were a function, then it’s the same as calling `find_all()` on that object. \n",
    "\n",
    "These two lines of code are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_tags = soup.find_all(\"a\")\n",
    "a_tags_alt = soup(\"a\")\n",
    "print(a_tags[0])\n",
    "print(a_tags_alt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many links did we obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot! Many elements on a page will have the same HTML tag. For instance, if you search for everything with the `a` tag, you're likely to get more hits, many of which you might not want. Remember, the `a` tag defines a hyperlink (which can see by the \"href\" in the above output), so you'll usually find many on any given page.\n",
    "\n",
    "What if we wanted to search for HTML tags with certain attributes, such as particular CSS classes? What classes do you see in the above list of the first set of HTML tags?\n",
    "\n",
    "We can restrict our search to certain classes by adding an additional argument to the `find_all`. In the example below, we are finding all the `a` tags, and then filtering those with `class_=\"lp__nav_link\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the 'a' tags in 'lp__nav_link' class\n",
    "nav_link = soup(\"a\", class_=\"lp__nav_link\")\n",
    "nav_link[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more efficient way to search for elements on a website is via a **CSS selector**. For this we have to use a different method called `select()`. Just pass a string into the `.select()` to get all elements with that string as a valid CSS selector.\n",
    "\n",
    "In the example above, we can use `\"a.lp__nav_link\"` as a CSS selector, which returns all `a` tags with class `lp__nav_link`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get elements with \"a.sidemenu\" CSS Selector.\n",
    "selected = soup.select(\"a.lp__nav_link\")\n",
    "selected[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, browsing the HTML, we notice that the list of links to all the blog posts is inside a div tag. We can look at all div tags, and also identify just the one we want by looking at its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_tags = soup.find_all(\"div\")\n",
    "print(len(div_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_div = soup.find('div', class_='text aem-GridColumn aem-GridColumn--default--12')\n",
    "main_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get Attributes and Text of Elements\n",
    "\n",
    "Once we identify elements, we want the access information in that element. Usually, this means two things:\n",
    "\n",
    "1. Text\n",
    "2. Attributes\n",
    "\n",
    "Getting the text inside an element is easy. All we have to do is use the `text` member of a `tag` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all navigation links as a list\n",
    "navigation_links = soup.select(\"a.lp__nav_link\")\n",
    "\n",
    "# Examine the first link\n",
    "first_link = navigation_links[0]\n",
    "print(first_link)\n",
    "\n",
    "# What class is this variable?\n",
    "print('Class: ', type(first_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a Beautiful Soup tag! This means it has a `text` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(first_link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want the value of certain attributes. This is particularly relevant for `a` tags, or links, where the `href` attribute tells us where the link goes. You can access a tag’s attributes by treating the tag like a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(first_link['href'])\n",
    "print(first_link.get('href')) # equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the main `<div>` section we are interested in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_div.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get the links from within this section? \n",
    "\n",
    "We need to search for the 'a' tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_div.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scraping WB Development Impact blog posts\n",
    "\n",
    "Believe it or not, those are really the fundamental tools you need to scrape a website. Once you spend more time familiarizing yourself with HTML and CSS, then it's simply a matter of understanding the structure of a particular website and intelligently applying the tools of Beautiful Soup and Python.\n",
    "\n",
    "Let's apply these skills to scrape the [World Bank Development Impact 2025 JMP blog posts](https://blogs.worldbank.org/en/impactevaluations/wrap-up-of-job-market-series-2025).\n",
    "\n",
    "Specifically, our goal is to scrape information on each blog post, including the link, title, author, website, institution, and blog text.\n",
    "\n",
    "Right now what we have is the page hosting the links to all our target blog post. Our first task is to collect those links, and then we will scrape information for each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all links\n",
    "\n",
    "We've already scraped and souped the context of the page linking to all the blogs. We identified the section containing the links, and seen how we can focus on just the link content. Now we want to extract those links. \n",
    "\n",
    "We'll use list comprehension (faster than a loop to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list comprehension: get the URL href for every tag a in the list main_div.find_all(‘a’), if the href exists\n",
    "links = [a.get('href') for a in main_div.find_all('a') if a.get('href')]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get rid of the first link to `econthatmatters`. We could do this structurally by searching on the strings, but since it's the first result we'll just drop that one. \n",
    "\n",
    "How can we do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping blog content\n",
    "\n",
    "Our goal is to obtain data from the contents of the individual blog pages. \n",
    "\n",
    "Here is how our data extraction process. We want:\n",
    "1. To loop through each link in the list of links\n",
    "2. To scrape and soup the content of the page\n",
    "3. To extract specific information from the page: title, author, author's website, institution, and blog text\n",
    "\n",
    "Let's look at the code for one of these to understand how it is set up. In particular, we need to identify how we will target the information we want to extract.\n",
    "\n",
    "Let's start digging through one blog. Then we'll write a loop to extract data systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the content\n",
    "page_res = requests.get(links[0])\n",
    "page_soup = BeautifulSoup(page_res.text, 'lxml')\n",
    "page_soup.prettify()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first extract of the data already tells us we can extract some information from the `<title>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup.find_all('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The title is also a direct attribute of the soup that we can call. Note that title tags have a `string` associated with them, rather than `text`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to extract what we want? We need to use string methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by \"Guest post by \"\n",
    "if \"Guest post by \" in page_soup.title.string:\n",
    "    title_part, author_name = page_soup.title.string.split(\"Guest post by \", 1)\n",
    "    # Remove trailing colon/spaces from title\n",
    "    title_part = title_part.strip().rstrip(':')\n",
    "else:\n",
    "    title_part = page_soup.title.string\n",
    "    author_name = \"Unknown\"\n",
    "print(title_part)\n",
    "print(author_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the blog source code, it looks like the rest of what we want is in a `div` tag for the main body, with a specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is only one div with the class we are targeting, so we will use find\n",
    "content_div = page_soup.find('div', class_='text aem-GridColumn aem-GridColumn--default--12')\n",
    "content_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice the following things:\n",
    "\n",
    "* The first `<p>` tag relates to the blog post series.\n",
    "* The last `<p>` tag is the author bio.\n",
    "* The main body text of the blog post is in between.\n",
    "\n",
    "We can therefore extract the main text of the blog by selecting specific `<p>` tags, and then extract author information from the last `<p>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine main text into one string\n",
    "paragraphs = content_div.find_all('p')\n",
    "post_text = \"\\n\".join([p.get_text() for p in paragraphs[1:-1]])\n",
    "post_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some formatting would be needed to clean this up, but that is a problem for later - in particular if we wanted to do any text analysis on these blogs.\n",
    "\n",
    "Now, let's extract author bio information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bio_p = paragraphs[-1]\n",
    "\n",
    "# Link: Extract the href from the <a> tag in the bio\n",
    "author_site = bio_p.find('a').get('href')\n",
    "                \n",
    "# Institution: Extract text after \"at \" and clean &nbsp;\n",
    "# in Python, the HTML entity &nbsp; is represented as \\xa0. \n",
    "# This line replaces those \"non-breaking spaces\" with standard spaces\n",
    "bio_text = bio_p.get_text().replace('\\xa0', ' ') # Remove &nbsp;\n",
    "if \"at \" in bio_text:\n",
    "    # Capture everything after \"at \" until the period\n",
    "    institution = bio_text.split(\"at \", 1)[1].split('.', 1)[0].strip()\n",
    "\n",
    "print(author_site)\n",
    "print(institution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic scraping protocol\n",
    "\n",
    "Now that we've figured out where to get the information we want, and assuming all the blog posts are set up the same way in the code, we can formalize the extraction code.\n",
    "\n",
    "We will do a few things in case there are differences, notably writing `if` statements that let the code move on if it doesn't find what we are looking for. \n",
    "\n",
    "We will also code in a short lag between requests, to not overload the server. This is a best practice for web scraping. We will use the `time` package to do this.\n",
    "\n",
    "We will start by creating a list of dictionaries, as this will go fast than creating a data frame and appending content in the loop. Then we will loop over each link in our list from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "print(f\"Starting extraction for {len(links)} links...\")\n",
    "\n",
    "for link in links:\n",
    "    try: # defensive coding - if it doesn't work, returns message below\n",
    "        \n",
    "        # Be polite to the server by building a lag between requests\n",
    "        time.sleep(1) \n",
    "        \n",
    "        # Fetch the content\n",
    "        page_res = requests.get(link)\n",
    "        page_soup = BeautifulSoup(page_res.text, 'lxml')\n",
    "        \n",
    "        # Extract Title and Author from <title>\n",
    "        full_title = page_soup.title.string if page_soup.title else \"\" # dealing with unexpected cases\n",
    "        # Split by \"Guest post by \"\n",
    "        if \"Guest post by \" in full_title: # dealing with unexpected cases\n",
    "            title_part, author_name = full_title.split(\"Guest post by \", 1)\n",
    "            # Remove trailing colon/spaces from title\n",
    "            title_part = title_part.strip().rstrip(':')\n",
    "        else:\n",
    "            title_part = full_title\n",
    "            author_name = \"Unknown\"\n",
    "\n",
    "        # Extract Main Content and Author Bio \n",
    "        # Find the div containing the post body\n",
    "        content_div = page_soup.find('div', class_='text aem-GridColumn aem-GridColumn--default--12')\n",
    "\n",
    "        # Set blank in case following code doesn't work as intended\n",
    "        post_text = \"\"\n",
    "        author_site = \"\"\n",
    "        institution = \"\"\n",
    "        \n",
    "        if content_div:\n",
    "            # Find all <p> tags inside this div\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            \n",
    "            if len(paragraphs) > 2:\n",
    "                # The first <p> tag is the \"This is the Xth post...\" intro\n",
    "                # The last <p> tag is the author bio\n",
    "                main_body_paragraphs = paragraphs[1:-1]\n",
    "                \n",
    "                # Combine main text into one string\n",
    "                post_text = \"\\n\".join([p.get_text() for p in main_body_paragraphs])\n",
    "                \n",
    "                # Link: Extract the href from the <a> tag in the last paragraph\n",
    "                bio_link = paragraphs[-1].find('a')\n",
    "                if bio_link:\n",
    "                    author_site = bio_link.get('href')\n",
    "                \n",
    "                # Institution: Extract text after \"at \" and clean &nbsp; in the last paragraph\n",
    "        \t    # in Python, the HTML entity &nbsp; is represented as \\xa0. \n",
    "        \t    # This line replaces those \"non-breaking spaces\" with standard spaces\n",
    "                bio_text = paragraphs[-1].get_text().replace('\\xa0', ' ') # Remove &nbsp;\n",
    "                if \"at \" in bio_text:\n",
    "                    # Capture everything after \"at \" until the period\n",
    "                    institution = bio_text.split(\"at \", 1)[1].split('.', 1)[0].strip()\n",
    "\n",
    "        # Save to our list\n",
    "        data_list.append({\n",
    "            'URL': link,\n",
    "            'Title': title_part,\n",
    "            'Author': author_name,\n",
    "            'Author_Website': author_site,\n",
    "            'Institution': institution,\n",
    "            'Full_Text': post_text\n",
    "        })\n",
    "        print(f\"Successfully processed: {title_part[:30]}...\")\n",
    "\n",
    "    except Exception as e: # what happens if our scrape doesn't work as intended\n",
    "        print(f\"Error skipping {link}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!! Now let's convert this a data frame to export it in case we want to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "results_df = pd.DataFrame(data_list)\n",
    "results_df.to_csv('Data/world_bank_job_market_blogs_2025.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things we could consider for next steps in text analysis when we get to that part of the class:\n",
    "* Standardizing Text: Stripping punctuation, handling those \"stray\" HTML characters, and lowercasing.\n",
    "* Tokenization: Breaking the posts into individual words or sentences.\n",
    "* Frequency Analysis: Seeing which topics (like \"RCT,\" \"India,\" or \"Labor\") appear most often in the 2025 series.\n",
    "* Contextual Filtering: Finding all sentences that mention \"Nigeria\" or \"Flooding\" to see how researchers are currently discussing the topics we are focused on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scraping downloadable files\n",
    "\n",
    "Another useful application of web scraping is bulk downloading files that are stored following some predictable format. This can save time if you know you have to download many files and don't want to go through the process of navigating to each page and manually clicking on download links.\n",
    "\n",
    "We'll apply these tools to the case of downloading country boundary shape files from [GADM](https://gadm.org/download_country.html). Let's first look at the website and observe what these look like.\n",
    "\n",
    "What do you observe?\n",
    "\n",
    "Now we will set up our code to download shapefiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GADM download page URL\n",
    "base_url = \"https://gadm.org/download_country.html\"\n",
    "\n",
    "# Start a session\n",
    "session = requests.Session()\n",
    "\n",
    "# Get the HTML content of the page\n",
    "response = session.get(base_url)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "soup.prettify()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what country codes to use to build our URLs? We can use the information in the country dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract country codes from the dropdown menu\n",
    "# Observe that the dropdown is formatted with name=\"Country\" as an attribute\n",
    "# Observe that each country is associated with an option value giving the GADM country code\n",
    "country_options = soup.select(\"select[name=country] option\")\n",
    "country_dict = {option.text.strip(): option[\"value\"] for option in country_options if option[\"value\"]}\n",
    "country_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dictionary of country names and associated GADM country values. But these are not exactly what we need to create the URLs for the shapefiles. We need to extract just the first three letters - this is straightforward. Then, we can to loop through the countries we want, find the appropriate download URL, and download shapefiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dict['Brazil'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of countries to download shapefiles for\n",
    "countries = [\"Bangladesh\", \"Brazil\", \"Burundi\"]\n",
    "\n",
    "# Create a directory to store downloaded shapefiles\n",
    "output_dir = \"Data\"\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over the list of desired countries\n",
    "for country in countries:\n",
    "    if country in country_dict:\n",
    "\n",
    "        # Constructing the target URL\n",
    "        country_code = country_dict[country][:3]\n",
    "        download_url = f\"https://geodata.ucdavis.edu/gadm/gadm4.1/shp/gadm41_{country_code}_shp.zip\"\n",
    "\n",
    "        # File path for saving the zip file\n",
    "        file_path = os.path.join(output_dir, f\"gadm41_{country_code}_shp.zip\")\n",
    "\n",
    "        print(f\"Downloading {country} shapefile\")\n",
    "\n",
    "        # Download and save the file\n",
    "        # The stream=True argument keeps the connection open so large files can be downloaded in small pieces.\n",
    "        response = session.get(download_url, stream=True)\n",
    "        if response.status_code == 200: # confirms the file was found\n",
    "        # Memory-efficient download; wb is write binary for a zip file\n",
    "            with open(file_path, \"wb\") as f:\n",
    "        # We go 1Mb (1024*1024 kb) at a time until the file is complete\n",
    "                for chunk in response.iter_content(chunk_size=1048576):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Successfully downloaded: {country}\\n\")\n",
    "        else:\n",
    "            print(f\"Failed to download: {country}\\n\")\n",
    "    else:\n",
    "        print(f\"Country not found in dropdown: {country}\\n\")\n",
    "\n",
    "print(\"All downloads complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done! We've successfully downloaded all the shapefiles we wanted.\n",
    "\n",
    "You can see how this is a powerful tool for bulk downloading data where the URLs follow a common format.\n",
    "\n",
    "We can also potentially make this much faster using parallel processing, downloading multiple countries simultaneously. Below is sample code for how we would do this. This would be much faster if we are interested in downloading data from many different countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fcurrence.concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# def download_country(country):\n",
    "#     # Code for one country, based on the above\n",
    "\n",
    "# # Download up to 5 countries at once\n",
    "# with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#     executor.map(download_country, countries)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (UCA DS Base)",
   "language": "python",
   "name": "ucads_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6f9fe9f4b7182690503d8ecc2bae97b0ee3ebf54e877167ae4d28c119a56988"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
