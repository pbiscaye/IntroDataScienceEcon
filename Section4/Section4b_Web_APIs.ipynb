{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caa532f-d502-4576-9ee6-5ad88ca0c550",
   "metadata": {},
   "source": [
    "# Section 4. Python Web APIs\n",
    "\n",
    "#### Instructor: Pierre Biscaye \n",
    "\n",
    "The objective of this notebook is to introduce you to some basic steps for extracting data from the web with APIs using Python, focusing on the New York Times API as a case study. The content of this notebook is taken from UC Berkeley D-Lab's Python Web APIs [course](https://github.com/dlab-berkeley/Python-Web-APIs).\n",
    "\n",
    "### Learning Objectives\n",
    "1. The New York Times API\n",
    "2. Top stories API\n",
    "3. Most Popular API\n",
    "4. Article Search API\n",
    "5. Examples of Data Analysis\n",
    "6. Another example: IMF API\n",
    "\n",
    "### Libraries loaded\n",
    "* matplotlib.pyplot\n",
    "* datetime\n",
    "* time\n",
    "* numpy\n",
    "* pandas\n",
    "* configparser\n",
    "* os\n",
    "* getpass\n",
    "* requests\n",
    "* pynytimes\n",
    "* Counter\n",
    "* seaborn\n",
    "* vadersentiment\n",
    "* ast\n",
    "* scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0379359-694d-491c-ba18-fa80ea3c65bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import basic required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2eeb34-dab0-464c-8fca-568624fe2cbb",
   "metadata": {},
   "source": [
    "# 1. Setting up The New York Times API\n",
    "\n",
    "We are going to use the NYT API to demonstrate how Web APIs can be used to access useful information in an easy way. Before proceeding with this lesson, you should have already set up an API key following the instructions in Web APIs overview slides. Copy that API key now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639eb32-15e0-48fc-9cb5-3bd804fae0f9",
   "metadata": {},
   "source": [
    "## Handling API Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da084101-19bc-46c1-9722-317d0514da5e",
   "metadata": {},
   "source": [
    "API keys are sensitive data! You **do not** want to accidentally share them publicly. This includes not pasting your API key directly into a notebook where it can be read.\n",
    "\n",
    "The following cell will:\n",
    "\n",
    "1. First try to obtain previously saved credentials by loading with `configparser`;\n",
    "2. If not found, use `getpass` to request the credentials from the user (which works in notebooks as an input prompt);\n",
    "3. Then save those user-inputted credentials using configparser to `~/.notebook-api-keys` which is outside of the directory for this notebook so it doesn't accidentally get uploaded publicly. In particular, it will save this to your main user directory.\n",
    "   \n",
    "Run the following cell and add the API Key you just created when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9823a-4e85-41ff-b321-4f9821359821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "def get_api_key(api_name):\n",
    "    config_file_path = os.path.expanduser(\"~/.notebook-api-keys\")\n",
    "    config = configparser.ConfigParser(interpolation=None)  # Disable interpolation to avoid issues with special characters\n",
    "    \n",
    "    # Try reading the existing config file\n",
    "    if os.path.exists(config_file_path):\n",
    "        config.read(config_file_path)\n",
    "    \n",
    "    # Check if API key is present\n",
    "    if config.has_option(\"API_KEYS\", api_name):\n",
    "        # Ask if the user wants to update the key\n",
    "        update_key = input(f\"An API key for {api_name} already exists. Do you want to update it? (y/n): \").lower()\n",
    "        if update_key == 'n':\n",
    "            return config.get(\"API_KEYS\", api_name)\n",
    "    \n",
    "    # If no key exists or user opts to update, prompt for the new key\n",
    "    api_key = getpass(f\"Enter your {api_name} API key: \")\n",
    "\n",
    "    # Save the API key in the config file\n",
    "    if not config.has_section(\"API_KEYS\"):\n",
    "        config.add_section(\"API_KEYS\")\n",
    "    config.set(\"API_KEYS\", api_name, api_key)\n",
    "    \n",
    "    with open(config_file_path, \"w\") as f:\n",
    "        config.write(f)\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "# Example usage to retrieve the NYT API key\n",
    "api_key = get_api_key(\"NYT\")\n",
    "\n",
    "print(\"NYT API key retrieved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dfdbb9-f4a4-4c3b-aaf4-a5a777900530",
   "metadata": {},
   "source": [
    "### Efficient loading of API keys\n",
    "\n",
    "Note that `~/.notebook-api-keys` can save keys for multiple APIs.\n",
    "\n",
    "We would need to run a version of this any time you have a new API key to save, or want to use a particular API tool. With that in mind, let's do a few things:\n",
    "1. Generalize the code (so it's not specific to the NYT API).\n",
    "2. Make \"updating\" the key an optional parameter so that it doesn't get asked each time.\n",
    "3. Save the code to a `.py` script you can then import in other notebooks.\n",
    "\n",
    "Below is a cleaner updated version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbf6bb-4475-40d2-9bf5-fd332d86d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "def get_api_key(api_name, force_update=False):\n",
    "    \"\"\"\n",
    "    Retrieves an API key. Prompts user only if the key is missing \n",
    "    or if force_update is True.\n",
    "    \"\"\"\n",
    "    config_file_path = os.path.expanduser(\"~/.notebook-api-keys\")\n",
    "    config = configparser.ConfigParser(interpolation=None)\n",
    " \n",
    "    # Try reading the existing config file\n",
    "    if os.path.exists(config_file_path):\n",
    "        config.read(config_file_path)\n",
    "    \n",
    "    # Check if key exists AND we aren't forcing an update\n",
    "    if config.has_option(\"API_KEYS\", api_name) and not force_update:\n",
    "        return config.get(\"API_KEYS\", api_name)\n",
    "    \n",
    "    # Otherwise, prompt for the key\n",
    "    api_key = getpass(f\"Enter your {api_name} API key: \")\n",
    "\n",
    "    # Save the API key in the config file\n",
    "    if not config.has_section(\"API_KEYS\"):\n",
    "        config.add_section(\"API_KEYS\")\n",
    "    config.set(\"API_KEYS\", api_name, api_key)\n",
    "    \n",
    "    # Save the updated config file\n",
    "    with open(config_file_path, \"w\") as f:\n",
    "        config.write(f)\n",
    "    \n",
    "    return api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571113b-9ced-4789-9378-ec844b8f4315",
   "metadata": {},
   "source": [
    "Save this as `api_utils.py` file in this notebook's directory, then try the below code.\n",
    "\n",
    "Note that this code runs *silently*, retrieving the key without displaying anything, unless there is no key or you are forcing an update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d200d0-cf4c-45d3-8a1f-66d35c43d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_utils import get_api_key\n",
    "\n",
    "# This will check the hidden file. If it's not there, it asks. \n",
    "# If it IS there, it just returns it silently.\n",
    "nyt_key = get_api_key(\"NYT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3361b80e-b821-4056-b62e-cbdd0019f43d",
   "metadata": {},
   "source": [
    "How can you check that this code returned the same thing as our original code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a849e1-b711-470e-b042-4dc57e654885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b395a-443f-4e40-91f7-8fcb5c33bbbe",
   "metadata": {},
   "source": [
    "## Loading `pynytimes`\n",
    "\n",
    "To access the NYTimes' databases, we'll be using a third-party library called [pynytimes](https://github.com/michadenheijer/pynytimes). This package provides an easy to use tool for accessing the wealth of data hosted by the Times.\n",
    "\n",
    "We installed this library using pip in our `environment.yaml` file. \n",
    "\n",
    "Let's go ahead import the library and initialize a connection to their servers using our api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce57534-fe5d-45a4-bb97-f2df8dc3d9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the NYTAPI object which we'll use to access the API\n",
    "from pynytimes import NYTAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a6dfa-9343-42b1-9014-165ae7507b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Intialize the NYT API class into an object using your API key\n",
    "nyt = NYTAPI(api_key, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13137c6-c776-4da0-b1a2-195726244be5",
   "metadata": {},
   "source": [
    "We are now ready to make some API calls!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deedf92b-e0bb-4d6a-996c-3c3b4078a7cf",
   "metadata": {},
   "source": [
    "Now that we've established a connection to New York Times' rich database, let's go over what kind of data and privileges we have access to.\n",
    " \n",
    " ### APIs\n",
    "\n",
    "[Here is the collection of the APIs the NYT gives us:](https://developer.nytimes.com/apis)\n",
    "\n",
    "- [Archive metadata](https://developer.nytimes.com/docs/archive-product/1/overview): Returns an array of NYT articles for a given month, going back to 1851.\n",
    "- [Article search](https://developer.nytimes.com/docs/articlesearch-product/1/overview): Look up articles by keyword. You can refine your search using filters and facets.\n",
    "- [Books](https://developer.nytimes.com/docs/books-product/1/overview): Provides information about book reviews and The New York Times Best Sellers lists.\n",
    "- [Most popular articles](https://developer.nytimes.com/docs/most-popular-product/1/overview): Provides services for getting the most popular articles on NYTimes.com based on emails, shares, or views.\n",
    "- [RSS feeds](https://developer.nytimes.com/docs/rss-api/1/overview): Returns articles ranked on the section fronts (Arts, HomePage, World, ...)\n",
    "- [Times Wire](https://developer.nytimes.com/docs/timeswire-product/1/overview): Get links and metadata for Times' articles as soon as they are published on NYTimes.com. The Times Newswire API provides an up-to-the-minute stream of published articles.\n",
    "- [Top stories](https://developer.nytimes.com/docs/top-stories-product/1/overview): Returns an array of articles currently on the specified section \n",
    "\n",
    "We will look at a few of these today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a18eac-436d-43ef-8151-6bfc85a74a8d",
   "metadata": {},
   "source": [
    "# 2. Top Stories API\n",
    "\n",
    "Let's look at the top stories of the day. All we have to do is call a single method on the `nyt` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d14b2a-67f3-42b6-b383-1eb78962a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the top stories from the home page\n",
    "top_stories = nyt.top_stories()\n",
    "\n",
    "print(f\"top_stories is a list of length {len(top_stories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724e114-fe34-4471-9ffe-310f9d921a7e",
   "metadata": {},
   "source": [
    "When working with a new API, a good way to establish an understanding of the data is to inspect a single object in the collection. Let's grab the first story in the list and inspect its attributes and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744cb9e-d991-4c2e-97bd-d12aa5912119",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_story = top_stories[0]\n",
    "top_story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0fa06-969c-4e20-b392-3e4adb298a7d",
   "metadata": {},
   "source": [
    "This is pretty typical output for data pulled from an API. We are looking at a list of nested JSON dictionaries.\n",
    "\n",
    "We are provided a diverse collection of data for the article ranging from the expected (title, author (byline), date, section), to keywords (organized by facet), and included media. Notice that the full article itself is not included - the API does not provide that to us.\n",
    "\n",
    "We can call specific elements of these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5908a-3893-4560-a1a9-170eda9e4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_story['title'])\n",
    "print(top_story['org_facet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73610bbb-e547-4dac-9d55-be815fba4009",
   "metadata": {},
   "source": [
    "## Organizing the API Results into a `pandas` DataFrame\n",
    "\n",
    "In order to conduct subsequent data analysis, we need to convert the list of JSON data to a `pandas` DataFrame. `pandas` allows us to simply pass in the JSON list and produce a clean table in one line of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab0d546-3798-4ed9-bb8c-15bfd6ad5333",
   "metadata": {},
   "source": [
    "First, let's see what happens when we pass in `top_stories` to `pd.json_normalize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176bb5a8-53db-4244-a525-bd836b902ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrmae\n",
    "df = pd.json_normalize(top_stories)\n",
    "# View the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e6202-f3f8-4d43-9601-c0fb0404ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the metadata\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241d131-f695-4aaa-bcef-104f57761099",
   "metadata": {},
   "source": [
    "For the most part, `pandas` does a good job of producing a table where:\n",
    "\n",
    "- The columns correspond with the JSON dictionary keys from our API call.\n",
    "- The number of rows matches the number of articles.\n",
    "- Each cell holds the corresponding value found under that article's dictionary key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f2a85-f044-4531-b96d-9c7be39787cf",
   "metadata": {},
   "source": [
    "What can we do with this? As an example, let's pull the information on individual 'persons' reported in the top stories by the API, and plot the frequency with which different persons appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e0ab6-f458-4915-8ab0-104c18567b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of lists into a single list\n",
    "all_words = [word for sublist in df['per_facet'] for word in sublist]\n",
    "\n",
    "# Count occurrences of each unique string\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Get the 10 most common strings\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "# Convert to DataFrame for easy plotting\n",
    "top_10_df = pd.DataFrame(top_10_words, columns=[\"word\", \"count\"])\n",
    "\n",
    "# Truncate long strings for readability on plot\n",
    "top_10_df[\"short_word\"] = top_10_df[\"word\"].apply(lambda x: x[:15] + \"...\" if len(x) > 15 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9736ee5-491c-44e8-b23c-737f1c7b1f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=top_10_df, x=\"short_word\", y=\"count\", hue=\"word\", palette=\"viridis\", legend=False)\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 Most Common People Named in Top Stories\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9dc2ec-1028-4904-b6d3-c130bf66f834",
   "metadata": {},
   "source": [
    "The `top_stories` method has a single parameter called `section` that defaults to \"home\".\n",
    "\n",
    "If we are interested in a specific section, we can pass in one of the following tags into the `section` parameter:\n",
    "```arts```, ```automobiles```, ```books```, ```business```, ```fashion```, ```food```, ```health```, ```home```, ```insider```, ```magazine```, ```movies```, ```national```, ```nyregion```, ```obituaries```, ```opinion```, ```politics```, ```realestate```, ```science```, ```sports```, ```sundayreview```, ```technology```, ```theater```, ```tmagazine```, ```travel```, ```upshot```, and ```world```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1f7d3-3e0b-414e-a32d-32ef975c89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_business = nyt.top_stories('business')\n",
    "len(top_business)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f7c29-1883-4cec-9439-5acb853352a5",
   "metadata": {},
   "source": [
    "Write some code to convert these results into a dataframe, and adapt the code to above to plot the top 10 most commonly names organizations in these stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590c991-769f-4c02-9be6-e031da1fcf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e8b137d-4365-4994-beee-04b8e06cc9cf",
   "metadata": {},
   "source": [
    "# 3. Most Popular API\n",
    "\n",
    "Retrieving the most viewed and shared articles is also quite simple. The `days` parameter returns the most popular articles based on the last $N$ days. Keep in mind, however, that `days` can only take on one of three values: 1, 7, or 30.\n",
    "\n",
    "Below we will retreive the most viewed articles for today.\n",
    "\n",
    "Note that each time we run a method on `nyt`, this is referred to as an **API call**. For the NYT Article API, we have somewhat strict rate limits: 500 requests per day and 5 requests per minute (recently changed from 10 per minute and 4000 per day). You should always bear this in mind when working with APIs, and structure your code to ensure you do not exceed your limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959dc92b-c33f-41b5-b271-15eb17a5a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the most viewed articles for today.\n",
    "# The days parameter defaults to 1\n",
    "most_viewed_today = nyt.most_viewed()\n",
    "print(f\"Title: {most_viewed_today[0]['title']}\")\n",
    "print(f\"Section: {most_viewed_today[0]['section']}\")\n",
    "most_viewed_today[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684faea3-9fab-499c-8869-b99747b04d86",
   "metadata": {},
   "source": [
    "For this piece of data, we can consult a guide or what's known as a schema to understand the information at our finger tips.\n",
    "\n",
    "The [Most Viewed Schema](https://developer.nytimes.com/docs/most-popular-product/1/types/ViewedArticle) can answer any questions we may have about the data provided by this API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210bc5f1-037d-4c49-85e1-186bfbc946b1",
   "metadata": {},
   "source": [
    "| Attribute      | Data Type | Definition      |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| url      | string       | Article's URL.       |\n",
    "| adx_keywords   | string        | Semicolon separated list of keywords.        |\n",
    "| column   | string        | Deprecated. Set to null.        |\n",
    "| section   | string        | Article's section (e.g. Sports).        |\n",
    "| byline   | string        | Article's byline (e.g. By Thomas L. Friedman).        |\n",
    "| type   | string        | Asset type (e.g. Article, Interactive, ...).        |\n",
    "| title   | string        | Article's headline (e.g. When the Cellos Play, the Cows Come Home).        |\n",
    "| abstract   | string        | Brief summary of the article.|\n",
    "| published_date   | string        | When the article was published on the web (e.g. 2021-04-19).        |\n",
    "| source   | string        | Publisher (e.g. New York Times).        |\n",
    "| id   | integer        | Asset ID number (e.g. 100000007772696).        |\n",
    "| asset_id   | integer        | Asset ID number (e.g. 100000007772696).        |\n",
    "| des_facet   | array        | Array of description facets (e.g. Quarantine (Life and Culture)).        |\n",
    "| org_facet   | array        | Array of organization facets (e.g. Sullivan Street Bakery).        |\n",
    "| per_facet   | array        | Array of person facets (e.g. Bittman, Mark).        |\n",
    "| geo_facet   | array        | Array of geographic facets (e.g. Canada).        |\n",
    "| media   | array        | Array of images.        |\n",
    "| media.type   | string        | Asset type (e.g. image).        |\n",
    "| media.subtype   | string        | Asset subtype (e.g. photo).        |\n",
    "| media.caption   | string        | Media caption        |\n",
    "| media.copyright   | string        | Media credit        |\n",
    "| media.approved_for_syndication   | boolean        | Whether media is approved for syndication.        |\n",
    "| media.media-metadata   | array        | Media metadata (url, width, height, ...).        |\n",
    "| media.media-metadata.url   | string        | Image's URL.        |\n",
    "| media.media-metadata.format   | string        | Image's crop name     |\n",
    "| media.media-metadata.height   | integer        | Image's height |\n",
    "| media.media-metadata.width   | integer        | Image's width      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681696d-752e-4d11-8f67-af5ae03bb73b",
   "metadata": {},
   "source": [
    "How many stories are provided to us via this function call?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358187a6-420d-4317-9429-5a021edb1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(most_viewed_today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33ce38-daf3-494a-9f0f-12d089321297",
   "metadata": {},
   "source": [
    "To pull most popular articles for the past week and month, we pass the numbers 7 or 30 into `days`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf701a93-68c4-4c0c-ab6d-30487b4da504",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_viewed_week = nyt.most_viewed(days=7)\n",
    "len(most_viewed_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a14916-a9fc-4826-a459-787780e26a41",
   "metadata": {},
   "source": [
    "Note that the API only results in 20 articles, and this is not a parameter we can modify.\n",
    "\n",
    "What is the most viewed article of the last week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2f679-4660-4c03-84b8-5f9257473165",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_viewed_week[0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3d542-3873-4a4f-acb2-c9aeb500887d",
   "metadata": {},
   "source": [
    "What individuals occurred most commonly in the most-viewed articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156bb565-54af-4b8b-b8ee-88bbc6cecc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.json_normalize(most_viewed_week)\n",
    "\n",
    "# Flatten the list of lists into a single list\n",
    "all_words = [word for sublist in df2['per_facet'] for word in sublist]\n",
    "\n",
    "# Count occurrences of each unique string\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Get the 10 most common strings\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "# Convert to DataFrame for easy plotting\n",
    "top_10_df = pd.DataFrame(top_10_words, columns=[\"word\", \"count\"])\n",
    "\n",
    "# Truncate long strings for readability on plot\n",
    "top_10_df[\"short_word\"] = top_10_df[\"word\"].apply(lambda x: x[:15] + \"...\" if len(x) > 15 else x)\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=top_10_df, x=\"short_word\", y=\"count\", hue=\"word\", palette=\"viridis\", legend=False)\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 Most Common People Named in Most Viewed Stories\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518264b-6c85-40aa-bec0-5d7b6937e265",
   "metadata": {},
   "source": [
    "Now let's look at the most *shared* stories. Here we can search by sharing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc4104-fcee-4b2c-b7c0-adfdc6a4f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most shared stories\n",
    "email = nyt.most_shared(days=30, method = 'email')\n",
    "facebook = nyt.most_shared(days=30, method = 'facebook')\n",
    "len(facebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a388a2-1a09-4193-9f42-4893685edbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the structure\n",
    "email[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d231103-c70d-40bd-9980-9703efd207fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique identifier for each story\n",
    "email_ids = [story[\"uri\"] for story in email]\n",
    "facebook_ids = [story[\"uri\"] for story in facebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd81026-c588-40bc-a62e-665dd7e70eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the intersection of unique IDs\n",
    "len(set(email_ids).intersection(set(facebook_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa9175-2c6c-4a0d-b1de-e3968cd81e7a",
   "metadata": {},
   "source": [
    "**Question**: How do we interpret the result of the last line of code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60086e2-a79b-47f3-a480-e58339194f3f",
   "metadata": {},
   "source": [
    "# 4. Article Search API\n",
    "\n",
    "The previous results are interesting but likely seem a bit restricted. Let's take it up a notch and use the search API to retrieve a set of articles about a particular topic in a chosen period of time.\n",
    "\n",
    "We'll use the `article_search` function. Two relevant parameters include:\n",
    "\n",
    "- `query`: The search query\n",
    "- `results`: Number of articles returned. The default is 10.\n",
    "\n",
    "Let's try pulling the 20 most recent articles about Tanzania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ff11f-7049-488f-9d96-f689c430a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = nyt.article_search(query=\"Tanzania\", results=20)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980bce2-ac96-46b6-bb3a-9d40217cd1e3",
   "metadata": {},
   "source": [
    "We asked for 20 articles, but only got 10! Why is this the case? \n",
    "\n",
    "The `article_search` method now returns results in \"pages\" of 10 articles each. Older versions of `pynytimes` would automatically loop through pages until you hit the targeted number of results. To retrieve more results, we need to do this explicitly, and keep in mind our rate limits: 5 calls per minute and 500 per day.\n",
    "\n",
    "Why is the NYT doing this? They want to prevent people from scraping their entire history for free to train AI models. The free API tier is now much harder to use for bulk downloads than it was even one year ago.\n",
    "\n",
    "We can handle these rate limits and pagination constraints directly by writing a loop and including a sleep timer to make sure we don't make the calls too quickly. \n",
    "\n",
    "Let's run the code below searching for the first 50 results about the Paris Olympics during a specific timeframe last year, and then discuss what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c6b3c-0d1b-4ad2-98e1-8d6f27094b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 1. Setup your parameters\n",
    "query_term = \"Paris Olympics\"\n",
    "begin = datetime(2024, 7, 25) # July 25, 2024\n",
    "end = datetime(2024, 8, 12) # August 12, 2024\n",
    "date_dict = {\"begin\": begin, \"end\": end}\n",
    "all_articles = []\n",
    "\n",
    "# 2. Loop to get 5 pages (10 articles per page = 50 total)\n",
    "for page_num in range(5):\n",
    "    print(f\"Fetching page {page_num}...\")\n",
    "    \n",
    "    try:\n",
    "        # We pass the specific page number in the options\n",
    "        results = nyt.article_search(\n",
    "            query=query_term,\n",
    "            dates=date_dict,\n",
    "            results=10, # Requesting 10 at a time is safer\n",
    "            options={\"page\": page_num}\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            all_articles.extend(results)\n",
    "            print(f\"Successfully retrieved {len(results)} articles.\")\n",
    "        else:\n",
    "            print(f\"No more results found on page {page_num + 1}. Ending loop.\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred on page {page_num + 1}: {e}\")\n",
    "        break\n",
    "\n",
    "    # 3. The \"Polite Scraper\" pause\n",
    "    if page_num < 4:  # No need to sleep after the last page\n",
    "        print(\"Sleeping for 12 seconds to respect rate limits...\")\n",
    "        time.sleep(12)\n",
    "\n",
    "print(f\"\\nFinished! Total articles collected: {len(all_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2f903-afb6-4f74-a542-711c5e3d149e",
   "metadata": {},
   "source": [
    "Let's look at the first 10 main headlines of these articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25dd533-b471-44fa-8739-0e7e0c756255",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [article['headline']['main'] for article in all_articles]\n",
    "headlines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34559fc2-0f44-4444-a19a-28817b103628",
   "metadata": {},
   "source": [
    "\n",
    "Notice that not all article data comes in the same format. Data from the search API is presented differently from that of the Most Popular and Top Stories APIs.\n",
    "\n",
    "Let's look at an example below. We can explore the schemas to understand the content.\n",
    "\n",
    "- [Article Schema](https://developer.nytimes.com/docs/articlesearch-product/1/types/Article)\n",
    "- [Byline](https://developer.nytimes.com/docs/articlesearch-product/1/types/Byline)\n",
    "- [Headline](https://developer.nytimes.com/docs/articlesearch-product/1/types/Headline)\n",
    "- [Keyword](https://developer.nytimes.com/docs/articlesearch-product/1/types/Keyword)\n",
    "- [Multimedia](https://developer.nytimes.com/docs/articlesearch-product/1/types/Multimedia)\n",
    "- [Person](https://developer.nytimes.com/docs/articlesearch-product/1/types/Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3cbdbe-69a1-4ebb-b233-e359658e7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a3695-8199-4773-87a3-b7573a780436",
   "metadata": {},
   "source": [
    "You can see how this could be a powerful tool for data search. But we also see how call limits constrain what we are able to access over a given period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4b3b3-2b4a-4a20-9a79-ca90d90262f8",
   "metadata": {},
   "source": [
    "# 5. Data Analysis\n",
    "\n",
    "Now, we'll perform some analysis on a database of articles published about the 2024 United States presidential election.\n",
    "\n",
    "We will work with a previously queried set of 1919 articles because making the API call in class will take too much time, and will exceed current rate limits. \n",
    "\n",
    "An updated version of the code used to query the articles we'll analyze can be found in the following cell. This code returns only 500 articles to respect the daily maximum. This requires browsing 50 pages of results. At 5 calls per minute, this code would take 10 minutes to run. \n",
    "\n",
    "Feel free to adapt it for future queries. Keep in mind your **API call and rate limits**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d5e83-a645-4b35-9084-fca5dd408363",
   "metadata": {},
   "source": [
    "## Query Using the Article Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a7832-3186-45c6-8b43-e52c99486526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this variable if you'd like to run the query yourself; note it can take a long time to run\n",
    "run_query = False\n",
    "\n",
    "# Only run this code if you're able to wait for the query to finish\n",
    "if run_query:\n",
    "    \n",
    "    # Set parameters\n",
    "    query_term = \"presidential election\"\n",
    "    n_results = 500\n",
    "    pages_to_fetch = n_results // 10\n",
    "    begin = datetime(2024, 9, 7) # September 7, 2020\n",
    "    end = datetime(2024, 11, 7) # November 7, 2020\n",
    "    date_dict = {\"begin\": begin, \"end\": end}\n",
    "\n",
    "    articles = []   \n",
    "\n",
    "    # Run the search\n",
    "    for page in range(pages_to_fetch):\n",
    "        print(f\"Fetching page {page + 1} of {pages_to_fetch}...\")\n",
    "        \n",
    "        try:\n",
    "            # We must pass the page number inside the options dictionary\n",
    "            options_dict = {\n",
    "                \"sort\": \"oldest\",\n",
    "                \"sources\": [\"New York Times\"],\n",
    "                \"type_of_material\": [\"News Analysis\", \"News\", \"Article\", \"Editorial\"],\n",
    "                \"page\": page\n",
    "            }\n",
    "\n",
    "            results = nyt.article_search(\n",
    "                query=query_term,\n",
    "                results=10, # Explicitly ask for one page's worth\n",
    "                dates=date_dict,\n",
    "                options=options_dict\n",
    "            )\n",
    "\n",
    "            if results:\n",
    "                articles.extend(results)\n",
    "            else:\n",
    "                print(\"No more results returned from API. Stopping.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {page + 1}: {e}\")\n",
    "            # If we hit a rate limit error, we want to save what we have so far\n",
    "            break\n",
    "\n",
    "        # The 12-second rate limit pause\n",
    "        if page < (pages_to_fetch - 1):\n",
    "            time.sleep(12)\n",
    "\n",
    "    # Create DataFrame from the collected list\n",
    "    if articles:\n",
    "        df = pd.json_normalize(articles)\n",
    "        \n",
    "        # Ensure 'lead_paragraph' column has no NaN \n",
    "        if 'lead_paragraph' in df.columns:\n",
    "            df['lead_paragraph'] = df['lead_paragraph'].fillna('')\n",
    "        \n",
    "        # Save DataFrame\n",
    "        df.to_csv(\"Data/election2024_articles.csv\", index=False)\n",
    "        print(f\"Successfully saved {len(df)} articles to Data/election2024_articles.csv\")\n",
    "    else:\n",
    "        print(\"No articles were collected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561fe76-6d07-46c5-bda7-9973e7631ed7",
   "metadata": {},
   "source": [
    "Let's load in the previously saved data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb310d22-895a-4dff-8442-247aa8b02b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "election = pd.read_csv(\"Data/election2024_articles.csv\")\n",
    "election.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4aa87-5f4b-42c6-8f6a-c252e91cdc60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect metadata\n",
    "election.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46337e-600b-45a6-bf77-6a858efc1383",
   "metadata": {},
   "source": [
    "## Perform Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is a common task when working with text data. Let's track the sentiment of articles about the election over the two month time period. We'll use the `vadersentiment` package to evaluate the sentiment of each article.\n",
    "\n",
    "According to the [VADER Github Repo](https://github.com/cjhutto/vaderSentiment), \"VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is *specifically attuned to sentiments expressed in social media*.\"\n",
    "\n",
    "We'll start by import the sentiment intensity analyzer from the `vadersentiment` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477c354-ce11-478c-9f99-1a24c0fbc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SentimentIntensityAnalyzer object\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12840c-372b-4b5d-96cc-c8f33c6116ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# Calculate the polarity scores of the lead paragraph \n",
    "election[\"sentiment\"] = election[\"lead_paragraph\"].apply(lambda x: analyzer.polarity_scores(x) if isinstance(x, str) else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a2157-641e-45e9-a9c8-232fc3317ca3",
   "metadata": {},
   "source": [
    "What do you think the above code does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005cb19-9a18-4c99-9835-ec2801cb5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the sentiment column\n",
    "election.sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf1b6c-b87d-40e6-a1ba-26ca80056ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View single row\n",
    "election.sentiment.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b774a12-732b-46a4-aabc-b4c29c8fdb3e",
   "metadata": {},
   "source": [
    "The `compound` score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most negative) and +1 (most positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. We can think of this score as a normalized, weighted composite score. It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. \n",
    "\n",
    "Typical threshold values are:\n",
    "\n",
    "1. **Positive Sentiment**: compound score $\\geq 0.05$\n",
    " \n",
    "2. **Neutral  Sentiment**: $-0.05 <$ compound score $< 0.05$\n",
    " \n",
    "3. **Negative Sentiment**: compound score $\\leq -0.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f20a8-96bc-4009-8424-d427e2b16755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-assign sentiment as the compound score\n",
    "election[\"sentiment\"] = election[\"sentiment\"].apply(lambda x: x[\"compound\"] if isinstance(x, dict) else np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574114c9-7261-4b7e-8494-8f7e962ac6c2",
   "metadata": {},
   "source": [
    "Let's get a sense of the distribution of scores by calculating some summary statistics and plotting a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45dbca-af6f-41fa-a70d-abd008b4ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "election.sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debbce8d-5e49-4b11-add9-0532b8cb2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-1.1, 1.1, 22)\n",
    "election.sentiment.hist(bins=bins, figsize= (9, 7))\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim([-1.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889b643-e6b1-406c-b320-eaafa278a3c0",
   "metadata": {},
   "source": [
    "Finally, using the VADER thresholds for positive, neutral, and negative, we can see how many articles qualify for each of those labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac1185-a853-4f2f-8fd5-eb36d8af1821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of positive, negative, and neutral texts\n",
    "def bin_func(x):\n",
    "    if x > 0.05:\n",
    "        return \"positive\"\n",
    "    elif x < -.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "# Calculate counts\n",
    "election.sentiment.apply(bin_func).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1297f-70a1-48cf-bdac-5e251ee697c2",
   "metadata": {},
   "source": [
    "## Sentiment Over the Course of the Campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db927bc2-514b-4990-8b06-94e9a8650fd7",
   "metadata": {},
   "source": [
    "Let's examine how the compound score evolved over the course of the campaign. Do you have expectations on how this quantity might behave as the election nears? \n",
    "\n",
    "First, let's create a new `pandas` series which tracks the sentiment over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16393da-bd5b-4042-b07c-568b659015b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change pub_date to DatetimeIndex format\n",
    "election[\"pub_date\"] = pd.to_datetime(election[\"pub_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdbe85-27c1-40a2-9693-48955352623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series with publication date as the index and sentiment score as the value\n",
    "sentiment_ts = pd.Series(index= election.pub_date.tolist(),\n",
    "                         data = election.sentiment.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e495e-dc35-447c-b903-d1b4dfde1288",
   "metadata": {},
   "source": [
    "Next, we'll calculate daily and weekly averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f9cb9-6c8d-4c6e-9596-2e7e0e2313ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data with daily averages and weekly averages\n",
    "daily = sentiment_ts.resample(\"d\").mean()\n",
    "weekly = sentiment_ts.resample(\"W\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bd7c7-1bf1-4c43-9f2b-294f7943cfb9",
   "metadata": {},
   "source": [
    "Let's plot the results. Do you notice any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4394c7-ea1d-4583-8a59-8284022ad2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily average sentiment of articles.\n",
    "daily.plot(figsize = (11, 7))\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Sentiment Score\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a741f8-d9a9-47a5-bbef-9c1e629b530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly average sentiment of articles.\n",
    "weekly.plot(figsize = (11, 7))\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Sentiment Score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da9002-dfeb-4c44-bc7b-c358221ba931",
   "metadata": {},
   "source": [
    "## Handling Nested Arrays of Keywords\n",
    "\n",
    "The NY Times has done us a favor in providing named entities in the article API results, thus relieving us of having to do the tagging ourselves. However, the data structure that it comes in can be tricky to handle. Here, we provide a short tutorial showing one way to cleanly extract keyword data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c743c86-6522-41fa-a4ca-50109b9c1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to a sample article's set of keywords\n",
    "election.keywords.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada82a2d-ab47-40e5-9425-d86342a5df10",
   "metadata": {},
   "source": [
    "We see a number of things here:\n",
    "- Each article's keywords are laid out in a list of dictionaries.\n",
    "- A dictionary tell us the name, value, rank, and major of the keyword.\n",
    "- 'Name' gives the category of keyword, with give possibilities: `subject`, `persons`, `glocations`, `organizations`, and `creative_works`.\n",
    "- 'Value' gives the actual keywork or phrase.\n",
    "- 'Rank' indicates the relative importance of the keywork. The ordering of the list corresponds to the ranking.\n",
    "- 'Major' indicates whether the keyword is a primary focus or a secondary reference.\n",
    "- All articles do not all have the same number of rankings.\n",
    "\n",
    "Let's write a function to extract keyword data based on the ranking. This function will be applied over the pandas series of keyword data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08391288-be34-4e06-9195-f5db61c37f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Convert the string representation of the list into actual lists of dictionaries\n",
    "election['keywords'] = election['keywords'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fee0d3-11a4-4cf6-99da-cf7067630c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "election[\"keywords\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1fede-fd8c-432c-b1e1-2a805a1e9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_extractor(data, rank):\n",
    "    \"\"\"Extracts keyword data based on the 'rank' field.\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        for keyword in data:\n",
    "            if isinstance(keyword, dict) and keyword.get(\"rank\") == rank:\n",
    "                return {\"name\": keyword.get(\"name\"), \"value\": keyword.get(\"value\")}\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701072e-8d36-4cab-9ee5-f58e8aed62b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the first, second, and third keywords\n",
    "rank1 = election.keywords.apply(lambda x: rank_extractor(x, 1))\n",
    "rank2 = election.keywords.apply(lambda x: rank_extractor(x, 2))\n",
    "rank3 = election.keywords.apply(lambda x: rank_extractor(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f18704-1d57-4c70-9170-04f644b915dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "rank1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec099e-b5d5-4e2c-b617-78043b28be1e",
   "metadata": {},
   "source": [
    "Let's convert these dictionaries into `pandas` Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ed380-57f2-4dec-8f31-210567a5057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank1 = rank1.apply(pd.Series)\n",
    "rank2 = rank2.apply(pd.Series)\n",
    "rank3 = rank3.apply(pd.Series)\n",
    "rank1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9b4ab-7113-4321-8e3e-82b86effeb6c",
   "metadata": {},
   "source": [
    "Voila! A nice clean format. Now can we conduct some light analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b066f-ce49-4aa4-889c-d79c919a3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent type of keyword in ranking #1\n",
    "rank1.name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a13f9-d941-4787-aa24-2caa650eae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most common keywords in ranking #1:\n",
    "rank1.value.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f29b16-3c06-4f25-88c1-7886767dc703",
   "metadata": {},
   "source": [
    "Let's do some sentiment analysis based on which presidential candidate was the rank 1 keyword.\n",
    "\n",
    "First, we need to join the rank1 names back to the main `election` dataframe and filter specifically for articles where the keyword is one of the candidates' names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fb550-74c5-4c94-a186-f1197d05e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the rank1 values back to your main dataframe\n",
    "df['rank1_name'] = rank1['value']\n",
    "\n",
    "# Create subsets for each candidate\n",
    "trump_articles = df[df['rank1_name'].str.contains(\"Trump\", na=False)]\n",
    "harris_articles = df[df['rank1_name'].str.contains(\"Harris\", na=False)]\n",
    "\n",
    "print(f\"Trump articles: {len(trump_articles)}\")\n",
    "print(f\"Harris articles: {len(harris_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60ca2d-1831-440a-86b6-26d4cad59059",
   "metadata": {},
   "source": [
    "Now let's compare the sentiment distributions using kernel density plots. What do you hypothesize the pattern to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa4225-37b9-4262-8e32-5a74fddd911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the distributions\n",
    "sns.kdeplot(trump_articles['sentiment'], fill=True, label='Trump (Rank 1)', color='red')\n",
    "sns.kdeplot(harris_articles['sentiment'], fill=True, label='Harris (Rank 1)', color='blue')\n",
    "\n",
    "# Adding formatting\n",
    "plt.title(\"Distribution of Sentiment: Trump vs. Harris (Lead Paragraphs)\")\n",
    "plt.xlabel(\"VADER Compound Sentiment Score (-1 to 1)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.axvline(0, color='gray', linestyle='--') # Add a neutral line\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bb936-d63e-4354-8718-2664c3e39212",
   "metadata": {},
   "source": [
    "Suppose voters believe this means the NYT coverage is biased. Let's formally test if the averages are significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0122c-3952-4426-9b0b-a8daa927b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# 1. Prepare the data (removing any NaNs to ensure the test runs)\n",
    "trump_sentiment = trump_articles['sentiment'].dropna()\n",
    "harris_sentiment = harris_articles['sentiment'].dropna()\n",
    "\n",
    "# 2. Run the Independent T-Test\n",
    "# equal_var=False is used for \"Welch's T-test\", which is safer if the \n",
    "# group sizes or variances are different (which they likely are here).\n",
    "t_stat, p_val = stats.ttest_ind(trump_sentiment, harris_sentiment, equal_var=False)\n",
    "\n",
    "# 3. Display the results\n",
    "print(\"--- Statistical Comparison ---\")\n",
    "print(f\"T-statistic: {t_stat:.4f}\")\n",
    "print(f\"P-value: {p_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7afe0c-edde-46a1-8589-04ce787dbbbb",
   "metadata": {},
   "source": [
    "What do we conclude?\n",
    "\n",
    "A lot of articles have neutral sentiment. Could it be that there are differences in more opinionated coverage? What do the results look like if we drop neutral articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73725db3-a6f1-440c-b258-900b473b3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter out neutral scores\n",
    "trump_filtered = trump_articles[(trump_articles['sentiment'] >= 0.05) | (trump_articles['sentiment'] <= -0.05)]['sentiment'].dropna()\n",
    "harris_filtered = harris_articles[(harris_articles['sentiment'] >= 0.05) | (harris_articles['sentiment'] <= -0.05)]['sentiment'].dropna()\n",
    "\n",
    "print(f\"Opinionated Trump articles: {len(trump_filtered)}\")\n",
    "print(f\"Opinionated Harris articles: {len(harris_filtered)}\")\n",
    "\n",
    "# 2. Run the Independent T-Test (Welch's T-test)\n",
    "t_stat_filt, p_val_filt = stats.ttest_ind(trump_filtered, harris_filtered, equal_var=False)\n",
    "\n",
    "# 3. Display the results\n",
    "print(\"\\n--- T-Test Results (Excluding Neutral Articles) ---\")\n",
    "print(f\"T-statistic: {t_stat_filt:.4f}\")\n",
    "print(f\"P-value: {p_val_filt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebed890-3232-4399-ac6d-ebefc948f6d8",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "* APIs allow structured web interactions, often using URLs to query databases and retrieve data.\n",
    "* API keys authenticate users, enabling access to APIs while monitoring and limiting the number of requests.\n",
    "* The NYT API allows users to do things like retrieve top stories, find most shared stories, and search for stories.\n",
    "* Text data acquired through APIs can be analyzed using natural language processing tools such as sentiment analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de54a374-4636-41d0-b66d-a2aae53ab196",
   "metadata": {},
   "source": [
    "# 6. Another example: The IMF API\n",
    "\n",
    "The code in this section draws on material prepared by Lucas Fages.\n",
    "\n",
    "There are many platforms with APIs you can explore. Some of these do not require API keys, and an example of this is the [IMF API](https://www.imf.org/external/datamapper/api/help), which allows you to retrieve country-level time series data.\n",
    "\n",
    "Some useful information:\n",
    "* [Indicator names and descriptions](https://www.imf.org/external/datamapper/api/v1/indicators)\n",
    "* [Countries](https://www.imf.org/external/datamapper/api/v1/countries)\n",
    "\n",
    "We interact with the IMF API using the `requests` package, as in the web scraping notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8b9cb-827d-477d-8f5b-98508d8c1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4112f324-4d3b-42d4-a3a2-94e703976f60",
   "metadata": {},
   "source": [
    "Let's explore the indicators first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b8a2a-4dee-4df9-bc87-9efdcc68b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1 = \"https://www.imf.org/external/datamapper/api/v1/indicators\"\n",
    "\n",
    "query_1 = requests.get(url_1)\n",
    "\n",
    "print(query_1.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c7d20b-e4bd-4d33-bb4a-296c646a241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all available keys (indicators)\n",
    "query_1.json()['indicators'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b2a94-a29e-47a3-8ada-4b04286be49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information about a specific indicator\n",
    "query_1.json()['indicators'][\"rev\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300151f-59cf-453c-aea5-ccf0e0d13691",
   "metadata": {},
   "source": [
    "Let's download time series of government revenue data. We just need to call this specifically in how we set up the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2bd3ec-83b2-44db-89a5-4ed585433204",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_2 = \"https://www.imf.org/external/datamapper/api/v1/rev\"\n",
    "\n",
    "query_2 = requests.get(url_2)\n",
    "\n",
    "print(query_2.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf192e-b161-49db-84b8-3cdc9b2cda17",
   "metadata": {},
   "source": [
    "This returns time series for this variable for all countries. Let's see one time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfaadd-7488-4637-96d2-5b348737837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2.json()['values'][\"rev\"][\"FRA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623eab8f-198f-478b-8d95-6b0540e3f64b",
   "metadata": {},
   "source": [
    "Let's load government expenditures as a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f0e95-4841-4936-8e21-a7887469473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_3 = \"https://www.imf.org/external/datamapper/api/v1/exp\"\n",
    "\n",
    "query_3 = requests.get(url_3)\n",
    "\n",
    "print(query_3.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c91079-2e31-4b56-bc55-f19f93d22509",
   "metadata": {},
   "source": [
    "Now that we have the data in hand, let's save the data for France as a dataframe and plot revenues against expenditures over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ebefb-a193-4f20-ba5b-b6653d80e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_fra = query_3.json()['values'][\"exp\"][\"FRA\"]\n",
    "rev_fra = query_2.json()['values'][\"rev\"][\"FRA\"]\n",
    "# convert these lists of tuples in dataframes :\n",
    "revenues = pd.DataFrame.from_records(list(rev_fra.items()),\n",
    "                             columns = [\"year\", \"revenues\"])\n",
    "\n",
    "expenditures = pd.DataFrame.from_records(list(exp_fra.items()),\n",
    "                             columns = [\"year\", \"expenditures\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d4642-cc4f-40e9-a336-c128c980a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_exp = revenues.merge(expenditures)\n",
    "rev_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5f787-7be8-40c7-8ee1-da35ecb79eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot these time series!\n",
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (UCA DS Base)",
   "language": "python",
   "name": "ucads_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
