{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8. Dimensionality Reduction\n",
    "\n",
    "#### Instructor: Pierre Biscaye \n",
    "\n",
    "The content of this notebook is taken from UC Berkeley D-Lab's Python Machine Learning [course](https://github.com/dlab-berkeley/Python-Machine-Learning).\n",
    "\n",
    "What if there was a way you could reduce the number of dimensions in your data and still retain a significant portion of the critical information in your data--it's 'identity'?\n",
    "\n",
    "That is **dimensionality reduction** in a nutshell. It is a statistical technique that reduces a dataset of `m` dimensions down to `k` while minimizing the loss of information. This technique is useful for generalizating, visualizing, and compressing data.\n",
    "\n",
    "### Sections\n",
    "\n",
    "1. Data and correlations between features\n",
    "2. Principal Component Analysis\n",
    "3. Interpreting PCA: understanding components, explaining variance\n",
    "4. PCA and supervised ML\n",
    "5. t-SNE and interactive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Part of the reason why dimensionality reduction works is because of the redundancy that exists in datasets. If you have a dataset of `N` features that are all highly correlated with another, then you don't really have `N` features worth of signal.\n",
    "\n",
    "Think about the results of a survey asking subjects on their political opinions. The survey asks a respondent to rate how strongly they support/oppose issues on topics such as taxes, abortion, the environment, etc... Opinions on gun control probably correlate with opinions on abortion and opinions on healthcare probably correlate with opinions on military spending.\n",
    "\n",
    "A dimensionality reduction technique could compress the results of this survey down to a single dimension that effectively represents a left-right political spectrum. And because of the multicollinearity in the data, the loss of information wouldn't be equal in proportion to the decrease in dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: World Happiness Report\n",
    "\n",
    "The data for this notebook originates from the [2022 World Happiness report](https://worldhappiness.report/ed/2022/) and was downloaded from this [kaggle repo](https://www.kaggle.com/datasets/ajaypalsinghlo/world-happiness-report-2022). The following data dictionary explains what the variables mean. You can read the full report [here](https://happiness-report.s3.amazonaws.com/2022/WHR+22.pdf).\n",
    "\n",
    "**happiness_score:** The national average response to the question of life evaluations. Question asks “Please imagine a ladder, with steps numbered from 0 at the bottom to 10 at the top. The top of the ladder represents the best possible life for you and the bottom of the ladder represents the worst possible life for you. On which step of the ladder would you say you personally feel you stand at this time?”\n",
    "\n",
    "**gdp:** GDP per capita (variable name gdp) in purchasing power parity (PPP) at constant 2017 international dollar prices.\n",
    "\n",
    "**life_expectancy:** Healthy life expectancies at birth are based on the data extracted from the World Health Organization’s (WHO) Global Health Observatory data repository \n",
    "\n",
    "**social_support:** The national average of the binary responses (either 0 or 1) to the GWP (Gallup World Poll) question “If you were in trouble, do you have relatives or friends you can count on to help you whenever you need them, or not?”\n",
    "\n",
    "**freedom:** Freedom to make life choices is the national average of responses to the GWP question “Are you satisfied or dissatisfied with your freedom to choose what you do with your life?”\n",
    "\n",
    "**generosity:** Generosity is the residual of regressing national average of response to the GWP question “Have you donated money to a charity in the past month?” on GDP per capita.\n",
    "\n",
    "**corruption:** The measure is the national average of the survey responses to two questions in the GWP: “Is corruption widespread throughout the government or not” and “Is corruption widespread within businesses or\n",
    "not?” The overall perception is just the average of the two 0-or-1 responses. The corruption perception at the national level is just the average response of the overall perception at the individual level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness = pd.read_csv(\"Data/world_happiness.csv\")\n",
    "happiness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is most effective when there is significant redundancy in the data. By redundancy, we mean a high level of multicollinearity in the data — frequent high correlations between variables in the data.\n",
    "\n",
    "Let's look at the correlation table for the world happiness data to see if that is the case. We won't include rank because that is perfectly correlated with happiness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = happiness.drop(columns=['rank']).corr()\n",
    "corr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heat map\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# Plot a heatmap using seaborn\n",
    "# Include the mask and correct aspect ratio, and a diverging colormap\n",
    "sns.heatmap(corr, mask=mask, cmap='RdBu', vmax=.8, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Do you observe high multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis \n",
    "\n",
    "Principal component analysis (PCA) is the most commonly used dimensionality reduction algorithm. It works by transforming a high dimensional dataset into a smaller one while still retaining a significant amount of the information. The principal components are the outputted variables that are a linear mixture of the original variables. This transformed data are all uncorrelated with one another by construction, with the most valuable information compressed into the the first few components. \n",
    "\n",
    "![](https://miro.medium.com/max/600/1*e_kBZQz2hsa7de6TxpgJqg.gif)\n",
    "Source: Towards Data Science\n",
    "\n",
    "PCA first aims to understand how the variables of the data differ from their means and if the relationship between the variables and their means varies across variables. So in order to complete this part, PCA calculates the covariance matrix which has the dimensions of N x N features. Here's the formula to calculate covariance:\n",
    "\n",
    "![](https://www.gstatic.com/education/formulas2/443397389/en/covariance_formula.svg)\n",
    "\n",
    "Next up the algorithm calculates the eigenvectors and eigenvalues. These are linear algebra calculations that are calculated from the covariance matrix. The eigenvectors and eigenvalues are used to calculate the principal components.\n",
    "\n",
    "The eigenvectors are defined as the directions of axes of the principal components while the eigenvalue refers to the magnitude of the eigenvector. PCA orders the eigenvalues from greatest to least — this explains why the first component has the most signal.\n",
    "![](https://miro.medium.com/max/600/1*BpwgqgR-dVZSmIPKTaM4JQ.gif)\n",
    "\n",
    "Finally PCA transforms the original data by taking the dot product of the transposed eigenvectors and the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://devopedia.org/images/article/139/4543.1548137789.jpg)\n",
    "\n",
    "Source: Devopedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with our data\n",
    "\n",
    "Let's select the numerical variables and initialize the PCA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = happiness.select_dtypes(\"number\").drop(\"rank\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to scale the data because PCA is sensitive to variables with higher variances/ranges. Therefore we want the algorithm to analyze data with all similar variances to avoid that kind of bias. We'll therefore normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "Xs = scale.fit_transform(X)\n",
    "Xs = pd.DataFrame(Xs, columns=X.columns)\n",
    "Xs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize PCA model and set n_components = 2\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "#Fit and transforms\\\n",
    "Xp = pca.fit_transform(Xs)\n",
    "Xp[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking a value for `n_components` is not required. We can apply pca to our data without setting a value for `n_components` and it will return a dataset with the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "Xp = pca.fit_transform(Xs)\n",
    "Xp[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape\n",
    "Xp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first two components, select the first two columns. Notice that they are the same as when we specified `ncomponents=2`. That is because PCA always calculates all the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overwrite Xp with just its first two columns.\n",
    "Xp = Xp[:, :2]\n",
    "Xp[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interpreting PCA\n",
    "\n",
    "It is not always obvious what the identified principal components represent. We can look at how they correlate with particular variables to get a sense. We can also plot them to see. \n",
    "\n",
    "A great thing about dimensionality reduction is that we can visualize highly dimensional data in 2D or 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put pca data into dataframe\n",
    "Xp = pd.DataFrame(Xp, columns=[\"comp1\", \"comp2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the pca data with a 2D scatter plot\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.scatter(x=Xp[\"comp1\"], y = Xp[\"comp2\"], s=80)\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add continent label to the chart to see if that helps with interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add continent to the pca dataset\n",
    "Xp[\"continent\"] = happiness[\"continent\"].tolist()\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "for continent in Xp.continent.unique():\n",
    "    data = Xp[Xp.continent == continent]\n",
    "    \n",
    "#     data.plot.scatter(x = \"comp1\", y = \"comp2\", label = continent)\n",
    "    plt.scatter(x=data[\"comp1\"], y = data[\"comp2\"], label = continent, s=80)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend(fontsize = \"large\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Based on these patterns and your expectations about likely differences in well-being across continents, are higher or lower values of these components likely to be associated with higher national well-being?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained variance\n",
    "\n",
    "Explained variance informs us how much of the original signal/identity each component possesses from the original dataset. The `explained_variance_ratio_` is a normalized version of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance ratio of each component\n",
    "exp_var_ratio = pca.explained_variance_ratio_.round(3)\n",
    "exp_var_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cumulative sum of the explained variance ratio of each component\n",
    "exp_var_ratio_cs = exp_var_ratio.cumsum()\n",
    "exp_var_ratio_cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two components (a quarter of the original dimensions) net almost three-quarters of the data's explained variance.\n",
    "\n",
    "Cutting the number of dimensions by more than half leaves us with 83.2% of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.bar(x=range(1, exp_var_ratio.shape[0] +1), height=exp_var_ratio, color = \"b\")\n",
    "plt.plot(range(1, exp_var_ratio.shape[0] +1), exp_var_ratio_cs, c = \"red\", marker = \"*\")\n",
    "plt.xlabel(\"N Components\", fontsize = 16)\n",
    "plt.ylabel(\"Explained Variance Ratio\",fontsize = 16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors\n",
    "\n",
    "[Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html): Eigenvectors are \"Principal axes in feature space, representing the directions of maximum variance in the data. Equivalently, the right singular vectors of the centered input data, parallel to its eigenvectors. The components are sorted by `explained_variance_`\"\n",
    "\n",
    "The `components_` attribute of PCA object is where the eigenvectors are stored. It is a matrix whose dimensions are equal to the number of components specificed by number of original dimensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chart we visualize each feature's first and second eigenvector on a two dimensional plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 11))\n",
    "plt.grid(True)\n",
    "cols = X.columns\n",
    "for i in range(len(cols)):\n",
    "    x = pca.components_[i][0]\n",
    "    y = pca.components_[i][1]\n",
    "    plt.arrow(0, 0, x, y, color = \"blue\", width = 0.005, alpha = .3)\n",
    "    plt.annotate(cols[i], xy = (x, y), fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML with PCA\n",
    "\n",
    "One thing that also makes dimensionality reduction useful is that we can train machine learning models on dimensionality-reduced data and achieve a similar performance.\n",
    "\n",
    "Let's analyze the relationship between principal components and the performance of a machine learning model. \n",
    "\n",
    "The plan\n",
    "1. Train a machine learning model on the untransformed data — this is our baseline. Observe accuracy score.\n",
    "2. Train a machine learning model on the PCA transformed data for every value between 1 and the number of dimensions.\n",
    "3. Plot the number of components used to train a model versus their performance in terms of accuracy and time elapsed in training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diamond Data Dictionary\n",
    "\n",
    "We will work with a dataset on characteristics of diamonds. The objective will be predict the price of a diamond based on its characteristics, using ridge regression. \n",
    "\n",
    "**carat:** weight of the diamond (0.2--5.01)\n",
    "\n",
    "**cut:** quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "\n",
    "**color:** diamond colour, from J (worst) to D (best)\n",
    "\n",
    "**clarity:** a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "\n",
    "**depth**: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
    "\n",
    "**table**: width of top of diamond relative to widest point (43--95)\n",
    "\n",
    "**price:** price in US dollars (\\$326--\\$18,823)\n",
    "\n",
    "**x**: length in mm (0--10.74)\n",
    "\n",
    "**y**: width in mm (0--58.9)\n",
    "\n",
    "**z**: depth in mm (0--31.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in diamonds data. \n",
    "#We will take a random sample of 15k rows to save time.\n",
    "\n",
    "diamonds = (\n",
    "    pd.read_csv(\"data/diamonds.csv\", index_col=[0])\n",
    "    .sample(n=15000, random_state=30)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "from sklearn.model_selection import  cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#grab X and y\n",
    "X = diamonds.drop(\"price\", axis = 1)\n",
    "y = diamonds[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To keep things simple let's use just numerical features\n",
    "# That means we drop color, clarity, and cut\n",
    "X = X.select_dtypes(\"number\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the six dimensions we will start from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data\n",
    "\n",
    "diamonds_scaler = StandardScaler()\n",
    "Xs = diamonds_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's establish the baseline performance when including these original features. We'll be deriving the cross-validated (to identify the optimal penalty) accuracy score for the non-PCA dataset and we'll observe the time taken to train the model. For convenience we'll do a simple search of just a few possible penalty parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# Create ridge model, with CV\n",
    "ridge_cv = RidgeCV(\n",
    "    # Which alpha values to test for?\n",
    "    alphas=np.arange(2,14,2),\n",
    "    # Number of folds\n",
    "    cv=5)\n",
    "# Fit model\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "end=time.time()\n",
    "elapsed=end-start\n",
    "# Evaluate model\n",
    "baseline_score=ridge_cv.score(X_test, y_test)\n",
    "print(ridge_cv.score(X_test, y_test))\n",
    "baseline_time=round(elapsed,3)\n",
    "print(\"Elapsed time: \",round(elapsed,3),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply PCA to our data and train a model for a range of components between 1 and the number of dimensions of our data. We collect the cross-validated accuracy scores and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize PCA model\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA on the training data and transform\n",
    "Xpca = pca.fit_transform(X_train)\n",
    "# Apply the same PCA transform to the test data\n",
    "Xtest_pca=pca.transform(X_test)\n",
    "\n",
    "acc_scores = []\n",
    "\n",
    "times = []\n",
    "\n",
    "components_range = np.arange(1, X_train.shape[1] + 1)\n",
    "\n",
    "for comp in components_range:\n",
    "    #Slice the columns of the Xpca matrix using comp\n",
    "    pca_features = Xpca[:, :comp]\n",
    "    pcatest_features = Xtest_pca[:, :comp]\n",
    "    \n",
    "    #cross-validate\n",
    "    start = time.time()\n",
    "    ridge_cv = RidgeCV(\n",
    "        # Which alpha values to test for?\n",
    "        alphas=np.arange(2,14,2),\n",
    "        # Number of folds\n",
    "        cv=5)\n",
    "    \n",
    "    # Fit model\n",
    "    ridge_cv.fit(pca_features, y_train)    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    acc_scores.append(ridge_cv.score(pcatest_features, y_test))\n",
    "    times.append(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize = (17, 8))\n",
    "\n",
    "fig.tight_layout(pad = 3)\n",
    "ax1.bar(components_range, acc_scores)\n",
    "ax1.set_title(\"PCA Accuracy Scores\", fontsize= 20)\n",
    "ax1.set_xlabel(\"N Components\", fontsize = 15)\n",
    "ax1.set_ylabel(\"CV Accuracy\", fontsize = 15)\n",
    "ax1.hlines(y = baseline_score, xmin = components_range.min(), xmax = components_range.max(),\n",
    "          colors = \"black\", linestyles = \"dashed\")\n",
    "ax1.annotate(text = 'Baseline Accuracy', xy = (1, baseline_score*1.05),size = 14)\n",
    "ax1.set_ylim(bottom=0, top = baseline_score*1.2)\n",
    "ax1.grid(False)\n",
    "\n",
    "\n",
    "ax2.set_title(\"PCA Times\", fontsize= 20)\n",
    "ax2.bar(components_range, times, color = \"#1D8A99\")\n",
    "ax2.set_xlabel(\"N Components\", fontsize = 15)\n",
    "ax2.set_ylabel(\"Seconds Elapsed\", fontsize = 15)\n",
    "ax2.hlines(y = baseline_time, xmin = components_range.min(), xmax = components_range.max(),\n",
    "          colors = \"black\", linestyles = \"dashed\")\n",
    "ax2.annotate(text = 'Baseline Time Elapsed', xy = (1, baseline_time*1.05),size = 14)\n",
    "ax2.set_ylim(bottom=0, top = baseline_time*1.2)\n",
    "ax2.grid(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do these two charts tell us about the tell timing-vs-performance tradeoff when we train a model on PCA components? What might be the implications if we had a dataset with millions of variables and potentially hundres of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Predicting Diamond Cut\n",
    "\n",
    "Suppose we were interested in predicting the cut of a diamond, rather than its price. This is a categorical variable. Using logistic regression but still keeping just numeric variables in the features dataset, repeat the above process to test the potential time benefits of using PCA for this kind of analysis.\n",
    "\n",
    "The below code could be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize k-fold with 5 splits\n",
    "# kf = KFold(n_splits=5)\n",
    "# #Intialize model\n",
    "# lr = LogisticRegression(random_state=1, max_iter = 300)\n",
    "# #Run cross validation\n",
    "# baseline_score = cross_val_score(lr, Xs, y, cv=kf, scoring = \"accuracy\").mean().round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. t-SNE\n",
    "\n",
    "![](https://tse2.mm.bing.net/th?id=OIP.OvotzpNWbWE8wZht7Pw3_QHaGF&w=690&c=7&pid=Api&p=0)\n",
    "\n",
    "t-SNE is another popular dimensionality reduction algorithm that is really popular when visualizing high-dimensional plots into a 2D or 3D space. What gives it an advantage over PCA is that it's more suitable for non-linear data.\n",
    "\n",
    "t-SNE works by producing a joint probability distribution that effectively measures correlations between data. The basis for this distribution comes from calculating the euclidean distance between every datapoint pair, the smaller the distance means the higher probability that two points are similar.\n",
    "\n",
    "Next t-SNE initializes the output dimensions with random data and through a process to similar to gradient descent continously transforms the random data so that its joint probability distribution is as similar possible to that of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding t-SNE\n",
    "\n",
    "The most important parameter in t-SNE is *perplexity* which is used to set the number of neighbors that are used in calculating the joint distributions mentioned previously. The following image shows four differents of t-SNE applied to the same dataset but with varying values for its perplexity parameter.\n",
    "\n",
    "![](https://tse2.mm.bing.net/th?id=OIP.C_e2LzgeM_TC7LcC15U_QQHaGj&w=690&c=7&pid=Api&p=0)\n",
    "\n",
    "\n",
    "For more how t-SNE works and how parameters impact its transformation check out [this excellent tutorial](https://distill.pub/2016/misread-tsne/).\n",
    "\n",
    "Before we use t-SNE, let's all first install the [ploty-express plotting](https://plotly.com/python/plotly-express/) package which will let us create interactive visualizations.\n",
    "\n",
    "If you do not have plotly-express installed, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly-express"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply t-SNE to the world happiness data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine X to drop rank, country, and continent\n",
    "X = happiness.iloc[:, 2:-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We imported TSNE at the beginning\n",
    "tsne = TSNE(n_components=2, perplexity=40, random_state=2, learning_rate=1)\n",
    "\n",
    "Xt = tsne.fit_transform(X)\n",
    "Xt = pd.DataFrame(Xt, columns=[\"tsne1\", \"tsne2\"])\n",
    "Xt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-produce the earlier plot where visualize the country dots color-encoded by continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add continent to the pca dataset\n",
    "Xt[\"continent\"] = happiness[\"continent\"].tolist()\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "for continent in Xt.continent.unique():\n",
    "    data = Xt[Xt.continent == continent]\n",
    "    \n",
    "#     data.plot.scatter(x = \"comp1\", y = \"comp2\", label = continent)\n",
    "    plt.scatter(x=data[\"tsne1\"], y = data[\"tsne2\"], label = continent, s=80)\n",
    "    plt.xlabel(\"TSNE 1\")\n",
    "    plt.ylabel(\"TSNE 2\")\n",
    "    plt.legend(fontsize = \"large\");\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scale = StandardScaler()\n",
    "Xs = scale.fit_transform(X)\n",
    "Xs = pd.DataFrame(Xs, columns=X.columns)\n",
    "#Compare to the PCA plot\n",
    "pca = PCA(n_components=2)\n",
    "#Fit and transform\n",
    "Xp = pca.fit_transform(X)\n",
    "Xp = pd.DataFrame(Xp, columns=[\"comp1\", \"comp2\"])\n",
    "Xp[\"continent\"] = happiness[\"continent\"].tolist()\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "for continent in Xp.continent.unique():\n",
    "    data_p = Xp[Xp.continent == continent]    \n",
    "    plt.scatter(x=data_p[\"comp1\"], y = data_p[\"comp2\"], label = continent, s=80)\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.legend(fontsize = \"large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How do the figures appear to relate to each other? Are they capturing similar dimensions?\n",
    "\n",
    "### Interactive plots\n",
    "\n",
    "These charts are nice, but what would really be great would be to be able to hover the mouse over the dots to reveal more information about the country dots.\n",
    "\n",
    "That's where plotly-express comes in! Let's create an interactive 2D plot using plotly-express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotly-express\n",
    "import plotly.express as px\n",
    "\n",
    "#Add the country name into Xt\n",
    "Xt[\"country\"] = happiness[\"country\"].tolist()\n",
    "\n",
    "#intialize plotting function\n",
    "fig = px.scatter(Xt, x=\"tsne1\", y = \"tsne2\", color=\"continent\", hover_data=[\"country\"])\n",
    "#generate plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hover your mouse over the dots to see which countries they represent.\n",
    "\n",
    "Let's improve the plot by making the dots larger and showing the original data when you hover over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add size variable to the dataframe\n",
    "Xt[\"size\"] = .3\n",
    "\n",
    "#Add happiness score and gdp to Xt\n",
    "Xt[\"happiness\"] = happiness.happiness_score.tolist()\n",
    "Xt[\"gdp\"] = happiness.gdp.tolist()\n",
    "\n",
    "#This dictionary allows us to turn on and turn off our chosen variables\n",
    "hover_data={\"country\":True, \n",
    "            \"continent\":True, \n",
    "            \"happiness\":True,\n",
    "            \"gdp\":True,\n",
    "            \"tsne1\":False,\n",
    "            \"tsne2\":False,\n",
    "            \"size\":False}\n",
    "\n",
    "#intialize plotting function\n",
    "fig = px.scatter(Xt, x=\"tsne1\", y = \"tsne2\", color=\"continent\", \n",
    "                 hover_data=hover_data, size=\"size\", opacity=.6)\n",
    "#generate plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look now?!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
