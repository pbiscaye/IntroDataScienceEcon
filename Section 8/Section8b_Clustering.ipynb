{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ee8038",
   "metadata": {},
   "source": [
    "# Section 8. Clustering\n",
    "\n",
    "#### Instructor: Pierre Biscaye \n",
    "\n",
    "The content of this notebook is taken from UC Berkeley D-Lab's Python Machine Learning [course](https://github.com/dlab-berkeley/Python-Machine-Learning).\n",
    "\n",
    "### Sections\n",
    "1. K-Means clustering, evaluation, interpretation, limitations\n",
    "2. Hierarchical clustering, agglomerative clustering\n",
    "\n",
    "**Clustering** is an unsupervised ML method used to group data points based on their features alone, with no observed grouping labels as in supervised classification. Thus most clustering alorithms seeks to group points by their distance in a high dimensional space generated by provided features.\n",
    "\n",
    "Below is a plot showing the results of the clustering algorithms in Scikit-Learn for several different toy datasets.\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.datasets import make_blobs, make_circles, make_moons\n",
    "\n",
    "#plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce38fbb",
   "metadata": {},
   "source": [
    "## Data and Clustering Objective\n",
    "\n",
    "[Spotify has a really cool api](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features) that provides access to a variety of numerical features encoding musical traits such as energy, danceability, and loudness. Below is the data dictionary explaining what each feature means.\n",
    "\n",
    "### Spotify Data Dictionary\n",
    "\n",
    "**acousticness:** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
    "\n",
    "**danceability:** Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
    "\n",
    "**duration_ms:** The duration of the track in milliseconds.\n",
    "\n",
    "**energy:** Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
    "\n",
    "**instrumentalness:** Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
    "\n",
    "**key:** The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\n",
    "\n",
    "**liveness:** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n",
    "\n",
    "**loudness:** The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\n",
    "\n",
    "**mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
    "\n",
    "**speechiness:** Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
    "\n",
    "**tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n",
    "\n",
    "**time_signature:** An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of \"3/4\", to \"7/4\".\n",
    "\n",
    "**valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
    "\n",
    "There are also song identifiers, including artist, album, and song names and internal Spotify IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614464ea",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Our job is to cluster around 25,000 songs based on the data provided by Spotify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/spotify_features.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6560b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8c152",
   "metadata": {},
   "source": [
    "## 1. K-means clustering  \n",
    "\n",
    "The first algorithm we cover is k-means clustering using `scikit-learn`. The scikit-learn documentation for clustering is found [here](http://scikit-learn.org/stable/modules/clustering.html).\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1200/1*rw8IUza1dbffBhiA4i0GNQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80847c2",
   "metadata": {},
   "source": [
    "K-Means works by dividing data into K number of group, where K is determined by you the data scientist.\n",
    "\n",
    "The K-Means algorithm assigns points to a group based on distance to that group's centroid. The algorithm works to find the centroids that minimize the *inertia*, or the sum of the squared distances, of every point and the possible centroids as shown in the following formula:\n",
    "\n",
    "$$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_j, \\mu_i||^2)$$\n",
    "\n",
    "Here $C$ is the set of centroids and the double bars represent a distance metric.\n",
    "\n",
    "Distance is determined using the euclidean distance formula, as in this two-dimensional case:\n",
    "\n",
    "$$d = \\sqrt {\\left( {x_1 - x_2 } \\right)^2 + \\left( {y_1 - y_2 } \\right)^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54518206",
   "metadata": {},
   "source": [
    "The algorithm first randomly comes up with the positions of the K centroids and then assigns labels to data based on their nearest centroids.\n",
    "\n",
    "After this first iteration the centroids are then recomputed by taking the average of all the points in each cluster. This process repeats until the model reaches a point of convergance or some other stopping criteria.\n",
    "\n",
    "The following gif demonstrates the iterative process of KMeans\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469c70e",
   "metadata": {},
   "source": [
    "### Visualizing K-Means in a 2D space\n",
    "\n",
    "Before we apply Kmeans to the spotify data, let's visualize how it works on a 2D matrix. We'll simulate data using `make_blobs` which generates synthetic datasets for clustering using Gaussian (following a normal distribution) 'blobs' of data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1537d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = make_blobs(n_samples=200, n_features=2, random_state=4)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6981427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = X[:, 0], y = X[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5fa001",
   "metadata": {},
   "source": [
    "It seems obvious there are two clusters, so let's intialize the algoritm with K = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1837222",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838bbbfb",
   "metadata": {},
   "source": [
    "Fit on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf957ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09458185",
   "metadata": {},
   "source": [
    "Grab the labels and centroids. (FYI, `labels_` has an _ on the end because it's an attribute created after a certain command, there is no `labels_` pre modeling fitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36177b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = km.labels_\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also grab labels predicting X\n",
    "labels2 = km.predict(X) \n",
    "labels2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57494418",
   "metadata": {},
   "source": [
    "These are the centroids/centers of the clusters. They are the average values of the features in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b960b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = km.cluster_centers_\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222dc62",
   "metadata": {},
   "source": [
    "Let's plot the data colored by the cluster labels along with the centriods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = X[:, 0], y = X[:, 1], c=labels, cmap=\"autumn\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\", \"black\"], cmap=\"autumn\", linewidths=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014e84f",
   "metadata": {},
   "source": [
    "What happens if we try a different K value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3, random_state=1)\n",
    "km.fit(X)\n",
    "labels = km.labels_\n",
    "centroids = km.cluster_centers_\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = X[:, 0], y = X[:, 1], c=labels, cmap=\"autumn\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\",\"black\", \"black\"], cmap=\"autumn\", linewidths=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d1efb",
   "metadata": {},
   "source": [
    "The algorithm will do its best to identify however many clusters you desire, but how much the addition of clusters adds to your understanding of the data depends upon its underlying structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d05bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5, random_state=1)\n",
    "km.fit(X)\n",
    "labels = km.labels_\n",
    "centroids = km.cluster_centers_\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = X[:, 0], y = X[:, 1], c=labels, cmap=\"autumn\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\",\"black\", \"black\"], cmap=\"autumn\", linewidths=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee41d3",
   "metadata": {},
   "source": [
    "## K-Means and Spotify\n",
    "\n",
    "Now we'll apply K-Means clustering to the Spotify data. \n",
    "\n",
    "The first thing we will do is select the song attribute columns for clustering. This determines the dimensions over which cluster distances are calculated for identifying optimal centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6078e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbc83f",
   "metadata": {},
   "source": [
    "For simplicity and efficiency, let's select the mathematical measures of musical 'personality'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c58790",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\"]\n",
    "X = df[cols]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b693be6",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb822998",
   "metadata": {},
   "source": [
    "Before we cluster the data, we first need to standardize the features. Because the euclidean distance function equally considers or weights each feature, larger features bias the results.\n",
    "\n",
    "We are going to transform the data into their Z-scores using the `StandardScaler` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557830af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler object\n",
    "scaler = StandardScaler()\n",
    "# Fit on the data\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e931b5",
   "metadata": {},
   "source": [
    "`scaler` has learned from the data but it hasn't produced any new data. That's why we need to use `.transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef52394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the transformed data using scaler\n",
    "Xs = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb643593",
   "metadata": {},
   "source": [
    "We can also do both operations in one line, as we've done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs[0:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b43df5",
   "metadata": {},
   "source": [
    "Now we can cluster the spotify data. Let's try five clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5, random_state=1)\n",
    "km.fit(Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bc26f",
   "metadata": {},
   "source": [
    "It's not straightforward to map the data and clusters with so many features. But we can grab the labels and tally the frequency in each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_5 = km.labels_\n",
    "pd.value_counts(labels_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16a937",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluating cluster models isn't as precise as evaluating supervised learning models since we don't know the true labels of the data.\n",
    "\n",
    "However use we can use techniques such as the elbow method and silhouette scores to try to deterine the optimal k-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835a334",
   "metadata": {},
   "source": [
    "**The Elbow Method**\n",
    "![](https://www.researchgate.net/publication/339823520/figure/fig3/AS:867521741733888@1583844709013/The-elbow-method-of-k-means.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27166ea",
   "metadata": {},
   "source": [
    "The `intertia_` attribute tells us the model's error — the sum of the squared distances between every point and its centriod.\n",
    "\n",
    "We make an 'elbow' plot by plotting an array of k-values versus each k-value's model's inertia score. An optimal number of clusters can be identified as an elbow or inflection point, where inertia is no longer decreasing as sharply with additional clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f5f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0049ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize list to collect intertia scores and array of k-values\n",
    "i_scores = []\n",
    "kvalues = np.arange(2, 11)\n",
    "\n",
    "#iterate over kvalues\n",
    "for k in kvalues:\n",
    "    #intialize model with k and fit it on Xs\n",
    "    km = KMeans(n_clusters=k, random_state=1)\n",
    "    km.fit(Xs)\n",
    "    #append inertia score to i_scores\n",
    "    i_scores.append(km.inertia_)\n",
    "    \n",
    "#plot kvalues versus i_scores\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(kvalues, i_scores, linewidth = 3)\n",
    "plt.xticks(ticks=kvalues, labels=kvalues)\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Inertia Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870cf9e",
   "metadata": {},
   "source": [
    "**Question**: Based on this graph, where would you say the inflection or elbow point is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536d5af",
   "metadata": {},
   "source": [
    "**Silhouette Score**\n",
    "\n",
    "![](https://uploads-ssl.webflow.com/5f5148a709e16c7d368ea080/5f7dea907b8e8c7769e769c8_5f7c9650bc3b1ed0ad2247eb_silhouette_formula.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd682c",
   "metadata": {},
   "source": [
    "**Question**: Do you understand why the different extreme values of $s(i)$ indicate different types of clusters?\n",
    "\n",
    "Let's calculate silhouette scores across different numbers of clusters and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize list to collect silhouette scores\n",
    "s_scores = []\n",
    "\n",
    "#iterate over kvalues\n",
    "for k in kvalues:\n",
    "    #intialize model with k and fit it on Xs\n",
    "    km = KMeans(n_clusters=k, random_state=1)\n",
    "    km.fit(Xs)\n",
    "    labels = km.labels_\n",
    "    #derive silhouette score by passing in data and labels\n",
    "    ss = silhouette_score(Xs, labels=labels)\n",
    "    #append silhouette score to s_scores\n",
    "    s_scores.append(ss)\n",
    "    \n",
    "#plot kvalues versus s_scores\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(kvalues, s_scores, linewidth = 3)\n",
    "plt.xticks(ticks=kvalues, labels=kvalues)\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Silhouette Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360002ba",
   "metadata": {},
   "source": [
    "**Question**: What consistutes a good silhouette score? With that in mind, which k value produces the best silhouette score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the maxiumum of s_scores and use that number to find its index value in s_scores\n",
    "k_index = s_scores.index(max(s_scores))\n",
    "#Index kvalues with k_index\n",
    "best_k = kvalues[k_index]\n",
    "best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edd878",
   "metadata": {},
   "source": [
    "That's not the $k$ we originally used! It is easy to re-train the model using the `best_k` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=best_k, random_state=1)\n",
    "km.fit(Xs)\n",
    "labels = km.labels_\n",
    "pd.value_counts(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84d626",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis \n",
    "\n",
    "Let's stick with the original model's classification of 5 types of songs. We now need to add some identity to the labels. Sklearn doesn't tell us what each label means, the labels of 0, 1, 2, ... hold no information about the types of music each label represents.\n",
    "\n",
    "Therefore we need to use some EDA techniques to derive meaning from these categorizations.\n",
    "\n",
    "First, we'll group the original dataset by the labels and calculate the means. These are the cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f546cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"labels\"] = labels_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17331c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(labels_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28dd24f",
   "metadata": {},
   "source": [
    "What does this suggest about what the 5 labels describe?\n",
    "\n",
    "Let's visualize the distributions of features by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0731a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(4, 2, figsize=(16, 15))\n",
    "\n",
    "palette = \"bright\"\n",
    "sb.histplot(data = X, x = 'acousticness', hue=\"labels\", kde = True , palette = palette, ax=axes[0,0])\n",
    "sb.histplot(data = X, x = 'danceability', hue=\"labels\", kde = True , palette = palette, ax=axes[0,1])\n",
    "sb.histplot(data = X, x = 'energy', hue=\"labels\", kde = True , palette = palette, ax=axes[1,0])\n",
    "sb.histplot(data = X, x = 'instrumentalness', hue=\"labels\", kde = True , palette = palette, ax=axes[1,1])\n",
    "sb.histplot(data = X, x = 'liveness', hue=\"labels\", kde = True , palette = palette, ax=axes[2,0])\n",
    "sb.histplot(data = X, x = 'loudness', hue=\"labels\", kde = True , palette = palette, ax=axes[2,1])\n",
    "sb.histplot(data = X, x = 'speechiness', hue=\"labels\", kde = True , palette = palette, ax=axes[3,0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a71b5",
   "metadata": {},
   "source": [
    "**Question**: What insights can we glean from this visualization? What are some other ways we can add meaning to the labels?\n",
    "\n",
    "This is a challenge for *unsupervised* machine learning. We need to interpret and make sense of the classifications that result from the model, to try and determine what is underlying them. This is harder with more features than with fewer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb648b",
   "metadata": {},
   "source": [
    "### The limits of K-Means\n",
    "\n",
    "The biggest flaw of K-Means is its inability to coherently cluster complicated and irregular patterns of data.\n",
    "\n",
    "Let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095eeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the circles moons datasets\n",
    "circles, _ = make_circles(n_samples=200, factor=.3, noise = .04, random_state=1)\n",
    "moons, _ = make_moons(n_samples=200, noise = .07, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba918eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize circles dataset\n",
    "plt.scatter(*circles.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize moons dataset\n",
    "plt.scatter(*moons.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272435e1",
   "metadata": {},
   "source": [
    "**Question** How would you go about clustering these two datasets? How do you think K-Means will cluster them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7ecd5",
   "metadata": {},
   "source": [
    "Let's use K-Means to identify two clusters for each of the two datasets and then visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2, random_state=1)\n",
    "km.fit(circles)\n",
    "labels = km.labels_\n",
    "centroids = km.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = circles[:, 0], y = circles[:, 1], c=labels, cmap=\"autumn\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\", \"black\"], cmap=\"autumn\", linewidths=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2, random_state=1)\n",
    "km.fit(moons)\n",
    "labels = km.labels_\n",
    "centroids = km.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = moons[:, 0], y = moons[:, 1], c=labels, cmap=\"autumn\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\", \"black\"], cmap=\"autumn\", linewidths=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df71731",
   "metadata": {},
   "source": [
    "## 2. Hierarchical Clustering\n",
    "\n",
    "Now we'll discuss an alternative clustering method: hierarchical clustering. In particular, we'll look at using agglomerative clustering. The documentation is [here](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) in case you want to know more about the parameters.\n",
    "\n",
    "Agglomerative clustering is a *bottom-up approach* that works by first assigning every data point its own cluster and then iteratively merging the closest clusters. This process involves first using a defined distance metric (euclidean, cosine, etc...) it measures the distance for every combination of clusters, and then merging the two clusters that are closest to each other. At the next iteration, the distances are recalculated, and a new merge occurs. The process continues until all points are in the same cluster or a stopping condition such as a cluster distance or a predefined number of clusters is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214a688",
   "metadata": {},
   "source": [
    "Hierarchical clustering is best explained through the graphic below called a dendrogram.\n",
    "\n",
    "### Dendogram\n",
    "![](https://miro.medium.com/max/740/1*VvOVxdBb74IOxxF2RmthCQ.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a2a6",
   "metadata": {},
   "source": [
    "All the datapoints are laid out on the x-axis and the y-axis represents the distance threshold. The distance threshold has a negative correlation with the number of clusters.\n",
    "\n",
    "The number of clusters in the figure above is four, which you can tell by the number of lines the red dotted-line crosses. The datapoints of f, g, and h are grouped in the same cluster because all of that cluster's intra-distances are less than the chosen distance threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebab795",
   "metadata": {},
   "source": [
    "### Linkage \n",
    "\n",
    "Linkage is hierarchical clustering's method of determining distances between clusters for when it either splits up or combines clusters depending on the movement of the distance threshold. Different linkage types determine what distances are calculated.\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/40351linkages.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c8acc",
   "metadata": {},
   "source": [
    "Single distance is effectively minimum distance while complete linkage is maximum difference. The former tends to create long chain-like clusters while the latter creates compact spherical clusters. Average linkage balances the compactness of clusters and sentitivity to outliers.\n",
    "\n",
    "An interesting and useful way to think about hierarchical clustering is to approach the same way you would a biological taxonomy.\n",
    "\n",
    "![](https://i.pinimg.com/originals/da/70/81/da708128987034e6c0b3b8a0ccac3c05.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb7c0e",
   "metadata": {},
   "source": [
    "The looser the taxonomical criteria the larger and more diverse collection of animals you get. For example: housecats, bobcats, lynx, tigers, lions, and leopards are a part of the felidae family despite possessing some obvious differences among them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b1a36",
   "metadata": {},
   "source": [
    "### Clustering with Agglomerative Clustering\n",
    "\n",
    "Let's see if Agglomerative Clustering can do a better job with the moons and circles datasets than K-Means, while demonstrating the impact of different linkage inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed77d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model with linkage = single\n",
    "agg = AgglomerativeClustering(n_clusters=2, linkage=\"single\")\n",
    "#Fit on circles dataset\n",
    "agg.fit(circles)\n",
    "labels = agg.labels_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = circles[:, 0], y = circles[:, 1], c=labels, cmap=\"autumn\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e966fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model with linkage = complete\n",
    "agg = AgglomerativeClustering(n_clusters=2, linkage=\"complete\")\n",
    "#Fit on circles dataset\n",
    "agg.fit(circles)\n",
    "labels = agg.labels_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = circles[:, 0], y = circles[:, 1], c=labels, cmap=\"autumn\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model with linkage = single\n",
    "agg = AgglomerativeClustering(n_clusters=2, linkage=\"single\")\n",
    "#Fit on circles dataset\n",
    "agg.fit(moons)\n",
    "labels = agg.labels_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = moons[:, 0], y = moons[:, 1], c=labels, cmap=\"autumn\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd56ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model with linkage = complete\n",
    "agg = AgglomerativeClustering(n_clusters=2, linkage=\"average\")\n",
    "#Fit on circles dataset\n",
    "agg.fit(moons)\n",
    "labels = agg.labels_\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = moons[:, 0], y = moons[:, 1], c=labels, cmap=\"autumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cdd2d9",
   "metadata": {},
   "source": [
    "### Spotify Data\n",
    "\n",
    "Now let's try it out on the Spotify data to see if it offers an improvement over K-Means.\n",
    "\n",
    "Like we did with K-Means, we're going to iterate over a range of k values and then determine the best k using silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba876b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize list to collect silhouette scores\n",
    "s_scores = []\n",
    "\n",
    "#iterate over kvalues\n",
    "for k in kvalues:\n",
    "    #intialize model with k and fit it on Xs; use affinity=\"euclidean\" on scikit v<1.2\n",
    "    agg = AgglomerativeClustering(n_clusters=k, metric=\"euclidean\", linkage=\"ward\", memory=\"..\")\n",
    "    agg.fit(Xs)\n",
    "    labels = agg.labels_\n",
    "    #derive silhouette score by passing in data and labels\n",
    "    ss = silhouette_score(Xs, labels=labels)\n",
    "    #append silhouette score to s_scores\n",
    "    s_scores.append(ss)\n",
    "    \n",
    "#plot kvalues versus s_scores\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(kvalues, s_scores, linewidth = 3)\n",
    "plt.xticks(ticks=kvalues, labels=kvalues)\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Silhouette Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89929caf",
   "metadata": {},
   "source": [
    "Now five clusters seems to be ideal! Let's look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use affinity=\"euclidean\" on scikit v<1.2\n",
    "agg = AgglomerativeClustering(n_clusters=5, metric=\"euclidean\", linkage=\"ward\", memory=\"..\")\n",
    "agg.fit(Xs)\n",
    "labels = agg.labels_\n",
    "\n",
    "pd.value_counts(labels)\n",
    "X[\"labels2\"] = labels\n",
    "X.groupby('labels2').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddac356",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.groupby('labels2')['labels'].describe().T.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e374db",
   "metadata": {},
   "source": [
    "**Question**: How closely related do the two sets of labels appear to be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
