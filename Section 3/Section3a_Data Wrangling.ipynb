{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 3. Data Wrangling\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n",
    "\n",
    "The content of this notebook draws on material from UC Berkeley's D-Lab Python Fundamentals [course](https://github.com/dlab-berkeley/Python-Fundamentals).\n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Load .csv files and other tabular data into a Pandas `DataFrame`.\n",
    "* Understand that Pandas can be used for exploratory analysis.\n",
    "* Learn how to select columns and rows in a Pandas `DataFrame`.\n",
    "* Learn how to combine or merge data frames.\n",
    "\n",
    "### Sections\n",
    "1. Data Frames: Spreadsheets in Python\n",
    "2. Selecting columns\n",
    "3. Selecting rows\n",
    "4. Merging data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Frames: Spreadsheets in Python\n",
    "\n",
    "**Tabular data** is everywhere. Think of an Excel sheet: each column corresponds to a different feature of each datapoint, while rows correspond to different samples.\n",
    "\n",
    "In scientific programming, tabular data is often called a **data frame**. In Python, the `pandas` package contains an object called `DataFrame` that implements this data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importing Packages\n",
    "\n",
    "A **package** is a collection of code that someone else wrote and put in a sharable format. Usually it's designed to add specific functionalities to Python. The package we will use in this notebook is called Pandas.\n",
    "\n",
    "Before we can use a package like Pandas, we have to **import** it into the current session. We reviewed packages in notebook 2b. Importing is done with the `import` keyword. We simply run `import [PACKAGE_NAME]`, and everything inside the package becomes available to use.\n",
    "\n",
    "For many packages, like `pandas`, we use an **alias**, or nickname, when importing them. This is just done to save some typing when we refer to the package in our code.\n",
    "\n",
    "Let's import the `pandas` module, and add the alias `pd`.\n",
    "\n",
    "As good practice going forward you will typically have one block of code at the start of your notebook where you import all the packages you expect to need at once. That makes it explicit what a given notebook script depends on. But you can also import packages as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a data frame\n",
    "\n",
    "You can use pandas to make your own data frame. The code below shows the structure: You specify a set of column/variable names, and for each one specify a list of values for each row for that column. You must ensure the lists are of the same length for each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an example data frame\n",
    "data_frame = pd.DataFrame({\"A\":[1,4,5,7,9],\n",
    "                           \"B\":[4,7,2,5,2], \n",
    "                           \"Height\":[44,55,77,33,22]}) \n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frames are similar to **arrays** we practiced making using numpy. \n",
    "In both cases, they refer to a table or matrix of data organized into columns and rows. \n",
    "What is unique about Pandas data frames is that the columns are labeled with variable names, and the rows are given explicit indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "array=np.array([[1,4,5,7,9],[4,7,2,5,2],[44,55,77,33,22]])\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do mathematical operations on columns of data frames in the same way that you can for an array of numbers. \n",
    "We will see this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data\n",
    "\n",
    "Usually we will load pre-existing data. For the rest of this notebook we will work with a dataset from [Gapminder](https://en.wikipedia.org/wiki/Gapminder_Foundation). The dataset contains data for 142 countries, with values for life expectancy, GDP per capita, and population, every five years, from 1952 to 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we use the `read_csv()` method of the Pandas package, which takes a string as its main argument. This string consists of a **relative file path** pointing to the file. It is **relative** because we are referring to the location of the file in relation to the current working directory (or folder), rather than using an absolute path that specifies the entire directory structure from the root. The current working directory will be the location of this notebook. \n",
    "\n",
    "In this case, the data file should be saved in the same \"Section 3\" folder. If the data were saved in a different folder, you can \"escape\" the working folder using `../` to \"go up one folder from where this notebook is\", which could be useful if you want to access something outside your working directory. If you need to traverse multiple levels, you can chain these commands together (e.g., `../../folder_name` to go up two levels). \n",
    "\n",
    "You can also specify an **absolute path**, though this opens the possibility of errors if files move around or if others are replicating your code. One option is to define a directory variable, and then refer to that when loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to check your working directory\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a relative path\n",
    "df = pd.read_csv('Data/gapminder.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an absolute path\n",
    "# Note for Windows users: python interprets \\ as an escape character. \n",
    "# To get around this, replace these with / or \\\\, or add \"r\" at the beginning of the string\n",
    "df = pd.read_csv(\"C:\\Users\\pibiscay\\Dropbox\\Class-Data Science\\Section 3\\Data\\gapminder.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\pibiscay\\Dropbox\\Class-Data Science\\Section 3\\Data\\gapminder.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a directory to work from\n",
    "home = \"C:\\\\Users\\\\pibiscay\\\\Dropbox\\\\Class-Data Science\"\n",
    "df = pd.read_csv(home+\"/Section 3/Data/gapminder.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.head()` method will show the first five rows of a Data Frame by default. You can put an integer in between the parentheses to specify a different number of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with tabular data\n",
    "As data scientists, we'll often be working with **Comma Seperated Values (.csv)** files. \n",
    "\n",
    "Comma separated values files are common because they are relatively small and look good in spreadsheet software. A comma separated values file is just a text file that contains data but that has commas (or other separators) to indicate column breaks.\n",
    "\n",
    "As you see, `pandas` comes with a function `read_csv()`\n",
    "that makes it really easy to import .csv files as Data Frames.\n",
    "\n",
    "You can use other Pandas methods to load **other data types**. For example, `read_stata()` loads .dta files. You can research other methods to load other forms of tabuler data. For non-tabular data, other packages are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stata = pd.read_stata(home+\"/Section 3/Data/gapminder.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Selecting Columns\n",
    "Now that we have our `DataFrame`, we can select a single column by selecting the name of that column. This uses bracket notation (like we do when accessing lists).\n",
    "\n",
    "Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type of this column is a `Series`. It's like a list. You can index a `Series` object just like you can with a list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_country = df['country'] # This creates a data frame with just the country variable - only one column, effectively a list\n",
    "gap_country[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns\n",
    "df[['country','continent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Methods on Columns\n",
    "\n",
    "`DataFrame` objects come with their own methods, many of which operate on a single column of the DataFrame. \n",
    "\n",
    "For example, we can identify the number of unique values in each column by using the `nunique()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, a package provides **documentation** that explains all of its functionalities. Let's have a look at the documentation for a method called `value_counts()` [online](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html). \n",
    "\n",
    "What does `value_counts()` do in the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some methods allow you to perform calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['lifeExp'].min())\n",
    "print(df['lifeExp'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes \n",
    "\n",
    "Packages like Pandas don't only come with methods, but also with so-called **attributes**.\n",
    "\n",
    "Attributes are like variables: they give you more information about the data that you have. As we saw last time, methods are like functions: they allow you to do something with data.\n",
    "\n",
    "For instance, we can easily check the column names of our data frame using the `columns` **attribute**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular attribute is `shape`. What do you think it tells you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Autocomplete\n",
    "\n",
    "As we have seen, Jupyter Notebooks allow for tab completion, just like many text editors. If you begin typing the name of something (such as a variable) that already exists, you can simply hit **Tab** and Jupyter will autocomplete it for you. If there is more than one possibility, it will show them to you and you can choose from there. \n",
    "\n",
    "Below we are selecting a column in our `DataFrame`. See what happens when you hit `TAB`! What are you seeing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['continent']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Selecting Rows\n",
    "\n",
    "What if we wanted to get some rows in our dataset based on some condition? For example, what if we just wanted a select only the rows for which the country is Egypt? Or only rows from a particular year?\n",
    "\n",
    "We can use so-called **value comparison operators** for this. For instance, to identify the rows that include data points from Egypt, we can use `==`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'] == 'Egypt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above Series is called a **Boolean mask**. It's like a list of True/False labels that we can use to filter our Data Frame for a certain condition! \n",
    "\n",
    "Here, we create a subset of our Data Frame with the Boolean mask we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting only the data points from Egypt\n",
    "df[df['country'] == 'Egypt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of this operation is a **new data frame**! We can assign it to a new variable so we can work with this subsetted data frame. Let's do it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new data frame with data from 2002\n",
    "egypt_df = df[df['country'] == 'Egypt']\n",
    "egypt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows with multiple conditions\n",
    "df[(df['country'] == 'Egypt') & (df['year'] == 1967)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symbol `&` computes an element-wise *and* operation, `| `for an *or* operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['country'] == 'Egypt') & ((df['year'] == 1967) | (df['year'] == 1972))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select rows by index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 3 rows \n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows 10-19\n",
    "df[9:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting both rows and columns\n",
    "\n",
    "To subset both rows and columns you can use `iloc` if you are referring to indices, or `loc` if you are referring to column/variable names.\n",
    "\n",
    "The syntax for `loc` is `df.loc[<row_filter>, <columns>]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting both rows and columns using iloc[]\n",
    "df.iloc[[3,4],[0,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting using loc[]\n",
    "df.loc[(df['country'] == 'Egypt'), ['pop','year','lifeExp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice:** Using the gapminder dataset, create a new subset with observations from after 1990 and the country, year, and lifeExp columns. Print the head of this subset of data. Then, calculate and print the means of lifeExp for countries observed before 1990 and after 1990. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merging/joining data frames\n",
    "\n",
    "Sometimes we will have multiple datasets with information on the same units of observation, and will want to merge or join these together for analysis purposes. \n",
    "\n",
    "To do this, we can use the `pd.merge()` method. \n",
    "\n",
    "The syntax is `pd.merge([dataset1],[dataset2], on=['varname1','varname2',...],how='inner')`, where you replace the [dataset] arguments with the names of the data frames, and the varname arguments with the variables used to identify how to match observations from the two datasets. \n",
    "\n",
    "The `on` argument indicates what columns/variables to use as '*keys*' for the merge.\n",
    "If the column names in the two datasets differ, you can use the left_on and right_on arguments, where you specify different variable/column names for the left (first) and right (second) datasets, in the order in which they should be matched.\n",
    "\n",
    "The `how` argument specifies the type of merge. Common options are:\n",
    "* 'inner': Keeps only rows where keys match in both datasets.\n",
    "* 'outer': Keeps all rows from both datasets, filling missing values with NaN.\n",
    "* 'left': Keeps all rows from the left dataset (df) and fills missing values from df2 with NaN.\n",
    "* 'right': Keeps all rows from the right dataset (df2) and fills missing values from df with NaN.\n",
    "\n",
    "Let's do an example with the gapminder data. First we will split the data into 2 dataframes with a subset of columns. Then we will merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the dataset\n",
    "df1=df[['country','year','pop','continent']]\n",
    "df2=df[['country','year','lifeExp','gdpPercap']]\n",
    "\n",
    "print(df1.head())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge it back together\n",
    "\n",
    "df_merge=pd.merge(df1,df2,on=['country','year'],how='outer')\n",
    "print(df_merge.head())\n",
    "print(df_merge.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you have **missing data**? How you do the merge will determine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2[df2['year']>1960]\n",
    "\n",
    "df_merge2=pd.merge(df1,df2,on=['country','year'],how='inner')\n",
    "df_merge3=pd.merge(df1,df2,on=['country','year'],how='outer')\n",
    "df_merge4=pd.merge(df1,df2,on=['country','year'],how='right')\n",
    "df_merge5=pd.merge(df1,df2,on=['country','year'],how='left')\n",
    "print(df_merge2.shape)\n",
    "print(df_merge3.shape)\n",
    "print(df_merge4.shape)\n",
    "print(df_merge5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to one or one to many merge\n",
    "\n",
    "In some cases your merging units are not one to one, for example if you have continent-level data you want to merge with a country-level dataset. The choice of merging `on` variables determines how this is approached. \n",
    "\n",
    "Let's first make a continent-year level dataset, taking means of values using the `groupby()` method.\n",
    "We will then merge this to the country-year dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the continent-level data frame\n",
    "grouped_df = df.groupby(['continent', 'year'])[['pop', 'lifeExp', 'gdpPercap']].mean().reset_index()\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we did:\n",
    "* `groupby(['continent', 'year'])`: Groups the data by the continent and year columns.\n",
    "* `[['pop', 'lifeExp', 'gdpPercap']]`: Selects the columns for which the mean will be calculated.\n",
    "* `.mean()`: Calculates the mean for each group.\n",
    "* `.reset_index()`: Converts the grouped data back into a regular data frame, moving the continent and year indices back into columns.\n",
    "\n",
    "Let's rename the new variables to avoid confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df2=grouped_df.rename(columns={'pop': 'pop_contmean', \n",
    "                                      'lifeExp' : 'lifeExp_contmean', \n",
    "                                      'gdpPercap' : 'gdpPercap_contmean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge this with the main `df` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cont=pd.merge(df,grouped_df2,on=['continent','year'],how='inner')\n",
    "df_merge_cont.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we hadn't renamed the variables in `grouped_df`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cont=pd.merge(df,grouped_df,on=['continent','year'],how='inner')\n",
    "df_merge_cont.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending data\n",
    "\n",
    "In some cases you have dataframes with the same information/variables but for different observations, and you want to combine them. \n",
    "\n",
    "We can combine these dataframes using the `pd.concat()` method. The arguments are a list of data frames. You must make sure that the columns are the same to avoid any errors. If the dataframes have columns with the same names but different types of data, information will be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[df['year']<1960]\n",
    "df2=df[df['year']>=1960]\n",
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb=pd.concat([df1,df2], ignore_index=True)\n",
    "print(df_comb.shape)\n",
    "df_comb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ignore_index=True` argument reindexes the resulting data frame.\n",
    "\n",
    "**Note**: You can also use `pd.concat` to merge datasets with the exact same observations but different columns.\n",
    "This is less flexible than `pd.merge` which allows different observations in the two datasets, and leads to some issues of repeated columns.\n",
    "To *concatenate columns*, you have to specify `axis=1`, to override the `axis=0` default argument.\n",
    "Here we will not specify `ignore_index=True` to keep the existing column headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[['country','year','pop','continent']]\n",
    "df2=df[['country','year','lifeExp','gdpPercap']]\n",
    "df_comb=pd.concat([df1,df2], axis=1)\n",
    "df_comb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the two variables don't have matching observations? Missing values (`NaN`) are filled in automatically with no rows dropped as in an `outer` merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2[df2['year']<1960]\n",
    "df_comb=pd.concat([df1,df2], axis=1)\n",
    "print(df_comb.shape)\n",
    "df_comb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Key Points\n",
    "\n",
    "* Import a library into Python using `import <libraryname>`.\n",
    "* Data frames allow you to work with tabular data (think Excel in Python).\n",
    "* A .csv file is just a text file that contains data separated by commas.\n",
    "* Use the `pandas` library to work with data frames.\n",
    "* Data frames are typically assigned as `df`.\n",
    "* `DataFrame` columns can be indexed using square brackets - e.g. `df[last_name]` indexes a column called \"last_name\" in `df`.\n",
    "* Rows are indexed similarly but using row numbers.\n",
    "* You can use Boolean operators to subset rows.\n",
    "* Data frames can be merged using the `.merge()` method.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
