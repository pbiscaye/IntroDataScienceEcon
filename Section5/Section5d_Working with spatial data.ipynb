{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Working with spatial data\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n",
    "\n",
    "The content of this notebook draws on material from my own research. \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Think about what processing steps are needed to prepare raw spatial data for analysis\n",
    "* Understand some of the common steps, such as changing resolutions, going from points to rasters, and deriving new variables\n",
    "* Think about ways to match spatial data to observation units for analysis\n",
    "\n",
    "### Sections\n",
    "\n",
    "1. Processing spatial data for analysis\n",
    "2. Creating a dynamic raster\n",
    "3. Point-level data analysis\n",
    "\n",
    "### Required Data\n",
    "* AustraliaRainfall.nc\n",
    "* ucdp_geo_DZA.csv\n",
    "* GPW.tif\n",
    "* NGA_HouseholdGeovariables_Y1.dta\n",
    "* NGA_water_areas_dcw.shp\n",
    "\n",
    "### Required Packages\n",
    "* numpy\n",
    "* matplotlib\n",
    "* geopandas\n",
    "* pandas\n",
    "* sys\n",
    "* rasterio\n",
    "* xarray\n",
    "* shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sys\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Processing spatial data for analysis\n",
    "\n",
    "Most analysis with spatial data requires a great deal of processing in order to get the data into a useful combined form. \n",
    "\n",
    "We will first go through examples of these processing in the contex of code I used for my paper on the impacts of exposure to locust swarms on the risk of conflict. We will then work through some examples ourselves with data I have provided.\n",
    "\n",
    "In this paper, the unit of observation for the analysis was 0.25$^{\\circ}$ cells at the annual level. This means that all my spatial data have to be processed to be at this level. \n",
    "\n",
    "I first constructed a grid of 0.25$^{\\circ}$ cells defined by cell centroids. This is what we have done previously in this class when creating meshed grids. I then duplicated this grid for the number of years in my dataset, to create a 3 dimensional array. The next step was to prepare my individual datasets to merge into this grid to create a master dataset for analysis. You can think of this dataset as a 3D array with many bands.\n",
    "\n",
    "Here is a basic summary of my main data inputs and the necessary transformations:\n",
    "* Locust swarm point data: use coordinates and date to merge to main array\n",
    "* Conflict event point data: use coordinates and date to merge to main arrays\n",
    "* Administrative boundaries: identify countries and administrative units for each grid cell midpoint; deal with edge cases\n",
    "* CHIRPS precipitation: daily at 0.05 degree resolution globally, resample taking total within years and mean within grid cells\n",
    "* ERA-5 temperature: monthly mean at 9 km resolution globally, resample taking maximum within years and mean within grid cells\n",
    "* NDVI: every 16 days at 1 km resolution globally, resample taking mean within months, max within years, and mean within grid cells\n",
    "* WorldClim precipitation and temperature: monthly at 2.5 arcmin (~0.04 degrees) resolution globally, resample taking total precip and max temp within years and mean within grid cells\n",
    "* GPW population: every 5 years at 0.25 degree resolution globally, interpolate linearly between years\n",
    "* Land cover: static raster at 0.0833 resolution, resample taking mean within grid cells\n",
    "* Net migration: annual data at 5 arcmin resolution globally, resample taking mean within grid cells\n",
    "* Crop yields: annual data at 5 arcmin resolution globally, resample taking mean within grid cells\n",
    "* PRIO-GRID data: annural at 0.5 degree resolution globally, resample spreading across included grid cells\n",
    "\n",
    "As you can see, one of the main tasks I had to deal with was resampling/transforming the resolution of different datasets to match my target dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing data resolution\n",
    "\n",
    "* **High** resolution: more detailed, more fine\n",
    "* **Low** resolution: less detailed, more aggregated\n",
    "\n",
    "What are the implications of using higher-resolution data? How do you decide?\n",
    "\n",
    "#### Spatial resolution\n",
    "\n",
    "When combining data sources it is necessary to match data resolution (as well as CRS!!).\n",
    "\n",
    "Most of the data I used for my locusts and conflict project were at a finer resolution than 0.25$^{\\circ}$ degrees (why did I use this resolution?). This means I had multiple pixels for each 0.25$^{\\circ}$ grid cell. The approach here is to **rescale** the original data to match the desired resolution. You could see that I did this in some of the GEE scripts I showed you (much of my processing was in GEE).\n",
    "\n",
    "When the original data is at a higher resolution, you are **collapsing** it to the lower target resolution. You can take a mean, a sum, a max, or other statistics for higher resolution pixels to then generate statistics at the lower target resolution. What you use depends on what you are trying to measure.\n",
    "\n",
    "When the original data is at a lower resolution, you are **spreading** the data across the higher target resolution. One approach is to assign all smaller pixels within the larger pixel the value of the larger pixel. More sophisticated approaches require assumptions about how the values in the larger pixel were generated, and using that as an input to determine how to spread the data. You might not want to do this for something like temperature, but you might for something like GDP or agricultural output, for example if you also have high-resolution data on population, economic activity, or land cover.\n",
    "\n",
    "One concern is that your pixels may not overlap exactly, e.g., if the spatial resolutions are not multiples of each other. Here you have to make decisions about how you deal with small pixels that are not entirely contained within larger pixels. An advanced approach would take weighted statistics based on the share of the small pixel area that overlaps with the larger pixel. A simplistic approach would just assign small pixels in their entirety to large pixels based on their centroids. In many cases these decisions may not make a large difference, but in some cases they might.\n",
    "\n",
    "#### Temporal resolution\n",
    "\n",
    "Merging data sources also requires matching the temporal resolution.\n",
    "\n",
    "How you approach this again depends on the relative resolutions of the original and target data sources. You can **collapse** data to lower temporal resolutions (i.e., months to years), or **spread** data across higher resolutions (i.e., years to days). Merging a static variable into a dynamic dataset implicitly spreads its values across all time periods.\n",
    "\n",
    "#### Example: Preparing WorldClim weather data\n",
    "\n",
    "WorldClim has data on monthly total precipitation and maximum temperature available at a 2.5 arcminute resolution (around 0.04 degrees) globally for every month from 1985-2018. \n",
    "\n",
    "I needed to merge this 0.04$^{\\circ}$ by month data to a 0.25$^{\\circ}$ by year target dataset.\n",
    "\n",
    "**Temporal resolution:** I took the sum of precipitation across months to get an annual measure and took the max of maximum monthly temperature. \n",
    "\n",
    "*Question:* Why did I use different methods for the two measures?\n",
    "\n",
    "**Spatial resolution:** I assigned every original 0.04$^{\\circ}$ degree cell to the larger grid cell its centroid fell in, and took means across smaller cells to get an average value for the larger cells. \n",
    "\n",
    "*Question:* When might taking a sum or a max have been appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating derived variables\n",
    "\n",
    "In some cases you want to assign raster data to point locations, which we will discuss below. \n",
    "\n",
    "In other cases, you want to use information from point events to create a raster. This is what I did for the conflict and locust swarms data in my paper. \n",
    "\n",
    "I first identified which grid cell and year each point event was located in.\n",
    "\n",
    "I then assigned values to cell-years based on the distribution of point events. I calculated two statistics for each cell-year:\n",
    "* Count of events\n",
    "* Indicators for any events\n",
    "\n",
    "Finally, I created additional variables for each cell-year based on proximity to events outside of the cell, to consider potential spillovers.\n",
    "\n",
    "When working with rasters, you may want to create some derived variables in the original resolution. Transforming first and then creating variables in a combined dataset at the target resolution may lead to different values of derived variables. You should think carefully about the math of what exactly the derived variables will be measuring depending on the order of operations and decide how to proceed. You also need to think hard about how to transform/rescale your data can affect the variables you have created, and how you will interpret them. \n",
    "\n",
    "With spatial data, we generally consider it best to create derived variables at the highest resolution, but even then we must be careful about the implications of how we rescale it. We will show a simulation with an example shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing effects of order of operations\n",
    "\n",
    "To illustrate how decisions about when to derive variables and rescale data can affect the final data, we will do some examples using the Australia rainfall. \n",
    "\n",
    "We first load the data and subset it to the southeast quadrant to reduce the time for later computations.\n",
    "\n",
    "The data are rainfall by month and year at a 0.25 degree resolution. We will create some variables:\n",
    "* a rainfall z-score within cell-months across years\n",
    "* an indicator for rainfall above the 90th percentile within cell-months across years\n",
    "\n",
    "We will create these before rescaling, then rescale to a 0.5 degree resolution by taking means of rainfall across sub-cells. In the rescale, we will also take the mean of z-score and the max of the high rainfall indicator, but also recalculate these from the rescaled rainfall data to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus = xr.open_dataset('Data/AustraliaRainfall.nc')\n",
    "seaus = aus.where((aus.lat < -30) & (aus.lon > 135), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaus['rainfall'].sel(month = 1).mean(dim = 'year').plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate required stats by cell-month across years\n",
    "rain_m = seaus['rainfall'].groupby('month').mean(dim='year')\n",
    "rain_s = seaus['rainfall'].groupby('month').std(dim='year')\n",
    "rain_p90 = seaus['rainfall'].groupby('month').quantile(0.9, dim='year')\n",
    "land_mask = seaus['rainfall'].mean(dim='year').notnull() & (seaus['rainfall'].mean(dim='year') > 0)\n",
    "\n",
    "# 2. Create new varables\n",
    "# Xarray automatically aligns the 'month' dimension\n",
    "seaus['rain_z'] = ((seaus['rainfall'].groupby('month') - rain_m) / rain_s).where(land_mask)\n",
    "seaus['rain_high'] = (seaus['rainfall'].groupby('month') >= rain_p90).astype(int).where(land_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Rescale the data\n",
    "# boundary='trim' ensures that if your dimensions aren't perfectly divisible by 2, it drops the edge\n",
    "coarse_obj = seaus.coarsen(lat=2, lon=2, boundary='trim') # sets up 2x2 blocks of subcells\n",
    "# collapse taking means\n",
    "seaus_low = coarse_obj.mean()  # This handles 'rainfall' and 'rain_z' as means\n",
    "# Handle 'rain_high' separately as a 'max' \n",
    "seaus_low['rain_high'] = coarse_obj.max()['rain_high']\n",
    "\n",
    "seaus_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate new variables at low resolution\n",
    "# Get required stats by cell-month across years\n",
    "rain_m = seaus_low['rainfall'].groupby('month').mean(dim='year')\n",
    "rain_s = seaus_low['rainfall'].groupby('month').std(dim='year')\n",
    "rain_p90 = seaus_low['rainfall'].groupby('month').quantile(0.9, dim='year')\n",
    "land_mask_low = land_mask.coarsen(lat=2, lon=2, boundary='trim').max() > 0\n",
    "\n",
    "# Create new varables\n",
    "seaus_low['rain_z_lores'] = ((seaus_low['rainfall'].groupby('month') - rain_m) / rain_s).where(land_mask_low)\n",
    "seaus_low['rain_high_lores'] = (seaus_low['rainfall'].groupby('month') > rain_p90).astype(int).where(land_mask_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what it looks like to have **coarsened** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "seaus['rainfall'].sel(year=1981, month=1).plot(ax=ax1, vmax=200)\n",
    "ax1.set_title(\"Original Rainfall (0.25°)\")\n",
    "seaus_low['rainfall'].sel(year=1981, month=1).plot(ax=ax2, vmax=200)\n",
    "ax2.set_title(\"Coarsened Rainfall (0.5°)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare what the derived rainfall z-scores and high rainfall indicators look like, in the original resolution and under the two methods for the coarse resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "\n",
    "seaus['rain_z'].sel(year=1981, month=1).plot(ax=ax1, cmap='RdBu', vmin=-3, vmax=3)\n",
    "ax1.set_title(\"Original Z (0.25°)\")\n",
    "seaus_low['rain_z'].sel(year=1981, month=1).plot(ax=ax2, cmap='RdBu', vmin=-3, vmax=3)\n",
    "ax2.set_title(\"Method A: Mean of Zs (0.5°)\")\n",
    "seaus_low['rain_z_lores'].sel(year=1981, month=1).plot(ax=ax3, cmap='RdBu', vmin=-3, vmax=3)\n",
    "ax3.set_title(\"Method B: Z of Mean Rain (0.5°)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "\n",
    "seaus['rain_high'].sel(year=1981, month=1).plot(ax=ax1, cmap='Greens')\n",
    "ax1.set_title(\"Original High Rain (0.25°)\")\n",
    "seaus_low['rain_high'].sel(year=1981, month=1).plot(ax=ax2, cmap='Greens')\n",
    "ax2.set_title(\"Method A: Max of Flags (0.5°)\")\n",
    "seaus_low['rain_high_lores'].sel(year=1981, month=1).plot(ax=ax3, cmap='Greens')\n",
    "ax3.set_title(\"Method B: Flag of Mean (0.5°)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?\n",
    "\n",
    "Let's plot the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Z-score Distributions\n",
    "sns.kdeplot(seaus['rain_z'].values.flatten(), label=\"Original\", ax=ax1, color='black', ls='--')\n",
    "sns.kdeplot(seaus_low['rain_z'].values.flatten(), label=\"Method A (Mean of Z)\", ax=ax1, color='royalblue', fill=True)\n",
    "sns.kdeplot(seaus_low['rain_z_lores'].values.flatten(), label=\"Method B (Z of Mean)\", ax=ax1, color='crimson')\n",
    "ax1.set_title(\"Z-Score Distribution Comparison\")\n",
    "ax1.legend()\n",
    "\n",
    "# Right: High Rain Frequency (Bar plot)\n",
    "# We compare the mean of the binary flags, which represents the probability/fraction of extremes\n",
    "freqs = [seaus['rain_high'].mean().values, \n",
    "         seaus_low['rain_high'].mean().values, \n",
    "         seaus_low['rain_high_lores'].mean().values]\n",
    "labels = ['Original', 'Method A (Max)', 'Method B (Lores)']\n",
    "\n",
    "ax2.bar(labels, freqs, color=['gray', 'royalblue', 'crimson'])\n",
    "ax2.set_title(\"Frequency of 'High Rain' Events\")\n",
    "ax2.set_ylabel(\"Fraction of total observations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening here? \n",
    "\n",
    "The z-scores overlap almost perfectly. This implies that the data have very high spatial autocorrelation. In this case, the z-score of the average is equivalent to the average of the z-scores. The implications is that coarsening the data doesn't lose too much information because neighboring cells were already largely in agreement.\n",
    "\n",
    "There are more differences with the high rainfall indicator. Of course, the method where we take the max will result in a higher frequency because any subcell high rainfall is sufficient to trigger this indicator in the coarser cell. We are effectively \"expanding the footprint\" of every high rainfall event. The frequency of high rainfall is pretty similar in the coarsened data, again suggesting high spatial correlations. \n",
    "\n",
    "In general we can be somewhat reassured by this simulation, but we should still be aware of the implications of how we approach this process. In particular, we show that taking maximums vs means when coarsening data can lead to big differences, which matters if we are creating binary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples from my project data processing\n",
    "\n",
    "*Note*: This code will not run on your machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current system username\n",
    "user = getpass.getuser()\n",
    "\n",
    "# Set the home path based on the user\n",
    "if user == \"pierrebiscaye\":\n",
    "    home = Path(\"/Users/pierrebiscaye/Dropbox\")\n",
    "elif user == \"pibiscay\":\n",
    "    home = Path(r\"C:\\Users\\pibiscay\\Dropbox\")\n",
    "else:\n",
    "    home = None\n",
    "    print(f\"Username '{user}' not recognized.\")\n",
    "\n",
    "# Example of how to join paths later:\n",
    "# data_path = home / \"Project\" / \"data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world=gpd.read_file(home / \"Data/Country boundaries/Country raw\" /\n",
    "                    \"UIA_World_Countries_Boundaries/World_Countries__Generalized_.shp\")\n",
    "data = pd.read_csv(home / \"Locusts/Clean data/mapping.csv\")\n",
    "x = data['lon']\n",
    "y = data['lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = rasterio.open(home / \"Data/Spatial/Land Cover/CIESIN 2000/gl-croplands-geotif/cropland.tif\")\n",
    "pasture = rasterio.open(home / \"Data/Spatial/Land Cover/CIESIN 2000/gl-pastures-geotif/pasture.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "style = {\n",
    "    'cmap': 'YlGn',\n",
    "    'vmin': 0, \n",
    "    'vmax': 1,\n",
    "    'extent': (-180, 180, -90, 90)\n",
    "}\n",
    "font_cfg = {'title': 12, 'label': 10, 'tick': 8}\n",
    "\n",
    "plot_configs = [\n",
    "    {'data': crop.read(1),    'title': 'Crop land share in 2000 (Source: CIESIN)',    'label': 'Crop share'},\n",
    "    {'data': pasture.read(1), 'title': 'Pasture land share in 2000 (Source: CIESIN)', 'label': 'Pasture share'},\n",
    "    {'data': data['ag_share'],'title': 'Ag (crop+pasture) share in 2000',           'label': 'Ag share', 'type': 'scatter'}\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# plots\n",
    "for ax, cfg in zip(axes, plot_configs):\n",
    "    # Handle imshow vs scatter\n",
    "    if cfg.get('type') == 'scatter':\n",
    "        im = ax.scatter(x, y, marker=\"s\", s=10, c=cfg['data'], **{k: v for k, v in style.items() if k != 'extent'})\n",
    "    else:\n",
    "        im = ax.imshow(cfg['data'], **style)\n",
    "    \n",
    "    # Subplot Formatting\n",
    "    ax.set_title(cfg['title'], fontsize=font_cfg['title'])\n",
    "    world.plot(ax=ax, color='none', edgecolor='k', alpha=0.3)\n",
    "    ax.set_xlim([30, 50])\n",
    "    ax.set_ylim([5, 20])\n",
    "    \n",
    "    # Colorbar Logic\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label(cfg['label'], fontsize=font_cfg['label'])\n",
    "    cbar.ax.tick_params(labelsize=font_cfg['tick'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarms = pd.read_csv(home / \"Data/Locust Hub/Retrieved 9.13.20/Swarms_geo_clean.csv\")\n",
    "swarms=swarms[swarms['yr']>1989] \n",
    "swarms2=swarms[swarms['yr']>1997] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New color scheme\n",
    "nodes = [0,  1] \n",
    "color_scheme3 = ['white', 'orange']  # corresponds to nodes\n",
    "custom_cmap3 = LinearSegmentedColormap.from_list(\n",
    "    'custom_name3', list(zip(nodes, color_scheme3)))\n",
    "custom_cmap3.set_under('white')  # set values under vmin to white\n",
    "custom_cmap3.set_over('orange')  # set values over vmax to blue\n",
    "\n",
    "# Define a function to handle repetitive formatting\n",
    "def format_map(ax, im, title, label, ticks=None):\n",
    "    ax.set_title(title, fontsize=15)\n",
    "    world.plot(ax=ax, color='none', edgecolor='k', alpha=0.3)\n",
    "    ax.set_xlim([-18, 60])\n",
    "    ax.set_ylim([-2, 38])\n",
    "    cbar = fig.colorbar(im, ax=ax, ticks=ticks)\n",
    "    cbar.set_label(label, fontsize=12)\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# Plotting \n",
    "fig, axes = plt.subplots(4, 1, figsize=(10, 15))\n",
    "\n",
    "im0 = axes[0].scatter(swarms['x'], swarms['y'], s=1, c=swarms['yr'], cmap='jet')\n",
    "format_map(axes[0], im0, \"A. Swarm locations (raw)\", \"Year\")\n",
    "\n",
    "im1 = axes[1].scatter(x, y, s=1, c=data['treat_yr2'], cmap='jet')\n",
    "format_map(axes[1], im1, \"B. First exposure (derived)\", \"Year\")\n",
    "\n",
    "im2 = axes[2].scatter(swarms2['x'], swarms2['y'], s=1, c=swarms2['yr'], cmap='jet')\n",
    "format_map(axes[2], im2, \"C. Swarm locations post-1997 (raw)\", \"Year\")\n",
    "\n",
    "im3 = axes[3].scatter(x, y, s=1, c=data['swarm100max'], cmap=custom_cmap3)\n",
    "format_map(axes[3], im3, \"D. Within 100km (1997-2020)\", \"0=No, 1=Yes\", ticks=[0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating a dynamic raster\n",
    "\n",
    "Let's practice building a dynamic raster. We'll use data on conflict events from the [UCDP](https://www.uu.se/en/websites/ucdp---uppsala-conflict-data-program) and add to this a static layer with 2000 population data from GPW. \n",
    "\n",
    "To make this tractable, we'll focus on just one country: Algeria. \n",
    "\n",
    "As a first step, let's import a shapefile of Algeria's boundaries. Rather than downloading and importing it, let's import it directly from the [GADM](https://gadm.org/download_country.html) website. We can use GeoPandas to directly pull a file from a URL, including a specific subfile from a zipped folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GADM download page URL\n",
    "url = \"https://geodata.ucdavis.edu/gadm/gadm4.1/shp/gadm41_DZA_shp.zip\"\n",
    "\n",
    "# Read in the country-level shapeful\n",
    "# The '!' symbol is used to separate the zip path from the specific file inside\n",
    "dza = gpd.read_file(f\"zip+{url}!gadm41_DZA_0.shp\")\n",
    "\n",
    "dza.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the GPW population data and mask it to keep only the country of Algeria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio.mask \n",
    "\n",
    "with rasterio.open('Data/GPW.tif') as src:\n",
    "    # Apply the mask directly to the open file object\n",
    "    # crop=True drops the rest of the world\n",
    "    gpw_dza_image, gpw_dza_transform = rasterio.mask.mask(src, dza.geometry, crop=True, nodata=np.nan)\n",
    "    # Copy the metadata and update it for the new smaller footprint\n",
    "    gpw_dza_meta = src.meta.copy()\n",
    "    gpw_dza_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": gpw_dza_image.shape[1],\n",
    "        \"width\": gpw_dza_image.shape[2],\n",
    "        \"transform\": gpw_dza_transform,\n",
    "        \"nodata\": -1\n",
    "    })\n",
    "\n",
    "# Save the Algeria-only population data\n",
    "gpw_dza = gpw_dza_image[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this worked by plotting the masked population data over a background basemap.\n",
    "\n",
    "We'll look at a **basemap** using the `contextily` package. We'll have to make sure the projections are aligned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as cx\n",
    "from rasterio.plot import plotting_extent\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Get extent of Algeria data so raster knows where to plot it\n",
    "dza_extent = plotting_extent(gpw_dza, gpw_dza_transform)\n",
    "\n",
    "# Plot the population raster\n",
    "im = ax.imshow(gpw_dza, \n",
    "               cmap='Reds', \n",
    "               extent=dza_extent, \n",
    "               vmin=0.1, vmax=100000, \n",
    "               alpha=0.6, # Alpha allows the basemap to peek through\n",
    "               zorder=2) # tells when to plot this\n",
    "fig.colorbar(im)\n",
    "\n",
    "# Plot the boundary (Transparent fill)\n",
    "dza.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1.5, zorder=3)\n",
    "\n",
    "# Add the Basemap\n",
    "cx.add_basemap(ax, \n",
    "               crs=dza.crs.to_string(), # point to current CS\n",
    "               source=cx.providers.CartoDB.Positron,\n",
    "              zorder=1) # says to plot this first (at the bottom/back)\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many `contextily` basemaps. We can browse the catalog.\n",
    "\n",
    "Some potentially useful ones:\n",
    "* Minimalist (Light): `cx.providers.CartoDB.Positron`, best for colorful heatmaps/rasters.\n",
    "* Minimalist (Dark): `cx.providers.CartoDB.DarkMatter`, great for \"neon\" or glowing data visualizations.\n",
    "* Satellite: `cx.providers.Esri.WorldImagery`, showing terrain, vegetation, or urban density.\n",
    "* Standard: `cx.providers.OpenStreetMap.Mapnik`, familiar \"Google Maps\" look with street names.\n",
    "* Terrain: `cx.providers.Stamen.Terrain`, highlighting mountains and physical geography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available providers\n",
    "print(cx.providers.keys())\n",
    "# List all styles for a specific provider (e.g., CartoDB)\n",
    "print(cx.providers.CartoDB.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, now we have the static population data and grid. We need to import the conflict event data, saved in `ucdp_geo_DZA.csv`, and turn it into a cell-year raster. \n",
    "\n",
    "We could choose any resolution, but for simplicy we'll want to set it to the same resolution as the GPW data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucdp = pd.read_csv(\"Data/ucdp_geo_DZA.csv\")\n",
    "ucdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the variables we need\n",
    "ucdp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's lay out our strategy:\n",
    "1. Extract grid coordinates for the target raster rom the `gpw_dza` metadata\n",
    "2. Map each conflict event to a grid cell\n",
    "3. Group by year and cell to get counts of events and deaths (the 'best' column)\n",
    "4. Create a 3D year-lat-lon xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the coordinate arrays from the raster metadata\n",
    "# First build a meshgrid of sufficient size to include all grid cells\n",
    "cols, rows = np.meshgrid(np.arange(gpw_dza.shape[1]), np.arange(gpw_dza.shape[0]))\n",
    "# Then use the transform from earlier to reconstruct the lat/lons\n",
    "lons, lats = rasterio.transform.xy(gpw_dza_transform, rows, cols)\n",
    "# Identify cell centroids to match with event lons and lats\n",
    "lon_coords = np.unique(lons) # longitude centroids\n",
    "lat_coords = np.sort(np.unique(lats))[::-1] # latitude centroids, top-down to maintain North-South orientation\n",
    "# Identify cell edges\n",
    "res = 0.25 # or gpw_dza_transform[0]\n",
    "lon_edges = np.append(lon_coords - (res / 2), lon_coords[-1] + (res / 2))\n",
    "lat_edges = np.append(lat_coords + (res / 2), lat_coords[-1] - (res / 2))\n",
    "\n",
    "# 2. Map UCDP points to grid indices\n",
    "# np.digitize() finds the index of the interval between edges that each event falls into\n",
    "ucdp['col_idx'] = np.digitize(ucdp['longitude'], lon_edges) - 1 # because the first interval (1) maps to the first column (0)\n",
    "ucdp['row_idx'] = np.digitize(ucdp['latitude'], lat_edges[::-1]) # flip back to South-North for digitize logic\n",
    "ucdp['row_idx'] = (len(lat_coords) - 1) - ucdp['row_idx'] # flip back again to North-South\n",
    "\n",
    "# 3. Collapse (aggregate) UCDP data by Year and Grid Cell\n",
    "grid_agg = ucdp.groupby(['year', 'row_idx', 'col_idx']).agg(\n",
    "    conflict_count=('year', 'count'),\n",
    "    total_deaths=('best', 'sum')\n",
    ").reset_index()\n",
    "# Convert the grid_agg indices to the actual coordinate values\n",
    "grid_agg['lat'] = lat_coords[grid_agg['row_idx'].astype(int)]\n",
    "grid_agg['lon'] = lon_coords[grid_agg['col_idx'].astype(int)]\n",
    "\n",
    "# 4. Initialize the Dynamic Xarray (3D)\n",
    "years = sorted(ucdp['year'].unique())\n",
    "ds = xr.Dataset(\n",
    "    data_vars={\n",
    "        \"conflicts\": ((\"year\", \"lat\", \"lon\"), np.zeros((len(years), len(lat_coords), len(lon_coords)))),\n",
    "        \"deaths\": ((\"year\", \"lat\", \"lon\"), np.zeros((len(years), len(lat_coords), len(lon_coords)))),\n",
    "    },\n",
    "    coords={\n",
    "        \"year\": years,\n",
    "        \"lat\": lat_coords,\n",
    "        \"lon\": lon_coords,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 5. Fill the Xarray with the aggregated data\n",
    "# Convert the dataframe to an xarray object\n",
    "sparse_ds = grid_agg.set_index(['year', 'lat', 'lon']).to_xarray()\n",
    "# Update your main dataset with the non-zero values\n",
    "ds.update(sparse_ds)\n",
    "# Merge in the static Population layer\n",
    "ds['population_2000'] = ((\"lat\", \"lon\"), gpw_dza)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quite that the data look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['conflict_count'].sel(year=2012).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Define the variables and titles to plot\n",
    "plot_configs = [\n",
    "    ('conflict_count', 'Conflict Events (2011)', 'Reds'),\n",
    "    ('total_deaths', 'Total Deaths (2011)', 'Oranges'),\n",
    "    ('population_2000', 'Population (2000)', 'Blues')\n",
    "]\n",
    "\n",
    "for i, (var, title, cmap) in enumerate(plot_configs):\n",
    "    # Plot the raster data\n",
    "    im = ds.sel(year=2011)[var].plot(ax=axes[i], cmap=cmap, add_colorbar=True, \n",
    "                           cbar_kwargs={'shrink': 0.8, 'label': ''})\n",
    "    \n",
    "    # Overlay the national boundary\n",
    "    dza.plot(ax=axes[i], facecolor='none', edgecolor='black', linewidth=1)\n",
    "    \n",
    "    axes[i].set_title(title)\n",
    "    axes[i].set_xlabel('Longitude')\n",
    "    axes[i].set_ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zooming in\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "plot_configs = [\n",
    "    ('conflict_count', 'Conflict Events (2011)', 'Reds', 15),\n",
    "    ('total_deaths', 'Total Deaths (2011)', 'Oranges', 30),\n",
    "    ('population_2000', 'Population (2000)', 'Blues', 150000)\n",
    "]\n",
    "\n",
    "for i, (var, title, cmap, vmax) in enumerate(plot_configs):\n",
    "    im = ds.sel(year=2011)[var].plot(ax=axes[i], cmap=cmap, vmax=vmax, add_colorbar=True, \n",
    "                           cbar_kwargs={'shrink': 0.8, 'label': ''})\n",
    "    \n",
    "    dza.plot(ax=axes[i], facecolor='none', edgecolor='black', linewidth=1)\n",
    "    axes[i].set_xlim([-4, 8])\n",
    "    axes[i].set_ylim([32, 38])\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].set_xlabel('Longitude')\n",
    "    axes[i].set_ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a 3D raster dataset which is great for visualization. But for analysis we would like to have a 2D dataset. \n",
    "\n",
    "We will **restructure the data** to a 2D cell-year panel where the cell centroids becomes lat and lon columns. We'll set this up as a pandas dataframe. Tis is fortunately pretty easy to do with built-in functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a MultiIndex (year, lat, lon)\n",
    "df_analysis = ds.to_dataframe().reset_index()\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Point-level analysis\n",
    "\n",
    "We've just looked at using point data to create rasters. But in many cases we are interested in conducting analyses at the level of points - locations of individuals, cities, businesses, etc. \n",
    "\n",
    "There are a huge number of datasets with geolocated information on events, communities, survey locations, etc. It is often very useful to map these to other spatial data for analysis.\n",
    "\n",
    "Let's work with locations of survey communities in the Nigeria General Household Survey Panel ([GHSP](https://microdata.worldbank.org/index.php/catalog/5835)). \n",
    "\n",
    "*Note*: these are not exact coordinates, which creates uncertainty with spatial joins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting point data over a raster\n",
    "\n",
    "Suppose we want to link the point data to some raster data. Let's first plot **GHSP survey locations** over a map of **population**, and then write some code to estimate population around the survey locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Nigeria ADM1 boundaries\n",
    "url = \"https://geodata.ucdavis.edu/gadm/gadm4.1/shp/gadm41_NGA_shp.zip\"\n",
    "\n",
    "# Read in the country-level shapeful\n",
    "# The '!' symbol is used to separate the zip path from the specific file inside\n",
    "nga0 = gpd.read_file(f\"zip+{url}!gadm41_NGA_0.shp\")\n",
    "nga1 = gpd.read_file(f\"zip+{url}!gadm41_NGA_1.shp\")\n",
    "nga1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load survey geovariables from 2010-11 round\n",
    "ghsp = pd.read_stata(\"Data/NGA_HouseholdGeovariables_Y1.dta\", convert_categoricals=False)\n",
    "ghsp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population and keep only NGA\n",
    "with rasterio.open('Data/GPW.tif') as src:\n",
    "    # Apply the mask directly to the open file object\n",
    "    # crop=True drops the rest of the world\n",
    "    gpw_nga_image, gpw_nga_transform = rasterio.mask.mask(src, nga0.geometry, crop=True, nodata=np.nan)\n",
    "    # Copy the metadata and update it for the new smaller footprint\n",
    "    gpw_nga_meta = src.meta.copy()\n",
    "    gpw_nga_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": gpw_nga_image.shape[1],\n",
    "        \"width\": gpw_nga_image.shape[2],\n",
    "        \"transform\": gpw_nga_transform,\n",
    "        \"nodata\": -1\n",
    "    })\n",
    "gpw_nga = gpw_nga_image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, figsize=(10,7))\n",
    "\n",
    "nga1.plot(ax=ax, color='none', edgecolor='k', label=\"State boundaries\")\n",
    "\n",
    "nga_extent = plotting_extent(gpw_nga, gpw_nga_transform)\n",
    "im = ax.imshow(gpw_nga, \n",
    "               cmap='viridis', \n",
    "               extent=nga_extent, \n",
    "               vmin=0.1, vmax=250000, \n",
    "               alpha=0.6) # tells when to plot this\n",
    "fig.colorbar(im)\n",
    "\n",
    "ax.scatter(ghsp['lon_dd_mod'],ghsp['lat_dd_mod'], marker='+', c='red')\n",
    "\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlim([2,15])\n",
    "ax.set_ylim([4,14])\n",
    "ax.set_title('GHSP 2010-11 survey locations and 2000 population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial calculations\n",
    "\n",
    "Now that we've brought together data sources, we can use them to conduct calculations.\n",
    "\n",
    "Let's calculate population around the survey locations by taking the mean for all cells with centroids within 25 km of the location.\n",
    "\n",
    "This requires a few steps:\n",
    "1) Set up a grid of centroids for the GPW data\n",
    "2) Write a function to estimate Haversine (great circle) distance between pairs of points\n",
    "3) Determine which grid centroids are within 25 km of each survey location\n",
    "4) Take the mean of population of those grid cells and assign it to each survey location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make an grid of centroid lon_coords and lat_coords \n",
    "# gpw_nga.shape gives us (rows, cols)\n",
    "rows, cols = gpw_nga.shape\n",
    "col_indices = np.arange(cols)\n",
    "row_indices = np.arange(rows)\n",
    "# Use the transform to get the x (lon) and y (lat) of the centroids\n",
    "# Note: we add 0.5 to indices to get the center of the pixel\n",
    "lon_coords, _ = rasterio.transform.xy(gpw_nga_transform, [0] * cols, col_indices, offset='center')\n",
    "_, lat_coords = rasterio.transform.xy(gpw_nga_transform, row_indices, [0] * rows, offset='center')\n",
    "# Convert to clean 1D arrays\n",
    "lon_coords = np.array(lon_coords)\n",
    "lat_coords = np.array(lat_coords)\n",
    "# Create the grid of all centroids for Nigeria\n",
    "lon_mesh, lat_mesh = np.meshgrid(lon_coords, lat_coords)\n",
    "# ravel() flattens a multidimensional array into a singel 1D line of numbers\n",
    "grid_points = np.vstack([lon_mesh.ravel(), lat_mesh.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Write a Haversine distance function\n",
    "def haversine_km(lon1, lat1, lon2, lat2):\n",
    "    R = 6371.0 # radius of the earth to convert radians to km\n",
    "    # distance varies by latitude\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2) # convert to radians\n",
    "    # relevant differences\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    # formula\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlambda/2)**2\n",
    "    # convert radian distance to km\n",
    "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Identify nearby locations and extract means\n",
    "# Note this could take a while to run with many points; would need to think about refining it\n",
    "\n",
    "# Prep the pop data and survey data\n",
    "flat_pop = gpw_nga.ravel() # convert to 1D\n",
    "survey_points = ghsp[['lon_dd_mod', 'lat_dd_mod']].values\n",
    "\n",
    "# Loop through survey points\n",
    "est_pop = []\n",
    "for survey in survey_points:\n",
    "    # Calculate distance from this survey to every centroid in the Nigeria grid\n",
    "    dist = haversine_km(survey[0], survey[1], grid_points[:, 0], grid_points[:, 1])\n",
    "    \n",
    "    # Filter for centroids within 25km\n",
    "    nearby_pop = flat_pop[dist <= 25]\n",
    "    \n",
    "    # Take the mean, ignoring NaNs (common at borders/coastlines)\n",
    "    if len(nearby_pop) > 0:\n",
    "        est_pop.append(np.nanmean(nearby_pop))\n",
    "    else:\n",
    "        est_pop.append(np.nan)\n",
    "\n",
    "ghsp['pop_25km_mean'] = est_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "ghsp['pop_25km_mean'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another example of using a shapeful of major rivers/water bodies in Nigeria. We'll create an indicator for whether each survey location is within 10 km of a (non-ocean) body of water.\n",
    "\n",
    "First we'll look the water data and plot it with the survey locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nga_wat_area = gpd.read_file(\"Data/NGA_water_areas_dcw.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, figsize=(10,10))\n",
    "nga1.plot(ax=ax, color='none', edgecolor='k', label=\"State boundaries\")\n",
    "nga_wat_area.plot(ax=ax, color='blue', edgecolor='b', alpha=0.1, label=\"Bodies of water\")\n",
    "ax.scatter(ghsp['lon_dd_mod'],ghsp['lat_dd_mod'], color='r', marker='+',alpha=0.6)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('GHSP 2010-11 survey locations and Nigeria water bodies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the indicator, instead of manually calculating all the distances, we will do something cleaner and more efficient. We will **use a buffer and a spatial join**, as we saw in previous notebooks.\n",
    "\n",
    "Here are the steps we will need to follow:\n",
    "1. Turn the pandas GHSP dataframe into a geopandas dataframe.\n",
    "2. Reproject the water and the survey points into meters instead of degrees (for distance calculations.\n",
    "3. Add a 10km buffer to the water shapes.\n",
    "4. Do a spatial join to check which points fall inside the buffered water polygons.\n",
    "5. Add the indicator to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert ghsp to a GeoDataFrame (currently it's likely just Pandas)\n",
    "ghsp_gpd = gpd.GeoDataFrame(\n",
    "    ghsp, \n",
    "    geometry=gpd.points_from_xy(ghsp['lon_dd_mod'], ghsp['lat_dd_mod']),\n",
    "    crs=\"EPSG:4326\" # Start with standard Lat/Lon\n",
    ")\n",
    "\n",
    "# 2. Re-project both to a Nigeria-specific meter-based system\n",
    "# EPSG:26392 is a common one for Nigeria (Minna / UTM zone 32N)\n",
    "ghsp_projected = ghsp_gpd.to_crs(epsg=26392)\n",
    "water_projected = nga_wat_area.to_crs(epsg=26392)\n",
    "\n",
    "# 3. Create the 10km buffer around the water bodies\n",
    "# 10,000 meters = 10km\n",
    "water_buffer = water_projected.copy()\n",
    "water_buffer['geometry'] = water_projected.buffer(10000)\n",
    "\n",
    "# 4. Perform a Spatial Join\n",
    "# 'predicate=\"within\"' checks if the point is inside the buffer\n",
    "points_near_water = gpd.sjoin(ghsp_projected, water_buffer, how=\"left\", predicate=\"within\")\n",
    "# Remove duplicates by grouping by the original index; we just need ANY match, not all matches\n",
    "is_near = points_near_water['index_right'].groupby(points_near_water.index).first().notnull().astype(int)\n",
    "\n",
    "# 5. Create the indicator\n",
    "# If the join found a match, the index of the water object (index_right) won't be NaN\n",
    "ghsp['near_water_10km'] = is_near"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it\n",
    "fig, ax = plt.subplots(ncols=1, figsize=(10,10))\n",
    "nga1.plot(ax=ax, color='none', edgecolor='k', label=\"State boundaries\")\n",
    "nga_wat_area.plot(ax=ax, color='blue', edgecolor='b', alpha=0.1, label=\"Bodies of water\")\n",
    "scatter = ax.scatter(ghsp['lon_dd_mod'],ghsp['lat_dd_mod'], \n",
    "                     c=ghsp['near_water_10km'], marker='o', \n",
    "                     cmap='PuOr', edgecolors='black',\n",
    "                     linewidths=0.1, alpha=0.6, s=40)\n",
    "handles, labels = scatter.legend_elements()\n",
    "ax.legend(handles, [\"Far (>10km)\", \"Near (<10km)\"], title=\"Water Proximity\", loc='lower right')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('GHSP 2010-11 survey locations and Nigeria water bodies')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
