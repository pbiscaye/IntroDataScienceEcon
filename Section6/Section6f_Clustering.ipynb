{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ee8038",
   "metadata": {},
   "source": [
    "# Section 6. Clustering\n",
    "\n",
    "#### Instructor: Pierre Biscaye \n",
    "\n",
    "The content of this notebook draws on from UC Berkeley D-Lab's Python Machine Learning [course](https://github.com/dlab-berkeley/Python-Machine-Learning).\n",
    "\n",
    "### Sections\n",
    "1. K-Means clustering, evaluation, interpretation, limitations\n",
    "2. Hierarchical clustering, agglomerative clustering\n",
    "3. Points for further learning\n",
    "\n",
    "### Required packages\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* seaborn\n",
    "* scikit-learn\n",
    "\n",
    "### Required data\n",
    "*spotify_features.csv\n",
    "\n",
    "## What is clustering?\n",
    "\n",
    "**Clustering** is an unsupervised ML method used to group data points based on their features alone, with no observed grouping labels as in supervised classification. Thus most clustering alorithms seeks to group points by their distance in a high dimensional space generated by provided features.\n",
    "\n",
    "Below is a plot showing the results of the clustering algorithms in Scikit-Learn for several different toy datasets. You can see that different algorithms may be better suited to different data structures.\n",
    "\n",
    "<img src='https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.datasets import make_blobs, make_circles, make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce38fbb",
   "metadata": {},
   "source": [
    "## Data and Clustering Objective\n",
    "\n",
    "[Spotify has a really cool api](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features) that provides access to a variety of numerical features encoding musical traits such as energy, danceability, and loudness. Below is the data dictionary explaining what each feature means.\n",
    "\n",
    "### Spotify Data Dictionary\n",
    "* **acousticness:** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
    "* **danceability:** Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
    "* **duration_ms:** The duration of the track in milliseconds.\n",
    "* **energy:** Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, speed metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
    "* **instrumentalness:** Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
    "* **key:** The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.\n",
    "* **liveness:** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n",
    "* **loudness:** The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\n",
    "* **mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
    "* **speechiness:** Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
    "* **tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n",
    "* **time_signature:** An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of \"3/4\", to \"7/4\".\n",
    "* **valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
    "\n",
    "There are also song identifiers, including artist, album, and song names and internal Spotify IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614464ea",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Our job is to cluster around 25,000 songs based on the data provided by Spotify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/spotify_features.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6560b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8c152",
   "metadata": {},
   "source": [
    "## 1. K-means clustering  \n",
    "\n",
    "The first algorithm we cover is K-means clustering using `scikit-learn`. The scikit-learn documentation for clustering is found [here](http://scikit-learn.org/stable/modules/clustering.html).\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1200/1*rw8IUza1dbffBhiA4i0GNQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80847c2",
   "metadata": {},
   "source": [
    "K-means works by dividing data into K number of groups, where K is determined by you the data scientist.\n",
    "\n",
    "The K-means algorithm assigns points to a group based on distance to that group's centroid. The algorithm works to find the centroids that minimize the **inertia**, or the sum of the squared distances, between every point and the possible centroids as shown in the following formula:\n",
    "\n",
    "$$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_j, \\mu_i||^2)$$\n",
    "\n",
    "Here $C$ is the set of centroids and the double bars represent a distance metric.\n",
    "\n",
    "Distance is determined using the Euclidean distance formula, as in this two-dimensional case:\n",
    "\n",
    "$$d = \\sqrt {\\left( {x_1 - x_2 } \\right)^2 + \\left( {y_1 - y_2 } \\right)^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54518206",
   "metadata": {},
   "source": [
    "The algorithm first randomly comes up with the positions of the K centroids and then assigns labels to data based on their nearest centroids.\n",
    "\n",
    "After this first iteration the centroids are then recomputed by taking the average of all the points in each cluster. This process repeats until the model reaches a point of convergance or some other stopping criteria.\n",
    "\n",
    "The following gif demonstrates the iterative process of K-means.\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469c70e",
   "metadata": {},
   "source": [
    "### Visualizing K-Means in a 2D space\n",
    "\n",
    "Before we apply K-means to the spotify data, let's visualize how it works on a 2D matrix. We'll simulate data using `make_blobs` which generates synthetic datasets for clustering using Gaussian (following a normal distribution) 'blobs' of data points. Here we will simulate two \"features\" to make 2D visualization easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1537d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = make_blobs(n_samples=200, n_features=2, random_state=212)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6981427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = X[:, 0], y = X[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5fa001",
   "metadata": {},
   "source": [
    "It seems obvious there are three clusters, so let's intialize the algoritm with K = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1837222",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3, random_state=212)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838bbbfb",
   "metadata": {},
   "source": [
    "Fit on the data. You can click the parameters dropdown to see other parameters we could have specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf957ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09458185",
   "metadata": {},
   "source": [
    "We'll store the the labels and centroids for visualization.\n",
    "\n",
    "Note: `labels_` has an _ on the end because it's an attribute created after a certain command, there is no `labels_` pre modeling fitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36177b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = km.labels_\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also grab labels predicting X\n",
    "labels2 = km.predict(X) \n",
    "labels2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57494418",
   "metadata": {},
   "source": [
    "The centroids/centers of the clusters are the average values of the features in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b960b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = km.cluster_centers_\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222dc62",
   "metadata": {},
   "source": [
    "Let's plot the data colored by the cluster labels along with the centriods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = X[:, 0], y = X[:, 1], c=labels, cmap=\"autumn\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\", \"black\"], cmap=\"autumn\", linewidths=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014e84f",
   "metadata": {},
   "source": [
    "What happens if we try a different K value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5, random_state=212)\n",
    "km.fit(X)\n",
    "labels = km.labels_\n",
    "centroids = km.cluster_centers_\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = X[:, 0], y = X[:, 1], c=labels, cmap=\"viridis\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\",\"black\", \"black\"], cmap=\"viridis\", linewidths=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cd1d9-37b4-4ce4-b7b8-87094d5f5dac",
   "metadata": {},
   "source": [
    "The results can also depend on the `random_state`, since this determines the starting points for the random centroids in the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6e598-c100-4d17-81a6-67e59fbfb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5, random_state=1)\n",
    "km.fit(X)\n",
    "labels = km.labels_\n",
    "centroids = km.cluster_centers_\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = X[:, 0], y = X[:, 1], c=labels, cmap=\"autumn\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\",\"black\", \"black\"], cmap=\"autumn\", linewidths=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d1efb",
   "metadata": {},
   "source": [
    "The algorithm will do its best to identify however many clusters you desire, but how much the addition of clusters adds to your understanding of the data depends upon its underlying structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d05bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=7, random_state=212)\n",
    "km.fit(X)\n",
    "labels = km.labels_\n",
    "centroids = km.cluster_centers_\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x = X[:, 0], y = X[:, 1], c=labels, cmap=\"autumn\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\",\"black\", \"black\"], cmap=\"autumn\", linewidths=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee41d3",
   "metadata": {},
   "source": [
    "## K-Means and Spotify\n",
    "\n",
    "Now we'll apply K-Means clustering to the Spotify data. \n",
    "\n",
    "The first thing we will do is select the song attribute columns for clustering. This determines the dimensions over which cluster distances are calculated for identifying optimal centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6078e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbc83f",
   "metadata": {},
   "source": [
    "For simplicity and efficiency, let's select the mathematical measures of musical 'personality'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c58790",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"tempo\"]\n",
    "X = df[cols].copy()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b693be6",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb822998",
   "metadata": {},
   "source": [
    "Before we cluster the data, we first need to standardize the features. Because the euclidean distance function equally considers or weights each feature, larger features bias the results.\n",
    "\n",
    "We are going to transform the data into their Z-scores using the `StandardScaler` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557830af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler object\n",
    "scaler = StandardScaler()\n",
    "# Fit on the data\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e931b5",
   "metadata": {},
   "source": [
    "`scaler` has learned from the data but it hasn't produced any new data. We need to apply it to the data using `.transform()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef52394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the transformed data using scaler\n",
    "Xs = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb643593",
   "metadata": {},
   "source": [
    "We can also do both operations in one line, as we've done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs[0:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b43df5",
   "metadata": {},
   "source": [
    "Now we can cluster the Spotify data. Let's try five clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5, random_state=212)\n",
    "km.fit(Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bc26f",
   "metadata": {},
   "source": [
    "It's not straightforward to map the data and clusters with so many features. But we can grab the labels and tally the frequency in each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_5 = km.labels_\n",
    "pd.Series(labels_5).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16a937",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluating cluster models isn't as precise as evaluating supervised learning models since we don't know the true labels of the data.\n",
    "\n",
    "However use we can use techniques such as the elbow method and silhouette scores to try to determine the optimal k-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835a334",
   "metadata": {},
   "source": [
    "#### The Elbow Method\n",
    "\n",
    "This approach is based on considering how model inertia changes as K increases, and identifying the value of K where inertia stops decreasing as fast. \n",
    "\n",
    "![](https://www.ibm.com/adobe/dynamicmedia/deliver/dm-aid--88641685-32a6-4eb0-bd17-6dbdf491ac48/k-means-clustering-graph.png?preferwebp=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27166ea",
   "metadata": {},
   "source": [
    "The `inertia_` attribute tells us the model's error — the sum of the squared distances between every point and its centroid.\n",
    "\n",
    "We make an 'elbow' plot by plotting an array of k-values versus each k-value's model's inertia score. The optimal number of clusters is identified as the \"elbow\" or inflection point, where inertia is no longer decreasing as sharply with additional clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f5f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0049ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize list to collect inertia scores and array of k-values\n",
    "i_scores = []\n",
    "kvalues = np.arange(2, 11)\n",
    "\n",
    "#iterate over kvalues\n",
    "for k in kvalues:\n",
    "    #intialize model with k and fit it on Xs\n",
    "    km = KMeans(n_clusters=k, random_state=212)\n",
    "    km.fit(Xs)\n",
    "    #append inertia score to i_scores\n",
    "    i_scores.append(km.inertia_)\n",
    "    \n",
    "#plot kvalues versus i_scores\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(kvalues, i_scores, linewidth = 3)\n",
    "plt.xticks(ticks=kvalues, labels=kvalues)\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Inertia Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870cf9e",
   "metadata": {},
   "source": [
    "**Question**: Based on this graph, where would you say the inflection or elbow point is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536d5af",
   "metadata": {},
   "source": [
    "#### Silhouette Score\n",
    "\n",
    "The silhouette score measures how distinct or separated the different clusters are by comparing average distances between points within clusters compared to the next-closest cluster centroid. A distinct cluster will have points that are all much closer to each other than to the next-closest cluster's centroid.\n",
    "\n",
    "![](https://uploads-ssl.webflow.com/5f5148a709e16c7d368ea080/5f7dea907b8e8c7769e769c8_5f7c9650bc3b1ed0ad2247eb_silhouette_formula.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd682c",
   "metadata": {},
   "source": [
    "**Question**: Do you understand why the different extreme values of $s(i)$ indicate different types of clusters?\n",
    "\n",
    "Let's calculate silhouette scores across different numbers of clusters and compare. This will take longer to run than calculating inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize list to collect silhouette scores\n",
    "s_scores = []\n",
    "\n",
    "#iterate over kvalues\n",
    "for k in kvalues:\n",
    "    #intialize model with k and fit it on Xs\n",
    "    km = KMeans(n_clusters=k, random_state=212)\n",
    "    km.fit(Xs)\n",
    "    labels = km.labels_\n",
    "    #derive silhouette score by passing in data and labels\n",
    "    ss = silhouette_score(Xs, labels=labels)\n",
    "    #append silhouette score to s_scores\n",
    "    s_scores.append(ss)\n",
    "    \n",
    "#plot kvalues versus s_scores\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(kvalues, s_scores, linewidth = 3)\n",
    "plt.xticks(ticks=kvalues, labels=kvalues)\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Silhouette Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360002ba",
   "metadata": {},
   "source": [
    "**Question**: What consistutes a good silhouette score? With that in mind, which k value produces the best silhouette score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the maxiumum of s_scores and use that number to find its index value in s_scores\n",
    "k_index = s_scores.index(max(s_scores))\n",
    "#Index kvalues with k_index\n",
    "best_k = kvalues[k_index]\n",
    "best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edd878",
   "metadata": {},
   "source": [
    "#### Comparing inertia and silhouette score\n",
    "\n",
    "These are the two primary tools for choosing the right number of clusters, and they measure very different things.\n",
    "\n",
    "**Inertia**: Inertia measures how **tightly packed** the clusters are. It is the sum of squared distances of samples to their closest cluster center. As $k$ increases, inertia always decreases because the more clusters you have, the closer each point is to a centroid. We look for the \"elbow\"—the point where the rate of decrease slows down significantly. \n",
    "\n",
    "A big advantage is that it is computationally fast. It is calculated during the K-Means algorithm itself, so it costs almost zero extra time. It is also intuitive: it directly reflects the \"tightness\" of the clusters. A disadvantage is that the \"elbow\" is often not distinct (though we can measure $\\Delta$ Inertia). It can also make it seems like \"more is better\" for the number of clusters.\n",
    "\n",
    "**Silhouette score**: The Silhouette Score measures how similar a point is to its own cluster compared to other clusters, or how **separate** the clusters are. The score ranges from -1 to +1. It is easy to interpret: a value of 1 means the point is far away from neighboring clusters, 0 means the point is on the boundary between two clusters, and -1 means the point is likely assigned to the wrong cluster.\n",
    "\n",
    "A big advantage is that unlike inertia, the Silhouette Score usually has a clear \"peak\" - a global maximum. The highest score is mathematically the \"best\" $k$. It is also a standardized measure, which allows comparisons of clustering quality across different datasets directly, unlike inertia where the values depend on the number of observations and features. Another advantage is that it directly measures separation, which is what we really care about in identifying distinct clusters. The main disadvantages is that it is computationally expensive. It requires calculating distances between every pair of points. For very large datasets (e.g., 100k+ rows), this can be very slow.\n",
    "\n",
    "A potential workflow for identifying the best K would be to first use inertia to identify the maximum K that is likely to be best (the elbow), and then estimating the silhouette score for values up to that maximum.\n",
    "\n",
    "After identifying the best K, it is easy to re-train the model using that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=best_k, random_state=212)\n",
    "km.fit(Xs)\n",
    "labels = km.labels_\n",
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac34177-8294-4c4b-bb65-a8a30b7859b9",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis \n",
    "\n",
    "Let's run a model classifying 4 types of songs - this is close to the best K and will create more clusters we can explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d530f9-2186-4566-88c2-c65a2d3efef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=4, random_state=212)\n",
    "km.fit(Xs)\n",
    "labels = km.labels_\n",
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84d626",
   "metadata": {},
   "source": [
    "We now need to add some identity to the labels. `scikit-learn` doesn't tell us what each label means. The labels of 0, 1, 2, 3 hold no information about the types of music each label represents.\n",
    "\n",
    "Therefore we need to use some EDA techniques to derive meaning from these categorizations.\n",
    "\n",
    "First, we'll group the original dataset by the labels and calculate the means. These are the cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f546cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28dd24f",
   "metadata": {},
   "source": [
    "What does this suggest about what the 5 labels describe?\n",
    "\n",
    "Let's visualize the distributions of features by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0731a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(4, 2, figsize=(16, 15))\n",
    "\n",
    "palette = \"bright\"\n",
    "sb.histplot(data = X, x = 'acousticness', hue=\"labels\", kde = True , palette = palette, ax=axes[0,0])\n",
    "sb.histplot(data = X, x = 'danceability', hue=\"labels\", kde = True , palette = palette, ax=axes[0,1])\n",
    "sb.histplot(data = X, x = 'energy', hue=\"labels\", kde = True , palette = palette, ax=axes[1,0])\n",
    "sb.histplot(data = X, x = 'instrumentalness', hue=\"labels\", kde = True , palette = palette, ax=axes[1,1])\n",
    "sb.histplot(data = X, x = 'liveness', hue=\"labels\", kde = True , palette = palette, ax=axes[2,0])\n",
    "sb.histplot(data = X, x = 'loudness', hue=\"labels\", kde = True , palette = palette, ax=axes[2,1])\n",
    "sb.histplot(data = X, x = 'speechiness', hue=\"labels\", kde = True , palette = palette, ax=axes[3,0])\n",
    "sb.histplot(data = X, x = 'tempo', hue=\"labels\", kde = True , palette = palette, ax=axes[3,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a71b5",
   "metadata": {},
   "source": [
    "**Question**: What insights can we glean from this visualization? What are some other ways we can add meaning to the labels?\n",
    "\n",
    "This is a challenge for *unsupervised* machine learning. We need to interpret and make sense of the classifications that result from the model, to try and determine what is underlying them. This is harder with more features than with fewer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb648b",
   "metadata": {},
   "source": [
    "### The limits of K-Means\n",
    "\n",
    "The biggest flaw of K-Means is its inability to coherently cluster complicated and irregular patterns of data.\n",
    "\n",
    "Let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095eeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the circles moons datasets\n",
    "circles, _ = make_circles(n_samples=200, factor=.3, noise = .04, random_state=1)\n",
    "moons, _ = make_moons(n_samples=200, noise = .07, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba918eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize circles dataset\n",
    "plt.scatter(*circles.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize moons dataset\n",
    "plt.scatter(*moons.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272435e1",
   "metadata": {},
   "source": [
    "**Question** How would you go about clustering these two datasets? How do you think K-Means will cluster them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7ecd5",
   "metadata": {},
   "source": [
    "Let's use K-Means to identify two clusters for each of the two datasets and then visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2, random_state=1)\n",
    "km.fit(circles)\n",
    "labels = km.labels_\n",
    "centroids = km.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x = circles[:, 0], y = circles[:, 1], c=labels, cmap=\"autumn\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\", \"black\"], cmap=\"autumn\", linewidths=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2, random_state=1)\n",
    "km.fit(moons)\n",
    "labels = km.labels_\n",
    "centroids = km.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x = moons[:, 0], y = moons[:, 1], c=labels, cmap=\"autumn\")\n",
    "plt.scatter(x = centroids[:, 0], y = centroids[:, 1], \n",
    "            s=800, marker=\"*\", c=list(set(labels)), \n",
    "            edgecolors=[\"black\", \"black\"], cmap=\"autumn\", linewidths=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df71731",
   "metadata": {},
   "source": [
    "## 2. Hierarchical Clustering\n",
    "\n",
    "Now we'll discuss an alternative clustering method: hierarchical clustering. In particular, we'll look at using agglomerative clustering. The documentation is [here](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) in case you want to know more about the parameters.\n",
    "\n",
    "Agglomerative clustering is a *bottom-up approach* that works by first assigning every data point its own cluster and then iteratively merging the closest clusters. This process involves first using a defined distance metric (euclidean, cosine, etc...) to measure the distance for every combination of clusters, and then merging the two clusters that are closest to each other. At the next iteration, the distances are recalculated, and a new merge occurs. The process continues until all points are in the same cluster or a stopping condition such as a cluster distance or a predefined number of clusters is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214a688",
   "metadata": {},
   "source": [
    "Hierarchical clustering is best explained through the graphic below called a *dendrogram*.\n",
    "\n",
    "![](https://miro.medium.com/max/740/1*VvOVxdBb74IOxxF2RmthCQ.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718a2a6",
   "metadata": {},
   "source": [
    "All the datapoints are laid out on the x-axis and the y-axis represents the distance threshold. The distance threshold has a negative correlation with the number of clusters.\n",
    "\n",
    "The number of clusters in the figure above is four, which you can tell by the number of lines the red dotted-line crosses. The datapoints of f, g, and h are grouped in the same cluster because all of that cluster's intra-distances are less than the chosen distance threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebab795",
   "metadata": {},
   "source": [
    "### Linkage \n",
    "\n",
    "Linkage is hierarchical clustering's method of determining distances between clusters for when it either splits up or combines clusters depending on the movement of the distance threshold. Different linkage types determine what distances are calculated.\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/40351linkages.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c8acc",
   "metadata": {},
   "source": [
    "Single distance is effectively minimum distance while complete linkage is maximum difference. The former tends to create long chain-like clusters while the latter creates compact spherical clusters. Average linkage balances the compactness of clusters and sentitivity to outliers.\n",
    "\n",
    "An interesting and useful way to think about hierarchical clustering is to approach it the same way you would a biological taxonomy.\n",
    "\n",
    "![](https://i.pinimg.com/originals/da/70/81/da708128987034e6c0b3b8a0ccac3c05.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb7c0e",
   "metadata": {},
   "source": [
    "The looser the taxonomical criteria the larger and more diverse collection of animals you get. For example: housecats, bobcats, lynx, tigers, lions, and leopards are a part of the felidae family despite possessing some obvious differences among them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b1a36",
   "metadata": {},
   "source": [
    "### Clustering with Agglomerative Clustering\n",
    "\n",
    "Let's see if Agglomerative Clustering can do a better job with the moons and circles datasets than K-Means, while demonstrating the impact of different linkage inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed77d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model with linkage = single\n",
    "agg = AgglomerativeClustering(n_clusters=2, linkage=\"single\")\n",
    "#Fit on circles dataset\n",
    "agg.fit(circles)\n",
    "labels = agg.labels_\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x = circles[:, 0], y = circles[:, 1], c=labels, cmap=\"autumn\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e966fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model with linkage = complete\n",
    "agg = AgglomerativeClustering(n_clusters=2, linkage=\"complete\")\n",
    "#Fit on circles dataset\n",
    "agg.fit(circles)\n",
    "labels = agg.labels_\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x = circles[:, 0], y = circles[:, 1], c=labels, cmap=\"autumn\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5688f-fb08-4b8c-9210-f1902cd0827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model with linkage = average\n",
    "agg = AgglomerativeClustering(n_clusters=2, linkage=\"average\")\n",
    "#Fit on circles dataset\n",
    "agg.fit(circles)\n",
    "labels = agg.labels_\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x = circles[:, 0], y = circles[:, 1], c=labels, cmap=\"autumn\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model with linkage = single\n",
    "agg = AgglomerativeClustering(n_clusters=2, linkage=\"single\")\n",
    "#Fit on circles dataset\n",
    "agg.fit(moons)\n",
    "labels = agg.labels_\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x = moons[:, 0], y = moons[:, 1], c=labels, cmap=\"autumn\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd56ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model with linkage = complete\n",
    "agg = AgglomerativeClustering(n_clusters=2, linkage=\"complete\")\n",
    "#Fit on circles dataset\n",
    "agg.fit(moons)\n",
    "labels = agg.labels_\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x = moons[:, 0], y = moons[:, 1], c=labels, cmap=\"autumn\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1073f-b993-4c80-be7e-f13ff57580d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model with linkage = ward\n",
    "agg = AgglomerativeClustering(n_clusters=2, linkage=\"ward\")\n",
    "#Fit on circles dataset\n",
    "agg.fit(moons)\n",
    "labels = agg.labels_\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x = moons[:, 0], y = moons[:, 1], c=labels, cmap=\"autumn\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cdd2d9",
   "metadata": {},
   "source": [
    "### Spotify Data\n",
    "\n",
    "Now let's try it out on the Spotify data to see if it offers an improvement over K-Means. \n",
    "\n",
    "We will use the **Ward's method** linkage criterion, because it tends to produce clusters that are compact and relatively equal in size, similar to K-Means. This criterion focuses on variance - it merges the two clusters that result in the minimum increase in total within-cluster variance after merging. In simpler terms: it looks for the two clusters that, when joined together, create the most \"tightly packed\" new cluster.\n",
    "\n",
    "This linkage criterion is most effective when the data naturally forms \"clumps\", but is less effective for unusual data structures. It has a slight bias toward creating clusters of roughly equal size, which can sometimes be desirable. \n",
    "\n",
    "This method is designed to work with Euclidean distances. Because it involves calculating centroids and variance for every possible merge, it is more computationally intensive than Single or Complete linkage for very large datasets\n",
    "\n",
    "Like we did with K-Means, we're going to iterate over a range of K values and then determine the best K using silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba876b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize list to collect silhouette scores\n",
    "s_scores = []\n",
    "kvalues = np.arange(2, 7)\n",
    "\n",
    "#iterate over kvalues\n",
    "for k in kvalues:\n",
    "    #intialize model with k and fit it on Xs; use affinity=\"euclidean\" on scikit v<1.2\n",
    "    agg = AgglomerativeClustering(n_clusters=k, metric=\"euclidean\", linkage=\"ward\", memory=\"..\")\n",
    "    agg.fit(Xs)\n",
    "    labels = agg.labels_\n",
    "    #derive silhouette score by passing in data and labels\n",
    "    ss = silhouette_score(Xs, labels=labels)\n",
    "    #append silhouette score to s_scores\n",
    "    s_scores.append(ss)\n",
    "    \n",
    "#plot kvalues versus s_scores\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(kvalues, s_scores, linewidth = 3)\n",
    "plt.xticks(ticks=kvalues, labels=kvalues)\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Silhouette Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89929caf",
   "metadata": {},
   "source": [
    "Now five clusters seems to be ideal! Let's look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use affinity=\"euclidean\" on scikit v<1.2\n",
    "agg = AgglomerativeClustering(n_clusters=5, metric=\"euclidean\", linkage=\"ward\", memory=\"..\")\n",
    "agg.fit(Xs)\n",
    "labels = agg.labels_\n",
    "\n",
    "pd.Series(labels).value_counts()\n",
    "X[\"labels2\"] = labels\n",
    "X.groupby('labels2').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddac356",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.groupby('labels2')['labels'].describe().T.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e374db",
   "metadata": {},
   "source": [
    "**Question**: How closely related do the two sets of labels appear to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef8a05-86b6-4a86-b0d5-7fa9069d3b9f",
   "metadata": {},
   "source": [
    "## 3. Points for further learning\n",
    "\n",
    "This introduction provides an overview of key considerations in unsupervised clustering algorithms. But there is much more to learn to address real-world complexities in clustering problems. Here I summarize some areas for further exploration.\n",
    "\n",
    "1. **DBSCAN**: K-Means and Agglomerative (Ward) clustering are \"distance-based\" and assume clusters are roughly spherical. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) can find clusters of arbitrary shapes (like \"moons\" or \"rings\") and—crucially—it identifies outliers (noise) rather than forcing every point into a cluster.\n",
    "\n",
    "2. **Soft Clustering**: K-Means is a \"hard\" clustering method—a point is either in Cluster A or Cluster B. Gaussian Mixture Models (GMM) provide a probability of belonging to each cluster. This is essential for \"fuzzy\" real-world data where an observation might show characteristics of two different clusters simultaneously.\n",
    "\n",
    "3. The **\"Curse of Dimensionality\"** in Clustering: As the number of features ($p$) increases, the distance between any two points starts to look the same because there are more opportunities for differences. A potential solution is Dimensionality Reduction (like PCA or UMAP) as a preprocessing step before clustering to help the algorithm find meaningful patterns in high-dimensional data. We will look at dimensionality reduction in the next notebook.\n",
    "\n",
    "4. **Stability and Robustness Testing**: In this introduction, we typically run a model once and accept the results, or perhaps after identifying the \"best\" K. An important consideration is the stability of clusters. If you take a random 80% sample of your data and run clustering again, do you get the same clusters? An approach to testing this is Bootstrap Resampling for clusters. If the clusters disappear when the data changes slightly, they likely aren't \"real\" patterns.\n",
    "\n",
    "5. **Constraint-Based Clustering**: Sometimes we have domain knowledge that should guide the model (e.g., \"These two specific data points must be in the same cluster\" or \"These two cannot be together\"). Semi-supervised clustering models allows the user to provide \"Must-Link\" and \"Cannot-Link\" constraints to influence the algorithm.\n",
    "\n",
    "6. **Categorical data**: Look into methods like K-Prototypes or Gower Distance.\n",
    "\n",
    "7. **Large N**: Look into efficient algorithms like Mini-Batch K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af01989-1579-4239-a452-234d5f5a90b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
