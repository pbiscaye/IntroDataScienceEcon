{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6. Dimensionality Reduction\n",
    "\n",
    "#### Instructor: Pierre Biscaye \n",
    "\n",
    "The content of this notebook draws from UC Berkeley D-Lab's Python Machine Learning [course](https://github.com/dlab-berkeley/Python-Machine-Learning).\n",
    "\n",
    "What if there was a way you could reduce the number of dimensions in your data and still retain a significant portion of the critical information in your data--it's 'identity'?\n",
    "\n",
    "That is **dimensionality reduction** in a nutshell. It is a statistical technique that reduces a dataset of `m` dimensions down to `k` while minimizing the loss of information. This technique is useful for generalizating, visualizing, and compressing data.\n",
    "\n",
    "### Sections\n",
    "\n",
    "1. Data and correlations between features\n",
    "2. Principal Component Analysis\n",
    "3. Interpreting PCA: understanding components, explaining variance\n",
    "4. PCA and supervised ML\n",
    "5. t-SNE and interactive plots\n",
    "\n",
    "### Required packages\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* seaborn\n",
    "* scikit-learn\n",
    "* time\n",
    "\n",
    "### Required data\n",
    "* world_happiness.csv\n",
    "* diamonds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Part of the reason why dimensionality reduction works is because of the redundancy that exists in datasets. If you have a dataset of `N` features that are all highly correlated with another, then you don't really have `N` features worth of signal.\n",
    "\n",
    "Think about the results of a survey asking subjects on their political opinions. The survey asks a respondent to rate how strongly they support/oppose issues on topics such as taxes, abortion, the environment, etc... Opinions on gun control probably correlate with opinions on abortion and opinions on healthcare probably correlate with opinions on military spending.\n",
    "\n",
    "A dimensionality reduction technique could compress the results of this survey down to a single dimension that effectively represents a left-right political spectrum. And because of the multicollinearity in the data, the loss of information wouldn't be equal in proportion to the decrease in dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: World Happiness Report\n",
    "\n",
    "The data for this notebook originates from the [2022 World Happiness report](https://worldhappiness.report/ed/2022/) and was downloaded from this [kaggle repo](https://www.kaggle.com/datasets/ajaypalsinghlo/world-happiness-report-2022). You can read the full report [here](https://happiness-report.s3.amazonaws.com/2022/WHR+22.pdf). The following data dictionary explains what the variables mean. \n",
    "* **happiness_score:** The national average response to the question of life evaluation. Question asks “Please imagine a ladder, with steps numbered from 0 at the bottom to 10 at the top. The top of the ladder represents the best possible life for you and the bottom of the ladder represents the worst possible life for you. On which step of the ladder would you say you personally feel you stand at this time?”\n",
    "* **gdp:** GDP per capita (variable name gdp) in purchasing power parity (PPP) at constant 2017 international dollar prices.\n",
    "* **life_expectancy:** Healthy life expectancies at birth are based on the data extracted from the World Health Organization’s (WHO) Global Health Observatory data repository \n",
    "* **social_support:** The national average of the binary responses (either 0 or 1) to the GWP (Gallup World Poll) question “If you were in trouble, do you have relatives or friends you can count on to help you whenever you need them, or not?”\n",
    "* **freedom:** Freedom to make life choices is the national average of responses to the GWP question “Are you satisfied or dissatisfied with your freedom to choose what you do with your life?”\n",
    "* **generosity:** Generosity is the residual of regressing national average of response to the GWP question “Have you donated money to a charity in the past month?” on GDP per capita.\n",
    "* **corruption:** The measure is the national average of the survey responses to two questions in the GWP: “Is corruption widespread throughout the government or not” and “Is corruption widespread within businesses or\n",
    "not?” The overall perception is just the average of the two 0-or-1 responses. The corruption perception at the national level is just the average response of the overall perception at the individual level.\n",
    "* **country** and **continent** identify the location of the country-level observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness = pd.read_csv(\"Data/world_happiness.csv\")\n",
    "happiness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is most effective when there is significant redundancy in the data. By redundancy, we mean a high level of multicollinearity in the data — frequent high correlations between variables in the data.\n",
    "\n",
    "Let's look at the correlation table for the world happiness data to see if that is the case. We won't include rank because that is perfectly correlated with happiness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = happiness.select_dtypes(\"number\").drop(columns=['rank']).corr()\n",
    "corr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heat map\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# Plot a heatmap using seaborn\n",
    "# Include the mask and correct aspect ratio, and a diverging colormap\n",
    "sns.heatmap(corr, mask=mask, cmap='RdBu', vmax=.8, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Do you observe high multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis \n",
    "\n",
    "Principal component analysis (PCA) is the most commonly used dimensionality reduction algorithm. It works by transforming a high dimensional dataset into a smaller one while still retaining a significant amount of the information. The principal components are the outputted variables that are a linear mixture of the original variables. This transformed data are all uncorrelated with one another by construction, with the most valuable information compressed into the the first few components. \n",
    "\n",
    "![](https://miro.medium.com/max/600/1*e_kBZQz2hsa7de6TxpgJqg.gif)\n",
    "Source: Towards Data Science\n",
    "\n",
    "PCA first aims to understand how the variables of the data differ from their means and if the relationship between the variables and their means varies across variables. So in order to complete this part, PCA calculates the covariance matrix which has the dimensions of K x K features. Here's the formula to calculate covariance for sample size N:\n",
    "\n",
    "![](https://www.gstatic.com/education/formulas2/443397389/en/covariance_formula.svg)\n",
    "\n",
    "Next up the algorithm calculates the eigenvectors and eigenvalues. These are linear algebra calculations that are calculated from the covariance matrix. The eigenvectors and eigenvalues are used to calculate the principal components.\n",
    "\n",
    "The eigenvectors are defined as the directions of axes of the principal components while the eigenvalue refers to the magnitude of the eigenvector. PCA orders the eigenvalues from greatest to least — this explains why the first component has the most signal.\n",
    "![](https://miro.medium.com/max/600/1*BpwgqgR-dVZSmIPKTaM4JQ.gif)\n",
    "\n",
    "Finally PCA transforms the original data by taking the dot product of the transposed eigenvectors and the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://devopedia.org/images/article/139/4543.1548137789.jpg)\n",
    "\n",
    "Source: Devopedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with our data\n",
    "\n",
    "Let's select the numerical variables and initialize the PCA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = happiness.select_dtypes(\"number\").drop(\"rank\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to scale the data because PCA is sensitive to variables with higher variances/ranges. If one variable (like GDP per capita) has a range of 0–80,000 and another (like Freedom) is 0–1, PCA will assume the high-range variable is more \"important\" simply because the numbers are bigger.\n",
    "\n",
    "Therefore we want the algorithm to analyze data with all similar variances to avoid that kind of bias. We'll therefore normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "Xs = scale.fit_transform(X)\n",
    "Xs = pd.DataFrame(Xs, columns=X.columns)\n",
    "Xs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize PCA model and set n_components = 2\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "#Fit and transforms\\\n",
    "Xp = pca.fit_transform(Xs)\n",
    "Xp[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking a value for `n_components` is not required. We can apply pca to our data without setting a value for `n_components` and it will return a dataset with the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "Xp = pca.fit_transform(Xs)\n",
    "Xp[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape\n",
    "Xp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first two components, select the first two columns. Notice that they are the same as when we specified `ncomponents=2`. That is because PCA always calculates all the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overwrite Xp with just its first two columns.\n",
    "Xp = Xp[:, :2]\n",
    "Xp[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interpreting PCA\n",
    "\n",
    "It is not always obvious what the identified principal components represent. We can look at how they correlate with particular variables to get a sense. We can also plot them to see. \n",
    "\n",
    "A great thing about dimensionality reduction is that we can visualize highly dimensional data in 2D or 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put pca data into dataframe\n",
    "Xp = pd.DataFrame(Xp, columns=[\"comp1\", \"comp2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the pca data with a 2D scatter plot\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.scatter(x=Xp[\"comp1\"], y = Xp[\"comp2\"], s=80)\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add continent label to the chart to see if that helps with interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add continent to the pca dataset\n",
    "Xp[\"continent\"] = happiness[\"continent\"].tolist()\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "for continent in Xp.continent.unique():\n",
    "    data = Xp[Xp.continent == continent]\n",
    "    plt.scatter(x=data[\"comp1\"], y = data[\"comp2\"], label = continent, s=80)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend(fontsize = \"large\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Based on these patterns and your expectations about likely differences in well-being across continents, are higher or lower values of these components likely to be associated with higher national well-being?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained variance\n",
    "\n",
    "Explained variance informs us how much of the original signal/identity each component possesses from the original dataset. The `explained_variance_ratio_` attribute is a normalized version of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance ratio of each component\n",
    "exp_var_ratio = pca.explained_variance_ratio_.round(3)\n",
    "exp_var_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cumulative sum of the explained variance ratio of each component\n",
    "exp_var_ratio_cs = exp_var_ratio.cumsum()\n",
    "exp_var_ratio_cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two components (a quarter of the original dimensions) net almost three-quarters of the data's explained variance.\n",
    "\n",
    "Cutting the number of dimensions by more than half leaves us with 83.2% of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.bar(x=range(1, exp_var_ratio.shape[0] +1), height=exp_var_ratio, color = \"b\")\n",
    "plt.plot(range(1, exp_var_ratio.shape[0] +1), exp_var_ratio_cs, c = \"red\", marker = \"*\")\n",
    "plt.xlabel(\"N Components\", fontsize = 16)\n",
    "plt.ylabel(\"Explained Variance Ratio\",fontsize = 16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors/PCA components\n",
    "\n",
    "For each of our features, the PCA calculates associated eigenvectors that relate to each of the principal components. The `components_` attribute of a PCA object is where the eigenvectors are stored. It is a matrix whose dimensions are equal to the number of components specified by the number of features. In this case it will be a 7x7 matrix.\n",
    "\n",
    "These eignvectors tell us how much each feature contributes to each PCA component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following chart we visualize each feature's first and second eigenvector on a two dimensional plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.grid(True)\n",
    "cols = X.columns\n",
    "for i in range(len(cols)):\n",
    "    x = pca.components_[0, i]\n",
    "    y = pca.components_[1, i]\n",
    "    plt.arrow(0, 0, x, y, color = \"blue\", width = 0.005, alpha = .3)\n",
    "    plt.annotate(cols[i], xy = (x, y), fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret these blue eigenvector arrows? They tell us about **how each feature influences the two components**.\n",
    "* Length: Longer arrows mean that feature has a stronger influence on the two components.\n",
    "* Direction: Arrows pointing in the same direction are positively correlated. Arrows pointing in opposite directions are negatively correlated.\n",
    "* Alignment: An arrow pointing straight right (or left) is a major driver of comp1 only; an arrow pointing straight up (or down) is a major driver of comp2 only. Angled arrows contribute to both.\n",
    "\n",
    "We can also look at the component loadings (eigenvectors) numerically, to try and identify what each component is capturing. This can be clearer than the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of the loading eigenvectors\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_[:2].T, \n",
    "    columns=['PC1', 'PC2'], \n",
    "    index=X.columns\n",
    ")\n",
    "print(loadings.sort_values(by='PC1', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us? Principal component 1 seems like a measure of \"development\" or \"affluence\". Happiness, GDP per capita, life expectancy, social support, and freedom all correlate strongly. The graph of eigenvectors tell us that we could also call PC1 \"happiness\".\n",
    "\n",
    "Principal component 2 is a bit harder to identify, but seems like it may reflect something like \"governance\" or \"norms\", as generosity, freedom, and corruption are the three strongest drivers. Since generosity points straight up, this feature varies independently of the PC1 features.\n",
    "\n",
    "A great way to check this is to color the scatter plot with the original data, to see if spatial distance in the PCA plot matches our intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sc = plt.scatter(x=Xp[\"comp1\"], y=Xp[\"comp2\"], c=X['generosity'], cmap='viridis', s=80)\n",
    "plt.colorbar(sc, label='Generosity')\n",
    "plt.xlabel(\"Component 1 (The Development Factor)\")\n",
    "plt.ylabel(\"Component 2 (The Governance/Norms Factor)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot a **biplot** which combines the scatterplot and arrows into a single visualization. \n",
    "\n",
    "Because component loadings are between -1 and 1, the PCA scores can be much larger. So we will scale the eigenvector loadings to be visible on the same axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the base scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(Xp[\"comp1\"], Xp[\"comp2\"], alpha=0.5, color='gray', s=50)\n",
    "\n",
    "# 2. Define the scaling factor for arrows (to make them visible against the dots)\n",
    "# We'll start with 5 based on the above plot\n",
    "arrow_scale = 5\n",
    "\n",
    "# 3. Plotting the eigenvectors\n",
    "for i, col_name in enumerate(X.columns):\n",
    "    x_loading = pca.components_[0, i] * arrow_scale\n",
    "    y_loading = pca.components_[1, i] * arrow_scale\n",
    "    plt.arrow(0, 0, x_loading, y_loading, color='blue', \n",
    "              width=0.05, head_width=0.1, alpha=0.8)\n",
    "    # Annotate with feature name\n",
    "    plt.text(x_loading * 1.15, y_loading * 1.1, col_name, \n",
    "             color='blue', fontsize=12, ha='center', va='center')\n",
    "\n",
    "plt.xlabel(f\"PC1 ({exp_var_ratio[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({exp_var_ratio[1]*100:.1f}%)\")\n",
    "plt.title(\"PCA Biplot: World Happiness Data\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.axhline(0, color='black', linewidth=1)\n",
    "plt.axvline(0, color='black', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret this plot? Each dot is a country. Countries on the far right have more of the characteristics of the arrows pointing right. Countries on the far left have less. The same is true for countries at the far top or bottom.\n",
    "\n",
    "**Question**: What are the characteristics of countries in the bottom left of the plot?\n",
    "\n",
    "Let's label the countries at the extremes of both principal components, to dig a little deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify the 4 extreme points\n",
    "# We use .idxmax() and .idxmin() to find the index of the extremes\n",
    "top_pc1_idx = Xp['comp1'].idxmax()\n",
    "bot_pc1_idx = Xp['comp1'].idxmin()\n",
    "top_pc2_idx = Xp['comp2'].idxmax()\n",
    "bot_pc2_idx = Xp['comp2'].idxmin()\n",
    "\n",
    "# Store them in a list to loop through easily\n",
    "extreme_indices = [top_pc1_idx, bot_pc1_idx, top_pc2_idx, bot_pc2_idx]\n",
    "\n",
    "# 2. Re-plot the Biplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(Xp[\"comp1\"], Xp[\"comp2\"], alpha=0.4, color='gray', s=50)\n",
    "\n",
    "# 3. Add the 4 specific labels\n",
    "for idx in extreme_indices:\n",
    "    # Get country name from the original dataframe using the index\n",
    "    name = happiness.loc[idx, 'country'] \n",
    "    plt.annotate(name, \n",
    "                 (Xp.loc[idx, 'comp1'], Xp.loc[idx, 'comp2']),\n",
    "                 textcoords=\"offset points\", \n",
    "                 xytext=(0,10), \n",
    "                 ha='center', \n",
    "                 fontweight='bold',\n",
    "                 fontsize=12,\n",
    "                 bbox=dict(boxstyle='round,pad=0.3', fc='yellow', alpha=0.3))\n",
    "\n",
    "# 4. Add the arrows\n",
    "arrow_scale = 5\n",
    "for i, col_name in enumerate(X.columns):\n",
    "    x_loading = pca.components_[0, i] * arrow_scale\n",
    "    y_loading = pca.components_[1, i] * arrow_scale\n",
    "    plt.arrow(0, 0, x_loading, y_loading, color='blue', width=0.04, head_width=0.1, alpha=0.6)\n",
    "    plt.text(x_loading * 1.1, y_loading * 1.1, col_name, color='blue', fontsize=11)\n",
    "\n",
    "plt.axhline(0, color='black', lw=1, alpha=0.5)\n",
    "plt.axvline(0, color='black', lw=1, alpha=0.5)\n",
    "plt.title(\"PCA Biplot with Extreme Country Labels\")\n",
    "plt.xlabel(f\"PC1 ({exp_var_ratio[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({exp_var_ratio[1]*100:.1f}%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that Finalnd is at the far right, and that Afghanistan is at the far left. \n",
    "\n",
    "The top and bottom are a bit less clear. Recall that generosity was measured as the residual from a regression of charitable giving on GDP. It's not clear what exactly this represents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML with PCA\n",
    "\n",
    "One thing that also makes dimensionality reduction useful is that we can train machine learning models on dimensionality-reduced data and achieve a similar performance.\n",
    "\n",
    "Let's analyze the relationship between principal components and the performance of a machine learning model. \n",
    "\n",
    "The plan:\n",
    "1. Train a machine learning model on untransformed data — this is our baseline. Observe accuracy score.\n",
    "2. Train a machine learning model on PCA-transformed data for every value between 1 and the number of features.\n",
    "3. Plot the number of components used to train a model versus their performance in terms of accuracy and time elapsed in training the model, for both the transformed and untransformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diamond Data Dictionary\n",
    "\n",
    "We will work with a dataset on characteristics of diamonds. The objective will be predict the price of a diamond based on its characteristics, using ridge regression. \n",
    "* **carat:** weight of the diamond (0.2--5.01)\n",
    "* **cut:** quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "* **color:** diamond color, from J (worst) to D (best)\n",
    "* **clarity:** a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "* **depth**: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
    "* **table**: width of top of diamond relative to widest point (43--95)\n",
    "* **price:** price in US dollars (\\$326--\\$18,823)\n",
    "* **x**: length in mm (0--10.74)\n",
    "* **y**: width in mm (0--58.9)\n",
    "* **z**: depth in mm (0--31.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in diamonds data. \n",
    "#We will take a random sample of 15k rows to save time.\n",
    "diamonds = (\n",
    "    pd.read_csv(\"data/diamonds.csv\", index_col=[0])\n",
    "    .sample(n=15000, random_state=212)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "from sklearn.model_selection import  cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#grab X and y\n",
    "X = diamonds.drop(\"price\", axis = 1)\n",
    "y = diamonds[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To keep things simple let's use just numerical features\n",
    "# That means we drop color, clarity, and cut\n",
    "X = X.select_dtypes(\"number\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the six dimensions we will start from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data\n",
    "diamonds_scaler = StandardScaler()\n",
    "Xs = diamonds_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's establish the baseline performance when including these original features. We'll be deriving the cross-validated (to identify the optimal penalty) accuracy score for the non-PCA dataset and we'll observe the time taken to train the model. For convenience we'll do a simple search of just a few possible penalty parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2, random_state=212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# Create ridge model, with CV\n",
    "ridge_cv = RidgeCV(\n",
    "    # Which alpha values to test for?\n",
    "    alphas=np.arange(2,14,2),\n",
    "    # Number of folds\n",
    "    cv=5)\n",
    "# Fit model\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "end=time.time()\n",
    "baseline_time=round(end-start,3)\n",
    "# Evaluate model\n",
    "baseline_score=ridge_cv.score(X_test, y_test)\n",
    "print(\"Baseline accuracy: \",ridge_cv.score(X_test, y_test))\n",
    "print(\"Baseline time: \",baseline_time,\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply PCA to our data and train a model for a range of components between 1 and the number of features of our data. We collect the cross-validated accuracy scores and then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize PCA model\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA on the training data and transform\n",
    "Xpca = pca.fit_transform(X_train)\n",
    "# Apply the same PCA transform to the test data\n",
    "Xtest_pca=pca.transform(X_test)\n",
    "\n",
    "acc_scores = []\n",
    "\n",
    "times = []\n",
    "\n",
    "components_range = np.arange(1, X_train.shape[1] + 1)\n",
    "\n",
    "for comp in components_range:\n",
    "    #Slice the columns of the Xpca matrix using comp\n",
    "    pca_features = Xpca[:, :comp]\n",
    "    pcatest_features = Xtest_pca[:, :comp]\n",
    "    \n",
    "    #cross-validate\n",
    "    start = time.time()\n",
    "    ridge_cv = RidgeCV(\n",
    "        # Which alpha values to test for?\n",
    "        alphas=np.arange(2,14,2),\n",
    "        # Number of folds\n",
    "        cv=5)\n",
    "    \n",
    "    # Fit model\n",
    "    ridge_cv.fit(pca_features, y_train)    \n",
    "    end = time.time()\n",
    "    elapsed = round(end - start,3)\n",
    "    \n",
    "    acc_scores.append(ridge_cv.score(pcatest_features, y_test))\n",
    "    times.append(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize = (13, 6))\n",
    "\n",
    "fig.tight_layout(pad = 3)\n",
    "ax1.bar(components_range, acc_scores)\n",
    "ax1.set_title(\"PCA Accuracy Scores\", fontsize= 20)\n",
    "ax1.set_xlabel(\"N Components\", fontsize = 15)\n",
    "ax1.set_ylabel(\"CV Accuracy\", fontsize = 15)\n",
    "ax1.hlines(y = baseline_score, xmin = components_range.min(), xmax = components_range.max(),\n",
    "          colors = \"black\", linestyles = \"dashed\")\n",
    "ax1.annotate(text = 'Baseline Accuracy', xy = (1, baseline_score*1.05),size = 14)\n",
    "ax1.set_ylim(bottom=0, top = baseline_score*1.2)\n",
    "ax1.grid(False)\n",
    "\n",
    "ax2.set_title(\"PCA Times\", fontsize= 20)\n",
    "ax2.bar(components_range, times, color = \"#1D8A99\")\n",
    "ax2.set_xlabel(\"N Components\", fontsize = 15)\n",
    "ax2.set_ylabel(\"Seconds Elapsed\", fontsize = 15)\n",
    "ax2.hlines(y = baseline_time, xmin = components_range.min(), xmax = components_range.max(),\n",
    "          colors = \"black\", linestyles = \"dashed\")\n",
    "ax2.annotate(text = 'Baseline Time Elapsed', xy = (1, baseline_time*1.05),size = 14)\n",
    "ax2.set_ylim(bottom=0, top = baseline_time*1.2)\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do these two charts tell us about the tell timing-vs-performance tradeoff when we train a model on PCA components? What might be the implications if we had a dataset with millions of variables and potentially hundreds of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. t-SNE\n",
    "\n",
    "![](https://tse2.mm.bing.net/th?id=OIP.OvotzpNWbWE8wZht7Pw3_QHaGF&w=690&c=7&pid=Api&p=0)\n",
    "\n",
    "t-SNE (t-distributed stochastic neighbor embedding) is another popular dimensionality reduction algorithm that is really popular when visualizing high-dimensional plots into a 2D or 3D space. What gives it an advantage over PCA is that it's more suitable for non-linear data.\n",
    "\n",
    "t-SNE works by producing a joint probability distribution that effectively measures correlations between data. The basis for this distribution comes from calculating the euclidean distance between every datapoint pair. The smaller the distance means the higher probability that two points are similar.\n",
    "\n",
    "Next t-SNE initializes the output dimensions with random data and through a process to similar to gradient descent continously transforms the random data so that its joint probability distribution is as similar possible to that of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding t-SNE\n",
    "\n",
    "The most important parameter in t-SNE is *perplexity* which is used to set the number of neighbors that are used in calculating the joint distributions. The following image shows four different instances of t-SNE applied to the same dataset but with varying values for its perplexity parameter.\n",
    "\n",
    "![](https://tse2.mm.bing.net/th?id=OIP.C_e2LzgeM_TC7LcC15U_QQHaGj&w=690&c=7&pid=Api&p=0)\n",
    "\n",
    "Another important parameter is the number of components. We'll stick with 2 to facilitate 2D visualizations.\n",
    "\n",
    "For more how t-SNE works and how parameters impact its transformation check out [this excellent tutorial](https://distill.pub/2016/misread-tsne/).\n",
    "\n",
    "Now let's apply t-SNE to the world happiness data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine X to drop rank, country, and continent\n",
    "X = happiness.iloc[:, 2:-1]\n",
    "# Scale the data\n",
    "scale = StandardScaler()\n",
    "Xs = scale.fit_transform(X)\n",
    "Xs = pd.DataFrame(Xs, columns=X.columns)\n",
    "Xs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We imported TSNE at the beginning of the notebook\n",
    "tsne = TSNE(n_components=2, perplexity=40, random_state=212, learning_rate=1)\n",
    "Xt = tsne.fit_transform(Xs)\n",
    "Xt = pd.DataFrame(Xt, columns=[\"tsne1\", \"tsne2\"])\n",
    "Xt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now reproduce the earlier plot where we visualize the country dots color-encoded by continent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add continent to the pca dataset\n",
    "Xt[\"continent\"] = happiness[\"continent\"].tolist()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for continent in Xt.continent.unique():\n",
    "    data = Xt[Xt.continent == continent]\n",
    "    plt.scatter(x=data[\"tsne1\"], y = data[\"tsne2\"], label = continent, s=80)\n",
    "    plt.xlabel(\"TSNE 1\")\n",
    "    plt.ylabel(\"TSNE 2\")\n",
    "    plt.legend(fontsize = \"large\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is t-SNE doing? t-SNE looks at a point and asks, \"Who are my 40 closest neighbors?\" (we set `perplexity=40`). It then tries to arrange the points in 2D so that those neighbors stay close together.\n",
    "\n",
    "Unlike PCA, which is a mathematical \"rotation\" of the data, t-SNE is a simulation. It starts with dots in random positions and \"wiggles\" them around until the 2D distances match the high-dimensional neighbors as closely as possible. Points that are close in high-dimensional space should also appear close in the 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare to the PCA plot\n",
    "pca = PCA(n_components=2)\n",
    "#Fit and transform\n",
    "Xp = pca.fit_transform(Xs)\n",
    "Xp = pd.DataFrame(Xp, columns=[\"comp1\", \"comp2\"])\n",
    "Xp[\"continent\"] = happiness[\"continent\"].tolist()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for continent in Xp.continent.unique():\n",
    "    data_p = Xp[Xp.continent == continent]    \n",
    "    plt.scatter(x=data_p[\"comp1\"], y = data_p[\"comp2\"], label = continent, s=80)\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.legend(fontsize = \"large\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How do the figures appear to relate to each other? Are they capturing similar dimensions?\n",
    "\n",
    "t-SNE often produces results that look similar to PCA at a glance, but the philosophy behind how those dots got there is fundamentally different. While PCA tries to preserve the global structure (the big distances/variance), t-SNE is obsessed with local structure (nearest neighbors). \n",
    "\n",
    "How to interpret the results? With PCA, the axes actually mean something - the analysis is global. If a country moves from the left to the right, you can say its development/happiness is increasing. The distance between the \"Europe\" cluster and the \"Africa\" cluster is a real representation of their mathematical difference.\n",
    "\n",
    "With t-SNE the analysis local, so the axes (tsne1, tsne2) are essentially meaningless. You cannot say \"moving right means more happiness.\" What matters is proximity. If two countries are in the same little cluster, they are very similar across all features. The size of clusters and the distance between clusters can be misleading. t-SNE tends to \"expand\" dense clusters and \"shrink\" sparse ones to make the map look nice.\n",
    "\n",
    "If the t-SNE and PCA plots look almost identical, it usually means the data has a very strong, linear structure (like the wealth-happiness correlation) that both algorithms are picking up easily. t-SNE is non-linear and can \"bend\" and \"twist\" the data. If the happiness data had a curved relationship (e.g., happiness increases with wealth only up to a point, then plateaus), t-SNE would capture that difference better than PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive plots\n",
    "\n",
    "These charts are nice, but what would really be great would be to be able to hover the mouse over the dots to reveal more information about the country dots.\n",
    "\n",
    "We can use the [ploty-express plotting](https://plotly.com/python/plotly-express/) package to create interactive visualizations.\n",
    "\n",
    "Let's create an interactive 2D plot using plotly-express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotly-express\n",
    "import plotly.express as px\n",
    "\n",
    "#Add the country name into Xt\n",
    "Xt[\"country\"] = happiness[\"country\"].tolist()\n",
    "\n",
    "#intialize plotting function\n",
    "fig = px.scatter(Xt, x=\"tsne1\", y = \"tsne2\", color=\"continent\", hover_data=[\"country\"])\n",
    "#generate plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hover your mouse over the dots to see which countries they represent.\n",
    "\n",
    "Let's improve the plot by making the dots larger and showing the original data when you hover over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add size variable to the dataframe\n",
    "Xt[\"size\"] = .3\n",
    "\n",
    "#Add happiness score and gdp to Xt\n",
    "Xt[\"happiness\"] = happiness.happiness_score.tolist()\n",
    "Xt[\"gdp\"] = happiness.gdp.tolist()\n",
    "\n",
    "#This dictionary allows us to turn on and turn off our chosen variables\n",
    "hover_data={\"country\":True, \n",
    "            \"continent\":True, \n",
    "            \"happiness\":True,\n",
    "            \"gdp\":True,\n",
    "            \"tsne1\":False,\n",
    "            \"tsne2\":False,\n",
    "            \"size\":False}\n",
    "\n",
    "#intialize plotting function\n",
    "fig = px.scatter(Xt, x=\"tsne1\", y = \"tsne2\", color=\"continent\", \n",
    "                 hover_data=hover_data, size=\"size\", opacity=.6)\n",
    "#generate plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look now?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
