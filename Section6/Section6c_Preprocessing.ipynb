{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebbc7db",
   "metadata": {},
   "source": [
    "# Section 6. Preprocessing\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n",
    "\n",
    "The content of this notebook draws on material from UC Berkeley D-Lab's Python Machine Learning [course](https://github.com/dlab-berkeley/Python-Machine-Learning).\n",
    "\n",
    "Preprocessing is the process of data cleaning and preparation for analysis. This is an essential step for any data work, and no less for the machine learning workflow and the performance of models. This notebook will introduce the major steps of preprocessing for machine learning. \n",
    "\n",
    "\n",
    "### Sections\n",
    "\n",
    "1. Missing data\n",
    "2. Processing categorical data: dummy encoding\n",
    "3. Processing continuous data: outliers and normalization\n",
    "\n",
    "### Required packages\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* scikit-learn\n",
    "\n",
    "### Required data\n",
    "* penguins.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4378b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10590fd",
   "metadata": {},
   "source": [
    "For today, we will be working with the `penguins` data set, a common public data set for teaching visualization and exploration. This data set is from [Kaggle](https://www.kaggle.com/parulpandey/penguin-dataset-the-new-iris) and includes data on penguins of three different species, their location, and some measurements for each penguin.\n",
    "\n",
    "First, let's import some packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c244fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300dbcd1",
   "metadata": {},
   "source": [
    "Now, let's load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de206905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/penguins.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260ae68",
   "metadata": {},
   "source": [
    "Below is the information for each of the columns:\n",
    "1. **species**: Species of penguin [Adelie, Chinstrap, Gentoo]\n",
    "2. **island**: Island where the penguin was found [Torgersen, Biscoe]\n",
    "3. **culmen_length_mm**: Length of upper part of penguin's bill (millimeters)\n",
    "4. **culmen_depth_mm**: Height of upper part of bill (millimeters)\n",
    "5. **flipper_length_mm**: Length of penguin flipper (millimeters)\n",
    "6. **body_mass_g**: Body mass of the penguin (grams)\n",
    "7. **sex**: Biological sex of the penguin [MALE, FEMALE]\n",
    "\n",
    "*Question:* Which of the columns are continuous? Which are categorical?\n",
    "\n",
    "So far our models have treated the two variable types as continuous. In this notebook we will cover approaches to properly preprocess categorical data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b61930",
   "metadata": {},
   "source": [
    "## 1. Missing Data Preprocessing\n",
    "\n",
    "First, let's check to see if there are any missing values in the data set. Missing values are represented by `NaN`. \n",
    "\n",
    "*Question:* In this case, what do missing values stand for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30296f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe1c9f",
   "metadata": {},
   "source": [
    "It is also possible to have non `NaN` missing values. For example, let's take a look at the `sex` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97753d4d",
   "metadata": {},
   "source": [
    "In this case, the `.` represents a missing value, so let's replace those with `np.nan` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('.', np.nan, inplace=True)\n",
    "\n",
    "data['sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c975f09",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "In the case of missing values, we have the option to fill in the missing values with the best guess. `sklearn` has a function `SimpleImputer` that has flexible options for how to approach this.\n",
    "\n",
    "There are many strategies that can be used to impute missing data ([see function documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)), some of which we have discussed previously.\n",
    "\n",
    "Here we'll impute any missing values for two selected variables using the average, or mean, of all the data that does exist for those variables -- this is making a best guess for what the values would have been. We'll then save this as a new dataset, rather than overwriting the original data.\n",
    "\n",
    "Let's see how the `SimpleImputer` works on a subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan,\n",
    "                        strategy='mean', \n",
    "                        copy=True)\n",
    "imputed = imputer.fit_transform(data[['body_mass_g','flipper_length_mm']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299fa288",
   "metadata": {},
   "source": [
    "Now let's check that the previously null values have been filled in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212cb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imputed[data[data['body_mass_g'].isna()].index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8458bf",
   "metadata": {},
   "source": [
    "### Dropping Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281496e",
   "metadata": {},
   "source": [
    "Another option option is to use `pd.dropna()` to drop `Null` values from the `DataFrame`. This should almost always be used with the `subset` argument which restricts the function to only dropping values that are null in a certain column(s).\n",
    "\n",
    "This is probably safer in the context of wanting to do prediction, because imputing values will introduce noise in the prediction process. The downside is that dropping values reduces your sample size and may reduce generalizability of the models if the observations with missing data have some common characteristics.\n",
    "\n",
    "Here we will actually overwrite the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c465f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['sex'])\n",
    "\n",
    "# Now this line will return an empty dataframe\n",
    "data[data['sex'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70140ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are now 11 fewer rows\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c8e2b",
   "metadata": {},
   "source": [
    "Note that it is not strictly necessary to drop observations with missing data. Any model that includes a variable with missing data will automatically drop the observations with missing data from the variable from the estimation. It is potentially important if you are running analyses on subsets of variables, and want to make sure you always include the same observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9b3c7",
   "metadata": {},
   "source": [
    "## 2. Categorical Data Processing\n",
    "\n",
    "As we saw earlier, the `penguins` dataset contains both categorical and continuous features, which will each need to be preprocessed in different ways. First, we want to transform the categorical variables from strings to **indicator variables**. \n",
    "\n",
    "Indicator variables have one column per level. For example, the island variable will change from Biscoe/Dream/Torgersen --> Biscoe (1/0), Dream (1/0), and Torgerson (1/0). For each set of indicator variables, there should be a 1 in exactly one column.\n",
    "\n",
    " Let's make a list of the categorical variable names to be transformed into indicator variables, and save a dataset of just these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eabc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable names that are categorical for use later\n",
    "cat_var_names = ['species','island', 'sex']\n",
    "data_cat = data[cat_var_names]\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a031b03",
   "metadata": {},
   "source": [
    "### Categorical Variable Encoding (One-hot & Dummy)\n",
    "\n",
    "Many machine learning algorithms require that categorical data be encoded numerically in some fashion. There are two main ways to do so:\n",
    "\n",
    "\n",
    "- **One-hot-encoding**, which creates `k` new variables for a single categorical variable with `k` categories (or levels), where each new variable is coded with a `1` for the observations that contain that category, and a `0` for each observation that doesn't. \n",
    "- **Dummy encoding**, which creates `k-1` new variables for a categorical variable with `k` categories\n",
    "\n",
    "When using some machine learning algorithms we can run into the so-called [\"Dummy Variable Trap\"](https://www.algosome.com/articles/dummy-variable-trap-regression.html) when using One-Hot-Encoding on multiple categorical variables within the same set of features. This occurs because each set of one-hot-encoded variables can be added together across columns to create a single column of all `1`s, and so are multi-colinear when multiple one-hot-encoded variables exist within a given model. This can lead to misleading results. \n",
    "\n",
    "To resolve this, we can simply add an intercept term to our model (which is all `1`s) and remove the first one-hot-encoded variable for each categorical variables, resulting in `k-1` so-called \"Dummy Variables\". \n",
    "\n",
    "Luckily the `OneHotEncoder` from `sklearn` can perform both one-hot and dummy encoding simply by setting the `drop` parameter (`drop = 'first'` for Dummy Encoding and `drop = None` for One Hot Encoding). This approach is useful because you can save the encoder object to apply the same transformation to additional data.\n",
    "\n",
    "We'll first use the `OneHotEncoder` approach, and then show ways to do this manually or with a pandas method.\n",
    "\n",
    "**Question:** How many total columns will there be in the output for the below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f97566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "dummy_e = OneHotEncoder(categories='auto', drop='first', sparse_output=False) # sparse_output=False for scikit-learn v> 1.2; else sparse=False\n",
    "dummy_e.fit(data_cat);\n",
    "dummy_e.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b861e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return array of values\n",
    "temp = dummy_e.transform(data_cat)\n",
    "# Get column names\n",
    "column_names = dummy_e.get_feature_names_out(data_cat.columns)\n",
    "# Create dataframe\n",
    "data_dummy = pd.DataFrame(temp, columns=column_names, index=data_cat.index)\n",
    "data_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5ec48",
   "metadata": {},
   "source": [
    "We can also create encode categorical variables into dummy variables manually by looping through categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in data_cat['sex'].unique():\n",
    "    data_cat.loc[:, v] = (data_cat['sex'] == v)\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63e725-d641-4b59-920d-30300022f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the built-in pandas method - recommended\n",
    "data_cat = data[cat_var_names]\n",
    "data_cat = pd.get_dummies(data_cat, columns=['sex'], prefix='', prefix_sep='')\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af05b49",
   "metadata": {},
   "source": [
    "That loop was one-hot encoding. What if we want to do dummy encoding? We want $k-1$ new variables. We can tweak the loop to accomplish this by including an if statement and telling the loop to stop before the last unique categorical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ebba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6768df",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for v in data_cat['species'].unique():\n",
    "    if i < data_cat['species'].nunique():\n",
    "        data_cat.loc[:, v]=data_cat['species']==v\n",
    "        i += 1\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb53d076-29ac-4f1a-9cde-a30db2ed9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the built-in pandas method - recommended\n",
    "data_cat = data[cat_var_names]\n",
    "data_cat = pd.get_dummies(data_cat, columns=['species'], drop_first=True, prefix='', prefix_sep='')\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fcc26d-1d6d-421a-91fa-4a2c648089fc",
   "metadata": {},
   "source": [
    "### Boolean vs integer type\n",
    "\n",
    "In Python, True and False are subclasses of integers, where `True==1` and `False==0`. Most modern libraries will handle Booleans without crashing, but some libraries are very strict and expect floats/integers or will throw an error. It is therefore safer to cast Boolean variables to integers.\n",
    "\n",
    "This is quite easy to do in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90239c-f90b-4b55-9b0c-7474f68caf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols = data_cat.select_dtypes(include='bool').columns\n",
    "data_cat[bool_cols] = data_cat[bool_cols].astype(int)\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed0336",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Continuous Data Preprocessing\n",
    "\n",
    "There are many potential considerations for preparing continuous data for analysis. Right now we'll discuss dealing with outliers and normalization.\n",
    "\n",
    "Let's subset out the continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5dce80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_num = data.drop(columns=cat_var_names + ['species'])\n",
    "data_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6473b0",
   "metadata": {},
   "source": [
    "### Checking for outliers\n",
    "\n",
    "One prepocessing step that we discussed previously is checking for **outliers** and deciding how to treat those values.\n",
    "\n",
    "A simple approach to identifying potential outliers is plotting histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab267ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(8,8))\n",
    "\n",
    "data_num['culmen_length_mm'].hist(grid=False, ax=ax[0,0])\n",
    "data_num['culmen_depth_mm'].hist(grid=False, ax=ax[0,1])\n",
    "data_num['flipper_length_mm'].hist(grid=False, ax=ax[1,0])\n",
    "data_num['body_mass_g'].hist(grid=False, ax=ax[1,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df97111d",
   "metadata": {},
   "source": [
    "There are no obvious outliers for any of these variables. If there were, we would need to determine how to *define* an outlier (i.e., values outside some threshold, which may be a percentile of the distribution), and how to *process* the outliers (i.e., set to missing, set to the threshold value, set to the median, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a79f1",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Another processing approach that is often important for machine learning is normalizing our variables. This converts all variables into the same units and gives them a similar distribution, which helps improve performance of many machine learning models (see [here](https://en.wikipedia.org/wiki/Feature_scaling)).\n",
    "\n",
    "**Question**: Why might noramlization be particularly helpful for model regularization?\n",
    "\n",
    "[Normalization](https://en.wikipedia.org/wiki/Normalization_(statistics)) is a transformation that puts data into some known \"normal\" scale. There are many forms of normalization, but perhaps the most useful to machine learning algorithms is called the \"z-score\" also known as the standard score. This approach is also useful in econometrics, particularly when comparing effect sizes of different variables or on different outcomes. \n",
    "\n",
    "To z-score normalize the data, we simply subtract the mean of the data and divide by the standard deviation. This results in data with a mean of `0` and a standard deviation of `1`.\n",
    "\n",
    "We'll use the `StandardScaler` from `sklearn` to do normalization, but you can also code this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a5801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "norm_e = StandardScaler()\n",
    "norm_e.fit_transform(data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66993e",
   "metadata": {},
   "source": [
    "To check the normalization works, let's look at the mean and standard variation of the resulting columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a9abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean:',norm_e.fit_transform(data_num,).mean(axis=0))\n",
    "print('std:',norm_e.fit_transform(data_num,).std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b3690",
   "metadata": {},
   "source": [
    "## 4. Combine it all together\n",
    "\n",
    "Now let's combine what we've learned to preprocess the entire dataset.\n",
    "\n",
    "First we will reload the data set to start with a clean copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613debef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "data = pd.read_csv('Data/penguins.csv')\n",
    "# ensure all missing values are coded as np.nan\n",
    "data.replace('.', np.nan, inplace=True)\n",
    "# drop observations with missing sex\n",
    "data = data.dropna(subset=['sex'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065feef",
   "metadata": {},
   "source": [
    "We will now split the data into a training and test set. In the next notebook we will be developing a **classification model** to predict penguin species using other characteristics.\n",
    "\n",
    "We will **stratify the split** by species to ensure the training and test shares are the same within each species.\n",
    "\n",
    "Why do we do split the data before preprocessing? The best practice is to fit preprocessing methods on the training data. This avoids **data leakage** or influence of test data information on training data. For example, we don't want to impute means that include test data or normalize based on the distribution that includes the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split\n",
    "y = data['species']\n",
    "X = data.drop('species', axis =1, inplace=False)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=.25, stratify=y, random_state=212)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ddc194",
   "metadata": {},
   "source": [
    "We want to train our preprocessing protocols on the training data using the `fit_transform` function, then use the `transform` function to process the test data. This more closely resembles what the workflow would look like if you were bringing in brand new test data.\n",
    "\n",
    "First, we will subset out the categorical and numerical features separately. We will use the `select_dtypes` method in pandas, and use the fact that numerical types are referred to as 'number' and that categorical types can be referred to as 'object', 'category', or 'bool'. We include 'bool' variables here to ensure the preprocessing flow also converts these to 1/0 variables. \n",
    "\n",
    "We will define the sets of columns based on X_train to ensure the test set ends up with the exact same columns, even if a specific category is missing in one or the other samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d90a7-1e58-45bf-94ab-e2e809375240",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.select_dtypes(include=['number']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea032c39-d890-41d3-9663-50834b3ecb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.select_dtypes(include=['object', 'category', 'bool']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20079c7c-2eb2-42ac-819a-7bdec8654be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative\n",
    "X_train.select_dtypes(exclude=['number']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names based on X_train\n",
    "num_cols = X_train.select_dtypes(include=['number']).columns\n",
    "cat_cols = X_train.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Use those names to subset both sets\n",
    "X_train_num, X_train_cat = X_train[num_cols], X_train[cat_cols]\n",
    "X_test_num, X_test_cat = X_test[num_cols], X_test[cat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd850b",
   "metadata": {},
   "source": [
    "Now, let's process the categorical data with **Dummy encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ece48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_cat['island'].nunique())\n",
    "print(X_train_cat['sex'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical feature encoding\n",
    "dummy_e = OneHotEncoder(categories='auto', drop='first', sparse_output=False) # sparse_output=False for scikit-learn v> 1.2; else sparse=False\n",
    "X_train_dummy = dummy_e.fit_transform(X_train_cat) # train encoder on training data\n",
    "X_test_dummy = dummy_e.transform(X_test_cat) # apply encoding to test data\n",
    "\n",
    "# Check the shape\n",
    "X_train_dummy.shape, X_test_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f521eee",
   "metadata": {},
   "source": [
    "Is this the number of variables we expected?\n",
    "\n",
    "Now, let's process the numerical data by imputing any missing values using the mean in the training set and normalizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical feature standardization\n",
    "\n",
    "# Impute means for missing observations\n",
    "imputer = SimpleImputer(missing_values=np.nan,\n",
    "                        strategy='mean', \n",
    "                        copy=True)\n",
    "X_train_imp = imputer.fit_transform(X_train_num)\n",
    "X_test_imp = imputer.transform(X_test_num)\n",
    "\n",
    "# Check for missing values\n",
    "np.isnan(X_train_imp).any(), np.isnan(X_test_imp).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7963ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "norm_e = StandardScaler()\n",
    "X_train_norm = norm_e.fit_transform(X_train_imp)\n",
    "X_test_norm = norm_e.transform(X_test_imp)\n",
    "\n",
    "X_train_norm.shape, X_test_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847d320",
   "metadata": {},
   "source": [
    "Now that we've processed the numerical and categorical data separately, we can put the two arrays back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147055c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train_dummy, X_train_norm))\n",
    "X_test = np.hstack((X_test_dummy, X_test_norm))\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a362a39e",
   "metadata": {},
   "source": [
    "Finally, let's save our results as separate `.csv` files, so we won't have to run the preprocessing again. We will use these files later.\n",
    "\n",
    "First we will make them DataFrames, then add column names, and then save them as .csv files. Note that the order of column names is based on how we stacked the categorical and continuous data sets together, and how the one-hot encoding generated the dummy variables based on the island and sex categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558512f-830c-41e8-a529-fbadcaa4296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the Categorical Column Names (from the OneHotEncoder)\n",
    "# This handles the 'drop=first' logic automatically\n",
    "cat_cols_out = dummy_e.get_feature_names_out(X_train_cat.columns)\n",
    "\n",
    "# 2. Get the Numerical Column Names\n",
    "# These don't change, so we just use the original names from the numeric subset\n",
    "num_cols_out = X_train_num.columns.tolist()\n",
    "\n",
    "# 3. Combine the column names in the EXACT order of np.hstack\n",
    "# Order: Dummy columns + Normalized columns\n",
    "all_col_names = list(cat_cols_out) + num_cols_out\n",
    "\n",
    "# 4. Convert data to DataFrames and add columns labels\n",
    "# We use the index from the original cat/num subsets to keep row alignment\n",
    "X_train_final = pd.DataFrame(X_train, columns=all_col_names, index=X_train_cat.index)\n",
    "X_test_final = pd.DataFrame(X_test, columns=all_col_names, index=X_test_cat.index)\n",
    "\n",
    "# 5. Prepare the test data\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train.columns = ['species']\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test.columns = ['species']\n",
    "\n",
    "# 5. Save to CSV\n",
    "X_train_final.to_csv('Data/penguins_X_train.csv')\n",
    "X_test_final.to_csv('Data/penguins_X_test.csv')\n",
    "y_train.to_csv('Data/penguins_y_train.csv')\n",
    "y_test.to_csv('Data/penguins_y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8eba52",
   "metadata": {},
   "source": [
    "Although now we will move on to talk about classification, all of the choices we make in the preprocessing pipeline are extremely important to machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
