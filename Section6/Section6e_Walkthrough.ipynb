{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6. Supervised ML Walkthrough\n",
    "\n",
    "#### Instructor: Pierre Biscaye \n",
    "\n",
    "In this notebook, we're going to execute a machine learning project from start to finish. We'll use techniques covered in the previous section to facilitate this process, but we'll also introduce some new concepts. The goal is to demonstrate a basic machine learning pipeline.\n",
    "\n",
    "### Required packages\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* scikit-learn\n",
    "* seaborn\n",
    "* xgboost\n",
    "\n",
    "### Required data\n",
    "* heart_2020_cleaned_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Pipeline/Sections\n",
    "\n",
    "We'll take the following steps to develop our machine learning models:\n",
    "\n",
    "1. Introduce Dataset and Objectives\n",
    "2. Exploratory Data Analysis, Feature Engineering, and Preprocessing\n",
    "    - Produce several plots to give us a better understanding of the data.\n",
    "    - Split the sample.\n",
    "    - Perform feature engineering and preprocessing.\n",
    "    - Use the pipeline tool for speedy and reproducible preprocessing.\n",
    "3. Modeling Process\n",
    "    - Train three different models: Logistic Regression, Decision Trees, and Random Forest.\n",
    "    - Learn about and apply the grid search method for choosing hyperparameters.\n",
    "4. Evaluation and Interpretation\n",
    "    - Evaluate the models using a variety of metrics.\n",
    "    - Discuss how successful we were with our modeling.\n",
    "5. Bonus: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduce Dataset and Objectives\n",
    "\n",
    "We are going to be using a Kaggle dataset called [\"Personal Key Indicators of Heart Disease\"](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease).\n",
    "\n",
    "This dataset consists of 2020 annual CDC survey data from 400,000 adults related to their health status with regard to heart disease.\n",
    "\n",
    "Below, we provide an edited description of the data, taken from the Kaggle data description.\n",
    "\n",
    "### What topic does the dataset cover?\n",
    "\n",
    "According to the CDC, heart disease is one of the leading causes of death for people of most races in the USA (African Americans, American Indians and Alaska Natives, and white people). About half of all Americans (47%) have at least 1 of 3 key risk factors for heart disease: high blood pressure, high cholesterol, and smoking. Other key indicators include diabetic status, obesity (high BMI), not getting enough physical activity or drinking too much alcohol. \n",
    "\n",
    "Detecting and preventing the factors that have the greatest impact on heart disease is very important in healthcare. Computational developments allow the application of machine learning methods to detect \"patterns\" from the data that can predict a patient's condition.\n",
    "\n",
    "### Where did the dataset come from?\n",
    "\n",
    "Originally, the dataset came from the CDC and is a major part of the Behavioral Risk Factor Surveillance System (BRFSS), which conducts annual telephone surveys to gather data on the health status of U.S. residents. The dataset we are using includes data from 2020. The original dataset consists of 401,958 rows and 279 columns.\n",
    "\n",
    "The vast majority of columns are questions asked to respondents about their health status, such as \"Do you have serious difficulty walking or climbing stairs?\" or \"Have you smoked at least 100 cigarettes in your entire life?\".\n",
    "\n",
    "The dataset available for this course includes just a small set of the most relevant variables for heart disease risk (from a theoretical/medical perspective). The original dataset of nearly 300 variables was reduced to just about 20 variables, for computational tractability for this course. It has also undergone some basic cleaning so that it would be usable for machine learning projects, such as dealing with missing data.\n",
    "\n",
    "### What can you do with this dataset?\n",
    "\n",
    "This dataset can be used to apply a range of machine learning methods, most notably classifier models since the target variable \"HeartDisease\" is binary (\"Yes\" - respondent had heart disease; \"No\" - respondent had no heart disease). \n",
    "\n",
    "We note that classes are not balanced, so the classic model application approach is not advisable. Fixing the weights/stratified sampling should yield significantly better results. \n",
    "\n",
    "### Data Dictionary\n",
    "\n",
    "The features available in the dataset are shown in the table below. The first variable, **HeartDisease**, is the target variable. We aim to predict whether **HeartDisease** is true or false. \n",
    "\n",
    "| Feature     | Description |\n",
    "| ----------- | ----------- |\n",
    "| **HeartDisease**       | Respondents that have ever reported having coronary heart disease (CHD) or myocardial infarction (MI)    |\n",
    "| **BMI**   | Body Mass Index (BMI)        |\n",
    "| **Smoking** | Have you smoked at least 100 cigarettes in your entire life? |\n",
    "| **AlcoholDrinking** | Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week |\n",
    "| **Stroke** | Ever had a stroke? |\n",
    "| **PhysicalHealth** | Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 was your physical health not good. |\n",
    "| **MentalHealth** | Thinking about your mental health, for how many days during the past 30 days was your mental health not good? |\n",
    "| **DiffWalking** | Do you have serious difficulty walking or climbing stairs? |\n",
    "| **Sex** | Sex Assigned at Birth | \n",
    "| **AgeCategory** |  Fourteen-level age category |\n",
    "| **Race** | Race and ethnicity |\n",
    "| **Diabetic** | Have you ever had diabetes? |\n",
    "| **PhysicalActivity** | Adults who reported doing physical activity or exercise during the past 30 days other than their regular job |\n",
    "| **GenHealth** | Would you say that in general your health is...|\n",
    "| **SleepTime** | On average, how many hours of sleep do you get in a 24-hour period?|\n",
    "| **Asthma** | Have you ever had asthma?|\n",
    "| **KidneyDisease** | Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease? |\n",
    "| **SkinCancer** | Have you ever had skin cancer? |\n",
    "\n",
    "### What is our objective?\n",
    "\n",
    "Our objective is to use a variety of demographic, health, and behavioral data to predict if the patients in this dataset have or ever had heart disease.\n",
    "\n",
    "### Import the Dataset\n",
    "\n",
    "We'll begin by importing needed libraries and functions, importing the dataset, and taking a look at the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Import functions from scikit-learn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             confusion_matrix,\n",
    "                             classification_report,\n",
    "                             f1_score,\n",
    "                             precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import (cross_val_score,\n",
    "                                     cross_val_predict,\n",
    "                                     StratifiedKFold,\n",
    "                                     GridSearchCV,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     train_test_split)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (LabelEncoder,\n",
    "                                   OneHotEncoder,\n",
    "                                   StandardScaler)\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `pandas` to import the dataset. Be sure to use the correct file path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/heart_2020_cleaned_sample.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 13 categorical features (though many are binary), 4 continuous featues, and one categorical (binary) target variable.\n",
    "\n",
    "The preprocessing of the dataset before it was posted including dealing with missing values - we will have to accept whatever method was used in order to work with these data. If we had missing values, we would have to make sure that all are coded as NaN and decide what imputation (if any) to do with missing continuous variable values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many null values per column?\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis and Feature Engineering\n",
    "\n",
    "Before we jump into modeling, it's important to get to know our data. This will help motivate the features we use, any additional features we construct, and how we perform preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Let's first get a sense of the distributions of the variables in the dataset. We'll start with the numerical data. Let's plot histograms of these features. Notice that, in some plots, we use a log-scale for the $y$-axis. Try turning it off to see how the distribution looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the numeric features\n",
    "df_numeric = df.select_dtypes(\"number\")\n",
    "numeric_features = df_numeric.columns\n",
    "df_numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BMI\n",
    "sns.histplot(data=df_numeric, x='BMI', bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PhysicalHealth\n",
    "sns.histplot(data=df_numeric, x='PhysicalHealth', bins=10)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MentalHealth\n",
    "sns.histplot(data=df_numeric, x='MentalHealth', bins=10)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SleepTime\n",
    "sns.histplot(data=df_numeric, x='SleepTime', bins=20)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot\n",
    "corr = df_numeric.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlations as a heatmap\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# Plot a heatmap using seaborn\n",
    "# Include the mask and correct aspect ratio, and a diverging colormap\n",
    "sns.heatmap(corr, mask=mask, cmap='RdBu', vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical = df.select_dtypes(exclude=['number'])\n",
    "categorical_features = df_categorical.columns\n",
    "df_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the number of unique values for each categorical feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distributions of all the categorical features.\n",
    "\n",
    "Note that we're doing this in a single set of subplots using `matplotlib`. There's a lot of code here - don't stress too much about the details. Instead, focus on the plot output and what the distribution of the variables look like. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose 3 rows - feel free to adjust\n",
    "nrows = 3\n",
    "# Number of columns chosen automatically based on number of features\n",
    "ncols = categorical_features.size // 3 + 1\n",
    "\n",
    "# Create subplots using matplotlib\n",
    "fig, axes = plt.subplots(nrows=3, ncols=ncols, figsize=(nrows * 9, ncols * 2.5))\n",
    "# Adjust subplot spacing\n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "\n",
    "# Iterate over categorical features\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    # Choose axis for features\n",
    "    ax = axes[idx // ncols, idx % ncols]\n",
    "    # Calculate proportions and plot bars\n",
    "    df_categorical[feature].value_counts(normalize=True).sort_index().plot(\n",
    "        kind='bar',\n",
    "        ax=ax)\n",
    "    # Rotate x ticks\n",
    "    ax.tick_params(axis='x', rotation=0)\n",
    "    # Set y limits\n",
    "    ax.set_ylim([0, 1])\n",
    "    # Create title for plot\n",
    "    ax.set_title(feature)\n",
    "\n",
    "# Turn off unused plot\n",
    "axes[-1, -1].axis(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on readability: some of the labels are hard to read. Below is code we could have used to make specific subplots more readable. Copy it in above, before the line to turn off the unused plot, to see how they change the plots. The code rotates the labels for specific subplots, and also edits certain labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjustments to single plots\n",
    "# axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=40, ha='right', fontsize=14)\n",
    "# cur_xticks = axes[1, 2].get_xticklabels()\n",
    "# cur_xticks[0] = 'AI/AN'\n",
    "# axes[1, 2].set_xticklabels(cur_xticks, rotation=40, ha='right')\n",
    "# axes[1, 1].set_ylim([0, 0.12])\n",
    "# cur_xticks = axes[1, 3].get_xticklabels()\n",
    "# cur_xticks[1] = 'Borderline'\n",
    "# cur_xticks[3] = 'During\\nPregnancy'\n",
    "# axes[1, 3].set_xticklabels(cur_xticks, rotation=40, ha='right')\n",
    "# axes[2, 0].set_xticklabels(axes[2, 0].get_xticklabels(), rotation=40, ha='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how these features correlate with the target variable.\n",
    "\n",
    "For the continuous variables, for example, we can examine the distribution of each feature separately for the samples where the patient had heart disease and the samples where the patient did not have heart disease.\n",
    "\n",
    "We can use a `seaborn` histogram with a `hue` argument to compare this directly. Pay attention to what arguments are passed into the function. What do these plots tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='BMI', hue='HeartDisease', stat='density', bins=50, common_norm=False)\n",
    "plt.xlim([10, 60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='PhysicalHealth', hue='HeartDisease', stat='density', bins=10, common_norm=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='MentalHealth', hue='HeartDisease', stat='density', bins=10, common_norm=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='SleepTime', hue='HeartDisease', stat='density', bins=20, common_norm=False)\n",
    "plt.xlim([0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the categorical data, we'll plot the average `HeartDisease` rate by each unique value of the variable. For example, consider how heart disease varies with smoking. Let's convert the heart disease feature into a binary label to make this easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable for heart disease\n",
    "df_categorical['HeartDiseaseBinary'] = np.where(df_categorical['HeartDisease'] == 'Yes', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we group the samples by smoking, and calculate the heart disease rate for each group by averaging across the `HeartDiseaseBinary` variable. We can then visualize the rates as a horizontal bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.groupby(\"Smoking\")['HeartDiseaseBinary'].mean().plot(kind = \"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this same procedure for all categorical features. Once again, don't worry too much about the code. Focus instead on what the data is telling you. What correlates with heart disease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose 3 rows - feel free to adjust\n",
    "nrows = 3\n",
    "# Number of columns chosen automatically based on number of features\n",
    "categorical_predictors = [feature for feature in df_categorical.columns\n",
    "                          if 'HeartDisease' not in feature]\n",
    "ncols = len(categorical_predictors) // 3 + 1\n",
    "\n",
    "# Create subplots using matplotlib\n",
    "fig, axes = plt.subplots(nrows=3, ncols=ncols, figsize=(nrows * 9, ncols * 2.5))\n",
    "# Adjust subplot spacing\n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "\n",
    "# Iterate over categorical features\n",
    "for idx, feature in enumerate(categorical_predictors):\n",
    "    # Make sure we skip over the heart disease features\n",
    "    if 'HeartDisease' not in feature:\n",
    "        # Choose axis for features\n",
    "        ax = axes[idx // ncols, idx % ncols]\n",
    "        # Calculate proportions and plot bars\n",
    "        df_categorical.groupby(feature)['HeartDiseaseBinary'].mean().sort_index().plot(kind='bar', ax=ax)\n",
    "        # Remove x label\n",
    "        ax.set_xlabel('')\n",
    "        # Rotate x ticks\n",
    "        ax.tick_params(axis='x', rotation=0)\n",
    "        # Create title for plot\n",
    "        ax.set_title(feature)\n",
    "\n",
    "# Adjustments to single plots\n",
    "axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=40, ha='right', fontsize=14)\n",
    "cur_xticks = axes[1, 1].get_xticklabels()\n",
    "cur_xticks[0] = 'AI/AN'\n",
    "axes[1, 1].set_xticklabels(cur_xticks, rotation=40, ha='right')\n",
    "cur_xticks = axes[1, 2].get_xticklabels()\n",
    "cur_xticks[1] = 'Borderline'\n",
    "cur_xticks[3] = 'During\\nPregnancy'\n",
    "axes[1, 2].set_xticklabels(cur_xticks, rotation=40, ha='right')\n",
    "axes[1, 4].set_xticklabels(axes[1, 4].get_xticklabels(), rotation=40, ha='right')\n",
    "# Turn off unused plots\n",
    "axes[-1, -2].axis(False)\n",
    "axes[-1, -1].axis(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Preprocessing\n",
    "\n",
    "The exploratory data analysis (EDA) suggests nearly all the features correlate with the target variable; this should not be surprising as they were specifically chosen from a larger dataset because they might be relevant for predicting heart disease. This would not be true in other cases, when EDA could help you sort through a larger set of features to identify a possible subset for modeling. \n",
    "\n",
    "Of course, an alternative approach is to include all features in the initial models and include penalties (such as in the lasso algorithm) to guide which features to ultimately include. But even in that situation, understanding relationships in the data is useful.\n",
    "\n",
    "One reason it can still be useful is to help indicate the nature of the relationships between a feature and the target. Might it be quadratic or some higher order polynomial? Might it vary in some way across the distribution of the feature? Might it vary based on values of another feature? Such findings can inform your feature engineering.\n",
    "\n",
    "Feature engineering is at the heart of machine learning, and there's no single way to do it. **Feature engineering** is the process of constructing new features that we think might be informative about the predictor variable. It could mean taking categorical data and one-hot encoding it, creating interaction terms, and preprocessing. These are all steps we take *prior* to fitting a model in order to make the data more suitable for prediction.\n",
    "\n",
    "We're going to do limited feature engineering in the interest of time. But you should always think about what useful features may exist/can be constructed while working with data. Specifically, we will:\n",
    "\n",
    "- Adjust the age features,\n",
    "- Label encode the target variable,\n",
    "- Scale the numerical features,\n",
    "- One-hot encode the categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll adjust the age variable into a pseudo-continuous variable. We'll do this because the age category feature has 13 unique values, which is quite a lot for a categorical variable. Furthermore, age has an ordered structure (increasing 5 year bins), which we lose when we use the categorical formulation. What we'll do is replace each age value with the lower limit of the age range. We lose some information this way, but sometimes there's a cost to preprocessing.\n",
    "\n",
    "We will also include a quadratic age term - this is common in many models and seems relevant based on the plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique age category values\n",
    "df[\"AgeCategory\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"Age\" column by taking the left number (remember, it's a string) and converting it to a float\n",
    "df['Age'] = df['AgeCategory'].str[:2].astype(float)\n",
    "df['Age'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age squared\n",
    "df['Agesq'] = df['Age']**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remove the age category as well as the heart disease column to obtain a \"design matrix\". We'll also extract the heart disease column into its own dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"HeartDisease\", \"AgeCategory\"], axis=1).copy()\n",
    "y = df[\"HeartDisease\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we scale and one hot encode data, let's first split it into training and validation datasets. \n",
    "\n",
    "Why do we do this? All preprocessing should be done separately on the training and test (or validation) set. We'll use `sklearn`'s `train_test_split` function to perform the split, **stratifying** by values of HeartDisease because the sample is not balanced.\n",
    "\n",
    "**Question:** What does this stratification ensure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_valid_raw, y_train_raw, y_valid_raw = train_test_split(X, y, \n",
    "                                                                      test_size=0.25, \n",
    "                                                                      random_state=212, \n",
    "                                                                      stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we converted the heart disease feature into a binary feature using a `numpy` function. Now, we'll do it again, using a `scikit-learn` function that does the same thing. The benefit to using the `LabelEncoder` is that `scikit-learn` does all the work for us. It will also give us an object that can be applied to new data. For example, we can fit the `LabelEncoder` to the training data, and apply it to the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize label encoder\n",
    "labeler = LabelEncoder()\n",
    "# Fit and transform the target variable from the training set\n",
    "y_train = labeler.fit_transform(y_train_raw)\n",
    "# Transform the validation target variable\n",
    "y_valid = labeler.transform(y_valid_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What classes did we obtain?\n",
    "print(labeler.classes_)\n",
    "# Confidence check: does it work?\n",
    "print(labeler.transform([\"No\", \"Yes\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to perform additional preprocessing, we're going to create a `scikti-learn` `Pipeline`. The **Pipeline** allows us to compose multiple steps into a single object, which we can then fit and apply to multiple datasets. Let's take it one step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all features\n",
    "feature_cols = X_train_raw.columns\n",
    "# Identify numeric features\n",
    "numeric_cols = X_train_raw.select_dtypes(\"number\").columns.tolist()\n",
    "# Identify categorical features\n",
    "categorical_cols = X_train_raw.select_dtypes(exclude=\"number\").columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every `Pipeline` is composed of steps. Each step is a tuple of two elements: the first tells us the name, and the second tells us what transformation is happening. We can create a `Pipeline` by stitching together steps via a list. We can also create a `Pipeline` by stitching together smaller `Pipeline`s.\n",
    "\n",
    "The tricky thing about a `Pipeline` is that it applies a transformation to all the data. This won't work in cases with heterogeneous data. For example, we don't want to one-hot encode continuous features, and we don't want to standardize categorical features. So, we need one more tool: the `ColumnTransformer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pipeline to create a numeric transformer: standard scaling/normalization\n",
    "numeric_transformer = Pipeline([(\"scaler\", StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pipeline to create a categorical transformer: one hot encoding\n",
    "categorical_transformer = Pipeline(\n",
    "    [(\"one_hot_encoder\",\n",
    "      OneHotEncoder(categories='auto', \n",
    "                    handle_unknown='error', \n",
    "                    sparse_output=False, # sparse_output=False for scikit-learn v> 1.2; else sparse=False\n",
    "                    drop=\"first\"))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we create the overall preprocessor with the ColumnTransformer.\n",
    "# The ColumnTransformer is itself a Pipeline, and needs steps (i.e., a list). \n",
    "# In this case, each step needs a tuple with length 3:\n",
    "# 1. The name of the step.\n",
    "# 2. The Pipeline to apply at that step.\n",
    "# 3. The columns to apply the step to.\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    # First step: numeric features\n",
    "    (\"numeric\", numeric_transformer, numeric_cols),\n",
    "    # Second step: categorical features\n",
    "    (\"categorical\", categorical_transformer, categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines can also have classifiers (e.g., a `LogisticRegression`) included as well. In that case, the output of the pipeline would be a trained model. For now, however, we'll simply just preprocess the data using the `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit transform the train dataset using the pipeline\n",
    "X_train = preprocessor.fit_transform(X_train_raw)\n",
    "# Transform the testing dataset with the rules learned from the training dataset\n",
    "X_valid = preprocessor.transform(X_valid_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why do we use `fit_transform` for the training data, and only `transform` for the validation data? \n",
    "\n",
    "Now, we have our data preprocessed. Notice how easy, clean, and reproducible this process was. This demonstrates the value of using the `Pipeline` to conduct machine learning analyses (and more generally). We can quickly and cleanly transform any new batches of data to confirm to the rules established by the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View result\n",
    "print(X_train.shape)\n",
    "print(X_train)\n",
    "print(X_train_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output of the preprocessor is a `numpy` array with more columns than the original dataframe. This is because the one-hot encoding created some new columns. It'd be nice if we had this data in a data frame, with named columns. So, the last step we'll do is convert this back into a data frame. First, we need to access the new column names, which we can do with the `get_feature_names_out` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For scikit-learn>1.0\n",
    "preprocessor.named_transformers_['categorical']['one_hot_encoder'].get_feature_names_out(categorical_cols).tolist()\n",
    "#For scikit-learn<1.0\n",
    "#preprocessor.named_transformers_['categorical']['one_hot_encoder'].get_feature_names(categorical_cols).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access pipeline data to so we can name the one-hot encoded columns\n",
    "new_categorical_cols = preprocessor.named_transformers_['categorical']['one_hot_encoder'].get_feature_names_out(categorical_cols).tolist()\n",
    "#new_categorical_cols = preprocessor.named_transformers_['categorical']['one_hot_encoder'].get_feature_names(categorical_cols).tolist()\n",
    "# Create list of column names - numeric columns don't change\n",
    "column_names = numeric_cols + new_categorical_cols\n",
    "# Create dataframes\n",
    "X_train = pd.DataFrame(data=X_train, columns=column_names)\n",
    "X_valid = pd.DataFrame(data=X_valid, columns=column_names)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! We have our finalized dataset and we can move on to building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling Proccess\n",
    "\n",
    "Now the fun begins!\n",
    "\n",
    "But first: we need to calculate a **baseline accuracy**. This is the fraction of the data that is of the most common class. With a binary target, we can calculate this by taking the mean of the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What does this baseline accuracy mean in the context of the data? What does it mean in the context of trying to predict heart disease outcomes?\n",
    "\n",
    "Accuracy is not the only thing we need to be worried about when building machine learning models. A model can be accurate, and still have issues in what samples it classifies correctly, and what samples it makes mistakes on.\n",
    "\n",
    "**Question**: For example, consider false positives and false negatives. What do each mean in the context of classifying heat disease? If such a model were deployed in real life, which of the two - false positives or false negatives - would be more costly? Which should we be more concerned about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model: Logistic Regression\n",
    "\n",
    "The first model we'll try is called logistic regression, which we've used already. As a reminder, logistic regression is a linear model that can be used to predict the probability of a sample falling in a certain class. Thus, it's a common model for classification.\n",
    "\n",
    "We're going to use the `cross_val_score` function to calculate model performance across folds in the training data. The way we'll cross-validate is via the `StatifiedKFold` cross-validator. You can read the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html). \n",
    "\n",
    "Compare this to the [documentation](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.KFold.html) for `KFold`. What does `StratifiedKFold` add in?  Why is stratifying cross-validation important, particularly in this context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of performance metric functions\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "# Choose number of folds\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-validator\n",
    "skfold = StratifiedKFold(n_folds)\n",
    "# Create model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Iterate over metrics\n",
    "for metric in metrics:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=skfold, scoring=metric)\n",
    "    print(f\"Mean {metric} score is {cv_results.mean().round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "How does the mean accuracy compare to the baseline accuracy? What does this tell you about the importance of establishing baseline accuracy? Is establishing baseline accuracy likely to be more or less important with more imbalanced target data?\n",
    "\n",
    "What does this precision score ($TP / (TP+FP)$) mean in this context?\n",
    "\n",
    "What does this recall score ($TP / (TP+FN)$) mean in this context?\n",
    "\n",
    "`f1` is the harmonic mean of precision and recall, integrating them both into a single metric.\n",
    "\n",
    "What do you conclude from these performance scores? Note that so far we are only analyzing performance within the training data set, without considering generalization to the test sample yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 1: Ridge Regression\n",
    "\n",
    "Re-run the above analysis, but use ridge regression instead. In particular, use `RidgeCV` so that you can choose the best regularization penalty. You can adapt the code from notebook 6b to do this.\n",
    "\n",
    "How does this model perform relative to logistic regression? Relative to baseline accuracy?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model: Decision Trees\n",
    "\n",
    "Next, let's try using a decision tree. You may recall that decision trees have a wide array of *hyperparameters*, or settings in the model we set before fitting it to the data. These can include the maximum depth, the criterion used for performing a split, etc. When we first fit the decision tree, we used default parameters for all of these specified by `scitkit-learn`, only varying `max_depth`. How can we go about *choosing* the best values instead?\n",
    "\n",
    "In the case of ridge regression, we did a cross-validation procedure to choose the best hyperparameter. When we have many hyperparameters though, we'll need to do cross-validation across all combinations. The approach for this is called **grid search**, which we can use to search across all combinations of hyperparameters to find the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for Model Selection\n",
    "\n",
    "Grid search is a brute-force method that executes cross-validation for *all* possible combinations of hyperparameters from a set of hyperparameter ranges.\n",
    "\n",
    "Let's consider an example. Suppose we have two hyperparameters $A$ and $B$. We don't know what values to choose for them, so we'll use a grid search to identify the best set. Grid search requires we specify hyperparameter ranges. So, let's say hyperparameter $A$ can be either of the two values $(0, 1)$, and hyperparameter $B$ can be either of the two values $(2, 3)$ (in practice, we might choose more values, but we'll use two each for simplicity). \n",
    "\n",
    "Grid search forms each combination of hyperparameters, and fits a model for it. We can then use the valiation performance to choose the best combination across all hyperparameters. In this case, we'd consider all the following combinations:\n",
    "\n",
    "- $A = 0$, $B = 2$\n",
    "- $A = 0$, $B = 3$\n",
    "- $A = 1$, $B = 2$\n",
    "- $A = 1$, $B = 3$\n",
    "\n",
    "and choose the combination that performs the best.\n",
    "\n",
    "We can easily perform this process by using `scikit-learn`'s `GridSearchCV`. Let's take a look at how it works by running it on the `max_depth` and `min_samples_leaf` hyperparameters in a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we specify a parameter grid as a dictionary\n",
    "param_grid = {\n",
    "    \"max_depth\": np.arange(2, 20, 2), # why are we starting at 5?\n",
    "    \"min_samples_leaf\": np.arange(20, 200, 10) # avoid over-splitting/over-fitting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the size of the parameter grid we're tuning on?\n",
    "param_grid[\"max_depth\"].shape[0] * param_grid[\"min_samples_leaf\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's 162 different sets of parameters. That's a lot of models to fit!\n",
    "\n",
    "Next, we pass some information into the `GridSearchCV` object (check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dt = GridSearchCV(\n",
    "    # Specify the model\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    # Specify the hyperparameter grid\n",
    "    param_grid=param_grid,\n",
    "    # What metric should we use to select for the best model?\n",
    "    scoring = \"accuracy\",\n",
    "    # How do we generate cross-validation folds?\n",
    "    cv=skfold) # stratified cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can fit the grid search. Let's use the `%%time` magic command to see how long this process takes (around 1 minute for me). \n",
    "\n",
    "Note that `%%time` is a magic command to measure the time of execution for a whole cell, whereas `%time` only measures the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit the grid search object on the training data\n",
    "grid_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a fitted grid search variable! Let's take a look at what we get with it. First, the best score (on accuracy, as we specified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not as good as logistic regression, but a little better than baseline accuracy.\n",
    "\n",
    "We can also get the best cross-validated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Are you surprised at the optimal `max_depth`? What does it imply?\n",
    "\n",
    "The grid search variable is its own predictor, and we can run it on any set of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the results of the best decision tree estimator for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt = grid_dt.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training: \", best_dt.score(X_train, y_train))\n",
    "print(\"Test: \", best_dt.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 2: Choosing a Different Scoring Metric\n",
    "\n",
    "Run a new grid search, this time searching over max depth from 2 to 10 by 1 and min samples leaf from 75 to 125 by 5, to see if the optimal parameter is one that was not included in the previous grid search. Did performance improve?\n",
    "\n",
    "Now run a new grid search using recall as the choice of the scoring metric. What are the best parameters in this case? Are they different from before?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Model: Random Forests\n",
    "\n",
    "So far, our modeling hasn't yield great results. Let's consider a different model, which is more commonly used in harder prediction problems.\n",
    "\n",
    "This model is called the Random Forest. As you might expect from the name, a random forest is a collection of many decision trees. Specifically, it's an **ensemble model**, since it consists of an ensemble of $N$ decision trees. The $N$ trees in the forest can separately make predictions, each of which counts as a vote toward the final prediction. The ensemble prediction - typically by majority voting - performs better than a single tree alone. This is the machine learning version of a model \"greater than the sum of its parts\".\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*i0o8mjFfCn-uD79-F1Cqkw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few important things to note about the random forest:\n",
    "\n",
    "- Each tree in the forest is not trained on the same data and features. That would be counterproductive, because you would up end up with dozens of duplicate trees.\n",
    "- Instead, the trees are trained on a subset (usually a random 2/3) of the features and a bootstrapped (sampled with replacement) sample of the data. This helps reduce the variance of the predictions. *Bagging* (Bootstrap aggregating) estimates $N$ trees on bootstrapped samples with the same features. Random forests have a second parameter that controls how many features to try when finding the best split. Common rules of thumb for determining this parameter when there are $p$ features are $\\sqrt{p}$ and $p/3$.\n",
    "- To further decorrelate the trees, pruning trees to prevent overfitting is discouraged - we want each tree to fit its sample well.\n",
    "\n",
    "![](https://miro.medium.com/max/1240/1*EemYMyOADnT0lJWSXmTDdg.jpeg)\n",
    "\n",
    "We are going to gloss over some of the details of the random forest in order to focus on their application in this context. However, those details are important! Check out this [blog post](https://victorzhou.com/blog/intro-to-random-forests/) for a gentle introduction to random forests. For a *very* in-depth explanation of random forests, check out Chapter 15 of [Elements of Statistical Learning Theory](https://hastie.su.domains/Papers/ESLII.pdf).\n",
    "\n",
    "Let's get a sense for how a random forest performs without any hyperparameter tuning. We'll use the `RandomForestClassifier` from `scikit-learn`. Read the [documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html). The `n_estimators` argument is where we specify the number of trees. We will rely on the default `max_features` value of $\\sqrt{p}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1] ** (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest\n",
    "rf = RandomForestClassifier(n_estimators=50)\n",
    "# Cross-validate\n",
    "#cv_results = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "#cv_results.mean()\n",
    "\n",
    "# Iterate over same metrics as before\n",
    "for metric in metrics:\n",
    "    cv_results = cross_val_score(rf, X_train, y_train, cv=skfold, scoring=metric)\n",
    "    print(f\"Mean {metric} score is {cv_results.mean().round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is worse accuracy than the logistic regression - we're still not getting much improvement above baseline performance. \n",
    "\n",
    "Let's bring in the grid search on the `n_estimators` and `min_samples_split` hyperparameters to see if we can improve on this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 150], # previously used 50\n",
    "    \"min_samples_split\": [2, 5, 10], # default is 2\n",
    "    \"max_depth\": [5, 10, 15] # default is none\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    cv=skfold,\n",
    "    scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this grid search may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_rf.best_score_)\n",
    "print(grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = grid_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training: \", best_rf.score(X_train, y_train))\n",
    "print(\"Test: \", best_rf.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did slightly better than the baseline accuracy, but not much - this is a hard problem! We'll now move to more evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Challenge 3: Random Search\n",
    "\n",
    "The downside of grid search is that it can take a long time, especially when you have a large number hyperparameters, a complex model, and a lot of data. Grid search quickly becomes unwieldy because the search space is multidimensional.\n",
    "\n",
    "RandomSearchCV is a potential solution to this issue. In a random search, we randomly select a fraction of the hyperparameters sets to evaluate model performance. In a random search, you're not guaranteed to find the best set of parameters. However, it oftens performs pretty well, especially when there are computational constraints.\n",
    "\n",
    "You can do a random search in `scikit-learn` with `RandomSearchCV`. Choose a set of hyperparameters (maybe for this random forests model), and run a random search with `RandomSearchCV`. How does the best set of parameters compare to what is identified by a grid search?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluation and Interpretation\n",
    "\n",
    "It's time to bring back our validation dataset to evaluate how well our models perform on out-of-sample data.\n",
    "\n",
    "We're going to use the logistic regression, decision tree, and random forest models we created to make predictions on the validation features and evaluate them on a variety of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit logisic regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "lr_pred = lr.predict(X_valid)\n",
    "dt_pred = best_dt.predict(X_valid)\n",
    "rf_pred = best_rf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `classification_report` and `confusion_matrix` functions to evaluate the predictions.\n",
    "\n",
    "Remember that in a confusion matrix, the columns indicate classes predicted by the model (starting from 0 on the left), while the rows indicate actual classes (starting from 0 on the top). A perfect model would therefore have only diagonal entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression\\n')\n",
    "print(confusion_matrix(y_valid, lr_pred))\n",
    "print(classification_report(y_valid, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree\\n')\n",
    "print(confusion_matrix(y_valid, dt_pred))\n",
    "print(classification_report(y_valid, dt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest\\n')\n",
    "print(confusion_matrix(y_valid, rf_pred))\n",
    "print(classification_report(y_valid, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do all the models compare to each other? Which model do you think performed best, and for what reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The model performance we obtained was OK, but not amazing. In particular, the models struggle to identify true cases of heart disease. This often happens in the development of machine learning algorithms. \n",
    "\n",
    "It's useful at this point to try and interpret our models to see where they're getting signal from in order to decide what to do next. Do we need more data? Do we need better features? Do we need a better model?\n",
    "\n",
    "First, let's take a look at the logistic regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = lr.coef_[0]\n",
    "coefs = pd.Series(index=column_names, data = coefs)\n",
    "coefs.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features, according to the model, are the most and least associated with heart disease? How should you interpret categorical coefficients versus numerical coefficients? What do the sign of the coefficients mean?\n",
    "\n",
    "With tree-based models, feature importance works a little differently. We can access a `feature_importannces_` attribute which captures the \"importance\", defined as \"The (normalized) total reduction of the criterion brought by that feature.\" Basically, a quantification of how much the criterion we used (in our case the Gini impurity) was impacted by the feature's decision point. Notice that these feature importances are not signed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_fi = best_dt.feature_importances_\n",
    "dt_fi = pd.Series(index=column_names, data=dt_fi)\n",
    "dt_fi.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fi = best_rf.feature_importances_\n",
    "rf_fi = pd.Series(index=column_names, data=rf_fi)\n",
    "rf_fi.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the two sets of feature importances differ greatly? What do they tell us about predicting heart disease?\n",
    "\n",
    "Some of the feature importances are pretty low. This could imply that we should cut them out of the model. This may improve generalization performance, since the model is not trying to incorporate those less predictive features during training. This choice falls in the domain of *feature selection*. So, in future work, one thing we could do is retrain models without these features. We could also use regularization to implicitly do feature selection (e.g., a Lasso regression).\n",
    "\n",
    "Do the sets of feature importance suggest anything about potential additional feature engineering?\n",
    "\n",
    "Try to think about steps you might take to improve your models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Walkthrough Recap\n",
    "\n",
    "In this exercise we attempted to predict the onset of heart disease. We did the following:\n",
    "\n",
    "- We familiarized ourselves with the data and its patterns by studying the data dictionary and conducting exploratory data analyses.\n",
    "- We applied a number of feature engineering techniques to the data to prepare for modeling.\n",
    "- We employed three different machine learning models, two of which we parameter tuned in order maximize the generalization performance.\n",
    "- We evaluated our models on a validation dataset to get a sense of how well they did on an out-of-sample dataset.\n",
    "- We analyzed the relationship between the features and target variable by using attributes provided by the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Bonus - XGBoost \n",
    "\n",
    "eXtreme Gradient Boosting, or XGBoost, is currently (as of early 2026) the gold standard to supervised machine learning with structured or tabular data. It is incredibly fast and highly accurate relative to other classification models.\n",
    "\n",
    "XGBoost is part of the evolution of decision tree models:\n",
    "1. Decision Tree: A single \"flowchart\" making decisions. High variance (overfits easily).\n",
    "2. Random Forest (Bagging): Builds many trees independently and averages them.\n",
    "3. Gradient Boosting (Boosting): Builds trees sequentially. Each new tree tries to fix the specific errors (residuals) made by the previous trees.\n",
    "4. XGBoost: A highly optimized version of Gradient Boosting that adds clever math to prevent overfitting and specialized engineering to run extremely fast.\n",
    "\n",
    "What makes XGBoost \"eXtreme\"? The main advantage is **regularization**. Standard boosting can be \"too aggressive\" and memorize noise. XGBoost includes L1 and L2 regularization (as we saw with Lasso and Ridge models) directly in its objective function. This penalizes complex trees, forcing the model to stay simple and generalize better.\n",
    "\n",
    "XGBoost also has some additional nice features. First, it has a Sparsity-Aware Split Finding algorithm to deal with missing values. If a value is missing, the model \"tries\" sending it to the left branch and the right branch, then learns which direction works best for that specific feature. This replaces manual imputation, though that can still be useful as well. Second, it is very fast. XGBoost is designed to utilize all the cores of a student's CPU simultaneously (parallel processing). It also uses \"Cache Awareness,\" meaning it organizes data in the computer's memory so the processor can grab it instantly without waiting.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "XGBoost has dozens of parameters you can set and tune. For this introduction, we'll focus just on a few \"big ones\":\n",
    "* `n_estimators`: Number of trees to build. Too many = overfitting; too few = underfitting.\n",
    "* `learning_rate` (eta): How much we \"listen\" to each new tree. Usually between 0.01 and 0.3. Smaller values require more n_estimators.\n",
    "* `max_depth`: How deep each individual tree can go. Standard is 36. Deep trees capture complex patterns but overfit quickly.\n",
    "* `subsample`: Percentage of rows used to train each tree. Helps prevent the model from focusing too much on specific outliers.\n",
    "\n",
    "### Estimating an XGBoost model\n",
    "\n",
    "We will use the `xgboost` package. Fortunately, the syntax is very similar to `scikit-learn`. We'll estimate a basic model here with some specific selected parameters. Click on \"Parameters\" after estimating to see what else we might have specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Determining the ratio for scale_pos_weight\n",
    "ratio = len(y_valid[y_valid==0]) / y_valid.sum()\n",
    "\n",
    "# Initialize with the \"Safe\" defaults\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=212,\n",
    ")\n",
    "\n",
    "# It fits just like the DecisionTree did!\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the other decision-tree based models, we can look at feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_fi = xgb_model.feature_importances_\n",
    "xgb_fi = pd.Series(index=column_names, data=xgb_fi)\n",
    "xgb_fi.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the model using the same syntax as previously. How does the model with the selected parameters compare to those we used previously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and Evaluate\n",
    "xgb_pred = xgb_model.predict(X_valid)\n",
    "print('XGB\\n')\n",
    "print(confusion_matrix(y_valid, xgb_pred))\n",
    "print(classification_report(y_valid, xgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important parameter, particularly for our case with a rare outcome, is `scale_pos_weight`. This parameter increases the penalty for making a mistake on rare outcomes. It effectively dorces the model to pay more attention to the minority class, which is useful for imbalanced data as in our current exercise. \n",
    "\n",
    "A common rule of thumb is to set the weight as the ration of the majority class to the minority class. Using this parameter rather than over/undersampling is efficient because it uses the loss function.\n",
    "\n",
    "Let's see how this parameter affects the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the ratio for scale_pos_weight\n",
    "ratio = len(y_valid[y_valid==0]) / y_valid.sum()\n",
    "\n",
    "# Set and fit the model\n",
    "xgb_model2 = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=212,\n",
    "    scale_pos_weight=ratio\n",
    ")\n",
    "xgb_model2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "xgb_pred2 = xgb_model2.predict(X_valid)\n",
    "print('XGB with scale_pos_weight\\n')\n",
    "print(confusion_matrix(y_valid, xgb_pred2))\n",
    "print(classification_report(y_valid, xgb_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How do the results compare? What are the implications?\n",
    "\n",
    "When we increase `scale_pos_weight`, we are telling the model: \"It is better to accidentally predict a 'False Positive' than to miss a 'True Positive'.\" As a result, Recall (Sensitivity) will go up, but Precision will go down.\n",
    "\n",
    "Note: if we have more than two classes, we can use the `sample_weight` parameter within the `.fit()` method instead, which assigns a specific weight to each row in the dataset. \n",
    "\n",
    "How can we improve the models? **Hyperparameter tuning** is where XGBoost truly shines, given its efficiency and speed. \n",
    "\n",
    "As before we can use a grid search and cross-validation to do the tuning. Note that we can combine `scikit-learn` and `xgboost` functions fairly easily.\n",
    "\n",
    "Below is an example.\n",
    "\n",
    "Note: `logloss` is the default **evaluation metric** for binary classification. It compares the actual label to the predicted probability to determine the \"loss\", and punishes \"confident but wrong\" predictions more severly than \"unsure\" predictions. This encourages more accurate probabilities. \n",
    "\n",
    "Alternative metrics for binary classification include `error` (number of wrong predictions/total), `auc` (how well the model separates two classes across all possible thresholds), and `aucpr` (similar to auc but incorporates recall, useful when the positive class is very rare). For multiclass classification there are `mlogloss` and `merror` multiclass versions of the binary metrics. For regression, `rmse` and `mae` are common, as we used in our linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Initialize the model\n",
    "xgb = XGBClassifier(random_state=212, eval_metric='logloss')\n",
    "\n",
    "# 2. Define the \"Grid\" of parameters to test\n",
    "# Tip: Start small! Even this grid creates 3 * 3 * 2 = 18 combinations.\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200],\n",
    "    'scale_pos_weight': [ratio/2, ratio, ratio*2]\n",
    "}\n",
    "\n",
    "# 3. Set up the Search\n",
    "# cv=3 means it will run 3-fold cross-validation for every combination\n",
    "grid_search = GridSearchCV(estimator=xgb, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=3, \n",
    "                           scoring='accuracy', \n",
    "                           verbose=1)\n",
    "\n",
    "# 4. Run the search on your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. See the results\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# 6. Use the best model to predict on test data and evaluate\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_valid)\n",
    "print(confusion_matrix(y_valid, predictions))\n",
    "print(classification_report(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was pretty fast! \n",
    "\n",
    "This is better than the original model, but not yet as good as the more basic models we estimated. To improve, we may need to run a broader hyperparameter search and to think hard about what metrics we want to prioritize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving beyond the basics\n",
    "\n",
    "Once you understand the basic \"Fit and Predict\" process, here are the four areas you could look into to become more proficient with XGBoost:\n",
    "\n",
    "1. Understand the **Bias-Variance Tradeoff**\n",
    "Tuning in XGBoost is a balancing act:\n",
    "* To reduce Overfitting (High Variance): Lower max_depth, increase gamma (minimum loss reduction to split), or decrease learning_rate.\n",
    "* To reduce Underfitting (High Bias): Increase max_depth or increase n_estimators.\n",
    "Grid search hyperparameter tuning can help with setting values, but it is important to have a strong conceptual understanding.\n",
    "\n",
    "2. **Early Stopping**\n",
    "Instead of picking a random number for n_estimators, you can tell XGBoost: \"Keep building trees until the validation score stops improving for 10 rounds, then stop.\" This saves time and prevents overfitting perfectly.\n",
    "\n",
    "3. **Feature Engineering** over Tuning\n",
    "Better data beats a better model. Spending two hours tuning hyperparameters usually yields a 1% improvement. Spending two hours creating a new, clever feature (like interactions between dummy variables, squared continuous variables, etc.) can yield a 510% improvement. This point holds for all supervised ML, not just XGBoost.\n",
    "\n",
    "4. XGBoost for **Regression**\n",
    "XGBoost can also be used to predict continuous numbers by simply using XGBRegressor instead of XGBClassifier. The logic and hyperparameters remain nearly identical. It can produce efficiency gains relative to alternatives such as Lasso or Ridge Regression.\n",
    "\n",
    "In addition, here are some basic **troubleshooting notes:**\n",
    "* Model is too slow: Decrease n_estimators or increase learning_rate.\n",
    "* Model is memorizing the training data: Decrease max_depth or use subsample (e.g., 0.8).\n",
    "* Classes are imbalanced (e.g., rare disease): Use scale_pos_weight to give the rare class more \"weight.\"\n",
    "* The computer is getting hot/loud: Set n_jobs=-1 to ensure it's using all CPU cores efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
