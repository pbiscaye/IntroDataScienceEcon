{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd5859b4",
   "metadata": {},
   "source": [
    "# Section 6. Machine Learning Basics Practice\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n",
    "\n",
    "The purpose of this notebook is to give you opportunities and challenge to practice applying the skills developed in the other notebooks. \n",
    "\n",
    "Most of this notebook is derived from UC Berkeley D-Lab's Python Machine Learning [course](https://github.com/dlab-berkeley/Python-Machine-Learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c534a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial packages import, modify as needed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d34016",
   "metadata": {},
   "source": [
    "### Challenge 1: More exploratory data analysis\n",
    "\n",
    "Load the `auto-mpg.csv` data. Using this dataset, create the following plots, or examine the following distributions, to better understand your data:\n",
    "\n",
    "1. A histogram of the displacement.\n",
    "2. A histogram of the horsepower.\n",
    "3. A histogram of the weight.\n",
    "4. A histogram of the acceleration.\n",
    "5. What are the unique model years, and their counts?\n",
    "6. What are the unique origin values, and their counts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1aff0b",
   "metadata": {},
   "source": [
    "### Challenge 2: Mean Absolute Error\n",
    "\n",
    "Another commonly used metric in regression is the **Mean Absolute Error (MAE)**. As the name suggests, this can be calculated by taking the mean of the absolute errors. \n",
    "\n",
    "Follow the steps in the Section 6a notebook to model `mpg` using an OLS regression with the training data variables. Using this trained model, calculate the mean absolute error on the training and test data. We've imported the MAE for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c77aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "# Your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955694e",
   "metadata": {},
   "source": [
    "### Challenge 3: Feature Engineering\n",
    "\n",
    "You might notice that the `origin` variable in the auto-mpg dataset has only three values. So, it's really a categorical variable, where each sample has one of three origins. But so far we've treated it like a continuous variable. \n",
    "\n",
    "How can we properly treat this variable as categorical? This is a question of preprocessing and **feature engineering**.\n",
    "\n",
    "What we can do is replace the `origin` feature with two binary variables. The first tells us whether origin is equal to 2. The second tells us whether origin is equal to 3. If both are false, that means origin is equal to 1.\n",
    "\n",
    "By fitting a linear regression with these two binary features rather than treating `origin` as continuous, we can get a better sense for how the origin impacts the MPG.\n",
    "\n",
    "Create two new binary features corresponding to origin, and then recreate the training and test data. Then, fit a linear model to the new data. What do you find about the performance? Compare the $R^2$ in this model to the original model which treated `origin` as continuous. How did the coefficients change?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b0254",
   "metadata": {},
   "source": [
    "See if you can create some features to improve the training fit without sacrificing the ability to generalize to a good test fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ae3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae387971",
   "metadata": {},
   "source": [
    "### Challenge 4: Nearest Neighbors\n",
    "\n",
    "Modify the nearest neighbors tuning function from class to save the Test RMSE for each value of $K$. Run the function on all integers from 1 to 20. Store the results and plot them. Identify the minimum value and plot it in a different color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede552d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048ead6",
   "metadata": {},
   "source": [
    "### Challenge 5: Benchmarking\n",
    "\n",
    "Re-run the ordinary least squares prediction model on the data using `LinearRegression`. Then, create a new ridge regression where the `alpha` penalty is set equal to zero. How do the performances of these models compare to each other? How do they compare to a ridge regression model with `alpha` set to 5? Be sure to compare both the training performances and test performances. Which one performs better with the training data? Which one generalizes better to the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db69034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# Your code\n",
    "# Create models\n",
    "\n",
    "# Fit models\n",
    "\n",
    "# Run predictions\n",
    "\n",
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f654f03",
   "metadata": {},
   "source": [
    "### Challenge 6: Lasso\n",
    "\n",
    "Write a loop to perform a lasso regression over a large range of penalty hyperparameters. For each model, store the count of features with non-0 coefficients and the test $R^2$. Then plot how these values change as the penalty increases. Identify the hyperparameter value at which no features have non-0 coefficients. Identify the hyperparameter value with the best test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c2339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c539cd4",
   "metadata": {},
   "source": [
    "### Challenge 7: Order of Preprocessing\n",
    "\n",
    "In the preprocessing of penguin data we did the following steps: \n",
    "\n",
    "1) Null values for categorical data\n",
    "2) Dummy encoding\n",
    "3) Imputation for continuous data\n",
    "4) Normalization\n",
    "\n",
    "Now, consider that we change the order of the steps in the following ways. What effect might that have on the algorithms? Try changing the code from notebook 6c and trying it out!\n",
    "\n",
    "- Dummy Encoding before Null Values for categorical data\n",
    "- Normalization before Imputation for continuous data\n",
    "\n",
    "**Bonus:** Are there any other switches in order that might affect preprocessing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee8662-ff42-4d0e-800c-b2f9b0be98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de30a5",
   "metadata": {},
   "source": [
    "Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d701a26",
   "metadata": {},
   "source": [
    "### Challenge 8: Decision Tree Classification\n",
    "\n",
    "Follow the steps notebook 6d on classification for estimating a decision tree model to predict penguin species.\n",
    "\n",
    "First, estimate the model adding culmen_depth_mm as a feaure, and set max_depth=2.\n",
    "\n",
    "Then, estimate this same model setting max_depth=3. \n",
    "\n",
    "Then, estimate this same model adding flipper_length_mm as a feature, with max_depth=3.\n",
    "\n",
    "How does the training and test performance of the models change? What do you conclude about the effects of feature selection and depth choice?\n",
    "\n",
    "Visualize the original and the final new decision tree to see how adding features and layers changed the split decisions. Do you understand the decision protocols?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97b32a",
   "metadata": {},
   "source": [
    "### Challenge 9: Decision Tree Pruning\n",
    "\n",
    "We've looked at some approaches to pre-pruning. Now estimate a decision tree to estimate penguin species using the default hyperparameters, and use cost complexity post-pruning to prune the tree. You can look up specific code for how to do this online. \n",
    "\n",
    "After identifying a range of complexity penalty alphas, estimate the decision tree for each possible alpha value. Which alpha value has the best validation performance in terms of accuracy? How does the accuracy compare to what you were finding in Challenge 8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56116e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f4196-c2a5-4c91-9032-c2e8c2f3857e",
   "metadata": {},
   "source": [
    "### Challenge 10: Classification Model Comparisons\n",
    "\n",
    "Work through the challenges in the Section 6e notebook. As a further challenge, run Support Vector Machines and XGBoost using appropriate parameter tuning. Create a table summarizing and comparing the results across models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f56a2-8a33-40ac-a154-5502d1f77aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872cad1-b702-46b2-8fba-2345c7867c88",
   "metadata": {},
   "source": [
    "### Challenge 11: Cluster stability\n",
    "\n",
    "We discussed at the send of the Section 6f notebook that if clusters change a lot when randomly sampling subsets of the data, they are likely not detecting real patterns. Data scientists are not just interested in whether clusters can emerge, but whether those clusters are reliable.\n",
    "\n",
    "K-Means will always find clusters, even in a cloud of completely random dots. A professional data scientist must prove that their clusters are stableâ€”meaning they don't change drastically if a few data points are removed.\n",
    "\n",
    "You will test the stability of your $k=3$ K-Means model with the Spotify data and the same features we used in the 6f notebook, by comparing the results of the full dataset against a 90% random sample. \n",
    "\n",
    "Steps:\n",
    "1. Run K-Means on your full scaled dataset (X_scaled) with $k=3$ and random_state=1. Save these labels as labels_full.\n",
    "2. Create a Subsample: Create a new variable X_sub that contains a random 90% sample of your original data. Hint: Use X_scaled.sample(frac=0.9, random_state=123).\n",
    "3. Subsample Run: Run K-Means (same $k$ and same random_state) on X_sub. Save these labels as labels_sub.\n",
    "4. The Alignment Check: Select only the rows in labels_full that exist in your 90% sample. Use a Confusion Matrix to compare the \"Full\" labels vs the \"Subsample\" labels for those overlapping rows. Discuss the results. Hint: consider the possibility of \"label switching\". Calculate a \"stability score\" using the [Adjusted Rand Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html).\n",
    "5. Run this ten times with different random seeds and different subsamples, and calculate a mean stability score. Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae4dc3-0a68-4ec7-807b-c0d42933738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1f992-40b7-4281-8e75-a782a4e82f48",
   "metadata": {},
   "source": [
    "### Challenge 12: Principal components\n",
    "\n",
    "We were not sure how to interpret the \"generosity\" variable. Re-run the PCA analysis we did on the World Happiness data after removing the generosity feature. Discuss how the results differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0d176-29f4-4183-b647-f0e8300fc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec27d2c-1863-4a56-bd76-d4babd0e74f5",
   "metadata": {},
   "source": [
    "### Challenge 13: Predicting Diamond Cut\n",
    "\n",
    "Suppose we were interested in predicting the cut of a diamond, rather than its price. This is a categorical variable. Using logistic regression but still keeping just numeric variables in the features dataset, repeat the process in the Section 6g notebook to test the potential time benefits of using PCA for this kind of analysis.\n",
    "\n",
    "The below code could be helpful to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292ce08-5d05-4838-b4e1-0b382b757c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize k-fold with 5 splits\n",
    "# kf = KFold(n_splits=5)\n",
    "# #Intialize model\n",
    "# lr = LogisticRegression(random_state=1, max_iter = 300)\n",
    "# #Run cross validation\n",
    "# baseline_score = cross_val_score(lr, Xs, y, cv=kf, scoring = \"accuracy\").mean().round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a4788a-f2fc-444c-87d6-a45f23d7ab33",
   "metadata": {},
   "source": [
    "### Challenge 14: t-SNE perplexity\n",
    "\n",
    "Using the code in the Section 6g notebook, test how changing perplexity can change what the clusters look like. Make a figure with 6 subplots showing how the clusters change with increasing perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868feeff-0252-46d3-83db-1cedc263632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
