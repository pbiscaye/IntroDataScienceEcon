{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09135278",
   "metadata": {},
   "source": [
    "# Section 6. Satellite Remote Sensing\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n",
    "\n",
    "The content of this notebook draws on material from UC Berkeley's Spatial Data Analysis [course](https://docs.google.com/document/d/1oC10pjyeBQTenQazCpaB8Lx1b5PC1SR3WFiPgCtXqcs/edit?tab=t.0) notes by [Jaecheol Lee](https://sites.google.com/view/jaecheollee) and from a University of Bordeaux Machine Learning [course](https://github.com/jdnmiguel/Applied-ML) by [Jeremy do Nascimento Miguel](https://jdnmiguel.github.io/).\n",
    "\n",
    "In developing countries, we can lack of some information about weather events, agricultural production, urban settlements, which can explain policy responses and ease policy targeting. One way to overcome this data scarcity is to complement traditional data sources with remote sensing data. \n",
    "\n",
    "Data from remote sensing cover a wide range of topics: for instance land use, urban settlements, population density, agricultural production, weather, etc. \n",
    "\n",
    "If you want to know more about these data sources, I strongly recommend to read [Donaldson, Dave, and Adam Storeygard. \"The view from above: Applications of satellite data in economics.\" Journal of Economic Perspectives 30.4 (2016): 171-198.](https://pubs.aeaweb.org/doi/pdf/10.1257/jep.30.4.171)\n",
    "Another great paper is [Jain, Meha. \"The benefits and pitfalls of using satellite data for causal inference.\" Review of Environmental Economics and Policy (2020).](https://www.journals.uchicago.edu/doi/abs/10.1093/reep/rez023?journalCode=reep)\n",
    "But there are many others! We will talk about how to identify some of these data sources today.\n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Understand what satellite imagery looks like in raw form\n",
    "* Introduction to accessing remotely sensed data through Google Earth Engine\n",
    "* Get familiar with how to plot spatial data available online\n",
    "* Think about different ways of measuring the same thing with spatial data\n",
    "\n",
    "### Sections\n",
    "\n",
    "1. Satellite imagery\n",
    "2. Google's Earth Engine\n",
    "3. Accessing spatial data\n",
    "4. Comparing data sources: mapping floods in Nigeria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5107221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sys\n",
    "import rasterio\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0a901",
   "metadata": {},
   "source": [
    "# 1. Satellite imagery\n",
    "\n",
    "Many remotely sensed data sources are derived from raw **satellite data**, either imagery or data. Let's have a look at an example of pulling raw satellite imagery from the web directly in python.\n",
    "\n",
    "Note that there are python integrations to **pull satellite imagery from other sources**, such as **Google Earth Engine**. We will not cover that today but we will cover working in Google Earth Engine to access geospatial data.\n",
    "\n",
    "### Pulling Landsat images from the NASA API\n",
    "\n",
    "We will pull imagery from the Landsat satellite using NASA API that assists in the indexing of a Google Earth API, which provides the satellite imagery. \n",
    "\n",
    "The NASA API is described in the imagery section of this link: https://api.nasa.gov/api.html#earth. You need to provide the API with the location of the image and a date, and it will return the image taken by Landsat 8 that is closest to that date, and an estimate of how much the image is covered in clouds. Note that this API returns near-true-color satellite imagery with just 3 RGB bands, rather than the full Landsat stack of spectral bands. Different services (such as Google Earth Engine) can allow access to the full stack.\n",
    "\n",
    "The NASA site allows a computer to make 30 queries per hour using the generic api key DEMO_KEY. But if you make more than 30 requests, you will need to register for your own key by going here: https://api.nasa.gov/index.html#apply-for-an-api-key.\n",
    "\n",
    "**Querying an API** means providing a URL that contains information about the query; the URL server will be pinged and provide back the requested information. It is important that the query and the information returned must be in standard forms. In this exercise, the NASA API will return information stored in JSON and the Google Earth API will return a RGB 8-bit image. You must construct a URL defining your query. One way is construct the URL as a string, using the standard format described by NASA. You assign certain fields the values you stored as strings using an '=' and seperating fields using '&'. The URL is built by simply concatenating several strings.\n",
    "\n",
    "We'll start by looking at the area around the Eiffel Tower in Paris. Note that individual images are 0.025 degree squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0aed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build query\n",
    "lat=\"48.861\"\n",
    "lon=\"2.295\"\n",
    "date=\"2022-06-15\"\n",
    "my_api_key = \"DEMO_KEY\"\n",
    "base_url=\"https://api.nasa.gov/planetary/earth/imagery/\"\n",
    "image_query=base_url+\"?lat=\"+lat+\"&lon=\"+lon+\"&date=\"+date\n",
    "image_query=image_query+\"&api_key=\"+my_api_key\n",
    "print(image_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request data and parse reply using requests library\n",
    "import requests as re\n",
    "import json\n",
    "r = re.get(image_query)\n",
    "print(r.url)\n",
    "# Alternative\n",
    "#r = re.get(base_url, params={'lat':lat,'lon':lon,\n",
    "#                             'date':date,'api_key':my_api_key})\n",
    "#print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01794c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve satellite image\n",
    "def download_file(url,local_filename):\n",
    "    r = re.get(url, stream=True)\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "download_file(image_query,\"Data/paris.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2eb1d",
   "metadata": {},
   "source": [
    "We'll now use the `skimage` library to load the satellite image file. `scikit-image` is a collection of algorithms for image processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96429ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display satellite image\n",
    "im = io.imread('Data/paris.png')\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c0938",
   "metadata": {},
   "source": [
    "It's hard to tell what we are seeing, and notice that we can clearly see the cloud cover! That is an important limitation of satellite imagery.\n",
    "\n",
    "Let's try another date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa04a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=\"2022-07-15\"\n",
    "image_query=base_url+\"?lat=\"+lat+\"&lon=\"+lon+\"&date=\"+date\n",
    "image_query=image_query+\"&api_key=\"+my_api_key\n",
    "download_file(image_query,\"Data/paris2.png\")\n",
    "im2 = io.imread('Data/paris2.png')\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(im2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7dd5d",
   "metadata": {},
   "source": [
    "That's more clear, but it's a little hard to tell what we are looking at. \n",
    "\n",
    "We can **stitch together satellite images** to give us a broader picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981b6e9-3f74-4622-9e8a-749d02eb4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_n=str(48.861+0.025)\n",
    "image_query=base_url+\"?lat=\"+lat+\"&lon=\"+lon+\"&date=\"+date\n",
    "image_query=image_query+\"&api_key=\"+my_api_key\n",
    "download_file(image_query,\"Data/paris2_n.png\")\n",
    "im2_n = io.imread('Data/paris2_n.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b296757-d265-44ae-851e-6a8f194b06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate images\n",
    "big_im = np.vstack((im2_n,im2))\n",
    "\n",
    "# Plot together\n",
    "fig, ax = plt.subplots(ncols=1, figsize=(6, 6))\n",
    "ax.imshow(big_im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4e3ff-7c59-40f0-b995-8df9a1114a87",
   "metadata": {},
   "source": [
    "Now let's zoom out a bit using a built-in feature of the NASA API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace87fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zooming out\n",
    "# The dim argument tells the width and height in degrees of the square area desired, at same resolution; the default is 0.025\n",
    "image_query=base_url+\"?lat=\"+lat+\"&lon=\"+lon+\"&date=\"+date+\"&dim=0.1\"\n",
    "image_query=image_query+\"&api_key=\"+my_api_key\n",
    "download_file(image_query,\"Data/paris3.png\")\n",
    "im3 = io.imread('Data/paris3.png')\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(im3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af7f6c-549e-4f82-a58e-915ba75ff144",
   "metadata": {},
   "source": [
    "### Equalization and sharpening\n",
    "\n",
    "Let's try two approaches to make the images a bit sharper. \n",
    "\n",
    "1. **Histogram equalization** redistributes the intensities to use the full available range more evenly. This can help make details more visible, and works well with images that are too dark or too bright.\n",
    "2. **Unsharp masking** sharpens images by subtracting a blurred version from the original.\n",
    "\n",
    "Functions for both approaches are available from the `skimage` library.\n",
    "\n",
    "We'll start with the smaller image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe92e5-c976-4147-b535-c288261f6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "from skimage.filters import unsharp_mask\n",
    "\n",
    "# Global histogram equalization\n",
    "im2_eq = exposure.equalize_hist(im2)\n",
    "\n",
    "# Unsharp masking\n",
    "# radius controls the size of the blur\n",
    "# amount controls how much the unsharp mask effect is amplified\n",
    "im2_sharp = unsharp_mask(im2_eq, radius=1, amount=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92868d70-272d-4955-a63a-f08a2862d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images together\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(14, 7))\n",
    "\n",
    "axes[0].imshow(im2)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(im2_eq, cmap='gray')\n",
    "axes[1].set_title(\"Enhanced Image (Equalized)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(im2_sharp, cmap='gray')\n",
    "axes[2].set_title(\"Enhanced Image (Equalized + Sharpened)\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b0e31-123a-4e81-9e5b-77ac709f5a95",
   "metadata": {},
   "source": [
    "Now we can see things a bit more clearly! Certainly equalizing helped highlight the contrast across pixels in the original image. It's not clear how much we gained from sharpening but there are some differences. \n",
    "\n",
    "Here's what it looks like in [Google Maps](https://www.google.com/maps/place/Eiffel+Tower/@48.8586559,2.2848253,2483m/data=!3m1!1e3!4m6!3m5!1s0x47e66e2964e34e2d:0x8ddca9ee380ef7e0!8m2!3d48.8583701!4d2.2944813!16zL20vMDJqODE?entry=ttu&g_ep=EgoyMDI1MDExNS4wIKXMDSoASAFQAw%3D%3D).\n",
    "\n",
    "Let's see how it works with the zoomed out image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feba72c-e8d1-4ee2-b5ef-fb76369e4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global histogram equalization\n",
    "im3_eq = exposure.equalize_hist(im3)\n",
    "\n",
    "# Unsharp masking\n",
    "im3_sharp = unsharp_mask(im3_eq, radius=1, amount=1)\n",
    "\n",
    "# Plot images together\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 7))\n",
    "\n",
    "axes[0].imshow(im3)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(im3_sharp, cmap='gray')\n",
    "axes[1].set_title(\"Enhanced Image (Equalized + sharpened)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d60a2-b1ab-4727-b9e9-c40f0dfc1e51",
   "metadata": {},
   "source": [
    "### Exploring image bands\n",
    "\n",
    "Most imagery comes with multiple spectral bands. Analyzing them can give us specific information about the image and its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "im3_sharp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312de0d",
   "metadata": {},
   "source": [
    "We can see it is a square of pixels with 4 bands. The first three correspond to red, green, and blue.\n",
    "\n",
    "Let's plot the different color bands and see how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11376a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(12, 6))\n",
    "ax[0,0].imshow(im3[:, :, 0], cmap='binary')\n",
    "ax[0,1].imshow(im3[:, :, 1], cmap='binary')\n",
    "ax[0,2].imshow(im3[:, :, 2], cmap='binary')\n",
    "ax[1,0].imshow(im3_sharp[:, :, 0], cmap='binary')\n",
    "ax[1,1].imshow(im3_sharp[:, :, 1], cmap='binary')\n",
    "ax[1,2].imshow(im3_sharp[:, :, 2], cmap='binary')\n",
    "ax[0,0].set_title('Red band')\n",
    "ax[0,1].set_title('Green band')\n",
    "ax[0,2].set_title('Blue band')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b87e52-fb2f-4c8a-97bd-b04bad9f0480",
   "metadata": {},
   "source": [
    "It is not very easy to tell apart the images from the red, green, and blue bands when they are plotted separately in black and white in the raw image, but it is easier after equalizing and sharpening. The red band has some lower values than the blue band, while the green band is in the middle, but they all look similar. Water, as in the river, as greenery as in the Bois de Boulogne, stand more in the blue than in the red band. \n",
    "\n",
    "What does this imply about how much blue buildings reflect relative to water and vegetation?\n",
    "\n",
    "With this image we do not have additional image bands, but if we had the near infrared band we could use it to calculate NDWI, the normalized difference water index.\n",
    "$$ NDWI= (GREEN - NIR)/(GREEN + NIR) $$\n",
    "\n",
    "We could then plot that to see where water is identified.\n",
    "\n",
    "Let's simulate what plotting an index would look like, using $\\frac{Green- Red}{Green + Red}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508a553-9cba-453e-86cf-46c8e3c858e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the red and green bands\n",
    "red   = im3_sharp[..., 0].astype(float)  # Band 0\n",
    "green = im3_sharp[..., 1].astype(float)  # Band 1\n",
    "\n",
    "# Compute the index: (G - R) / (G + R)\n",
    "#    Add a small epsilon to avoid possible division by zero\n",
    "eps = 1e-7\n",
    "index = (green - red) / (green + red + eps)\n",
    "\n",
    "# Plot the resulting index\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Use a diverging colormap like 'RdYlGn', 'bwr', or 'seismic'\n",
    "plt.imshow(index, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "plt.colorbar(label='(Green - Red) / (Green + Red)')\n",
    "plt.title('(G - R) / (G + R)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab68eb-4d4f-4c52-b66a-5e6c99a1d61c",
   "metadata": {},
   "source": [
    "What do we observe from this image? What are we most identifying? Compare to the [Google Maps](https://www.google.com/maps/place/Eiffel+Tower/@48.8586559,2.2848253,2483m/data=!3m1!1e3!4m6!3m5!1s0x47e66e2964e34e2d:0x8ddca9ee380ef7e0!8m2!3d48.8583701!4d2.2944813!16zL20vMDJqODE?entry=ttu&g_ep=EgoyMDI1MDExNS4wIKXMDSoASAFQAw%3D%3D) image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735ac96-f125-4025-98aa-36ab182a648c",
   "metadata": {},
   "source": [
    "### Identifying features\n",
    "\n",
    "We've seen how we could use even just 3 bands of optical imagery to make some progress toward identifying water and vegetation.\n",
    "\n",
    "The applications are almost limitless, and often combine remotely sensed data and some measures of 'ground truth' using machine learning.\n",
    "* [Measuring economic growth](https://www.aeaweb.org/articles?id=10.1257/aer.102.2.994)\n",
    "* [Mapping poverty](https://cdn.vanderbilt.edu/vu-my/wp-content/uploads/sites/2095/2019/04/14134552/ScienceMachineLearningArticle.pdf)\n",
    "* [Estimating crop yield](https://www.sciencedirect.com/science/article/pii/S2352938522000015)\n",
    "* [Predicting air quality](https://www.sciencedirect.com/science/article/pii/S0034425723001608)\n",
    "* and so much more...\n",
    "\n",
    "We are limited in what we can do here with just 3 bands of imagery, but let's try a rough proxy of **mapping buildings**.\n",
    "\n",
    "[Several](https://onlinelibrary.wiley.com/doi/10.1155/2022/4831223) [papers](https://www.researchgate.net/publication/360414085_Detecting_Buildings_and_Nonbuildings_from_Satellite_Images_Using_U-Net) have worked on models incorporating machine learning and satellite data for building identification.\n",
    "\n",
    "The Normalized Difference Built-up Index (NDBI): $\\frac{SWIR-NIR}{SWIR+NIR}$. We cannot calculate this, but we could potentially create a proxy that relies on the fact that buildings are typically brighter in RGB imagery - more 'white' meaning more reflectance on all bands.\n",
    "\n",
    "Can we do a rough proxy ourselves?\n",
    "\n",
    "Let's try this with images of the University of California at Berkeley, where there is a nice combination of scattered buildings and vegetation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Berkeley_lat=\"37.87\"\n",
    "Berkeley_lon=\"-122.26\"\n",
    "date=\"2014-05-21\"\n",
    "my_api_key = \"DEMO_KEY\"\n",
    "base_url=\"https://api.nasa.gov/planetary/earth/imagery/\"\n",
    "image_query=base_url+\"?lat=\"+Berkeley_lat+\"&lon=\"+Berkeley_lon+\"&date=\"+date\n",
    "image_query=image_query+\"&api_key=\"+my_api_key\n",
    "download_file(image_query,\"Data/image.png\")\n",
    "berk = io.imread('Data/image.png')\n",
    "plt.imshow(berk)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce262e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(12, 6))\n",
    "ax[0].imshow(berk[:, :, 0], cmap='binary')\n",
    "ax[1].imshow(berk[:, :, 1], cmap='binary')\n",
    "ax[2].imshow(berk[:, :, 2], cmap='binary')\n",
    "ax[0].set_title('Red band')\n",
    "ax[1].set_title('Green band')\n",
    "ax[2].set_title('Blue band')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44ba66",
   "metadata": {},
   "source": [
    "Buildings appear to show up as more dark in the blue band, but generally as more bright overall.\n",
    "\n",
    "Let's see if we can use pixel brightness to proxy for buildings. We'll proxy 'bright' pixels as those with values above 50 on all color bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a846f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright = np.logical_and(berk[:,:,0]>50, \n",
    "                        berk[:,:,1]>50, \n",
    "                        berk[:,:,2]>50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15752206",
   "metadata": {},
   "source": [
    "Now, let's plot this! \n",
    "\n",
    "We can also use the bright pixels as a mask to show what the satellite imagery looks like only in the identified areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "berk_masked= berk*np.stack((bright,bright,bright), axis=2)\n",
    "#berk_masked[berk_masked==0]=255 #Change masked out pixels to white\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(12,18))\n",
    "ax[0].imshow(berk)\n",
    "ax[1].imshow(bright, cmap='coolwarm')\n",
    "ax[2].imshow(berk_masked)\n",
    "ax[0].set_title('Satellite image of \\n north-south area around \\n UC Berkeley campus')\n",
    "ax[1].set_title('Building locations predicted \\n based on pixel brightness')\n",
    "ax[2].set_title('Satellite image of only \\n predicted building locations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83c23e9",
   "metadata": {},
   "source": [
    "The mask appears to do a good job identifying buildings; this is clear in both the plot of the mask and the plot of the masked satellite image, though choosing to white out all other locations makes it a bit harder to distinguish some of the buildings that are very bright/close to white. \n",
    "\n",
    "Varying the thresholds on some of the bands might lead to a slightly more effective mask. But this is just a rough proxy and there are better ways to identify buildings from satellites.\n",
    "\n",
    "For example, you can check out this blog summarizing the Mapping Africa's Buildings [project](https://research.google/blog/mapping-africas-buildings-with-satellite-imagery/) to get a sense of how they proceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b463a",
   "metadata": {},
   "source": [
    "# 2. Using Google's Earth Engine\n",
    "\n",
    "An important tool for working with remotely sensed and satellite data is Google's **Earth Engine**.\n",
    "\n",
    "I have used Earth Engine to access and process weather and climate data (CHIRPS, ERA5, SPEI), agricultural/vegetation data (NDVI), land cover data (from Copernicus and others), flood risk data (JRC, WRI), and more.\n",
    "\n",
    "### What is Earth Engine?\n",
    "\n",
    "Earth Engine (EE) is a cloud/browser-based platform for planetary scale geospatial analysis that relies on Google's processing and storage capabilities to enable large analyses in very little time. \n",
    "It’s most relevant for people that are interested in using satellite and aerial imagery to study large areas, long time periods, or both. \n",
    "Earth Engine is home to hundreds of public remote sensing/geospatial datasets totaling more than thirty petabytes, and is continuously updated as images are captured.\n",
    "\n",
    "Check out the [Data Catalog](https://developers.google.com/earth-engine/datasets) for more information.\n",
    "\n",
    "### How do you work with Earth Engine?\n",
    "\n",
    "First, you can create a free account - there is no cost for using Earth Engine for academic purposes. Once you have an account, you can log in to EE and start writing scripts to work with spatial data.\n",
    "\n",
    "EE is an Application Programming Interface (API), meaning that we request data (raw or processed in some way) using a programming language. \n",
    "EE has both a JavaScript and Python API – I have primarily used the JavaScript API because the JavaScript Playground is more visually interactive and is easier to set up. The Python API requires some additional authentication and set up, but it can be very useful to have it integrated into your Python code.\n",
    "\n",
    "### Is it worth learning?\n",
    "\n",
    "**Absolutely**, if you plan to work with large-scale spatial data. \n",
    "\n",
    "The main **constraint** is learning how to write the code in JavaScript to access and manipulate the data. But LLMs can really help to overcome this.\n",
    "\n",
    "The huge **advantage** is access to massive computational power and speed for treating big spatial data. It facilitates analysis that would crash or take a very long time on most personal machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3018ed",
   "metadata": {},
   "source": [
    "### Seeing an example\n",
    "\n",
    "We won't go into detail on how to use EE today, but I will walk you through you example code so you can see it.\n",
    "\n",
    "Let's look at some of my Earth Engine [code](https://code.earthengine.google.com/?project=ee-pbiscaye).\n",
    "\n",
    "1. Browsing for datasets: floods\n",
    "2. Asking ChatGPT for help with EE JavaScript code\n",
    "3. Looking through and running code for mapping flood hazard\n",
    "4. Using the console to understand the data\n",
    "5. Interacting with the map and using the Inspector\n",
    "6. Running a task/exporting data\n",
    "7. Showing how I then used the data in Python\n",
    "8. Looking briefly at code for other EE datasets (SPEI prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b069f30",
   "metadata": {},
   "source": [
    "# 3. Accessing spatial data\n",
    "\n",
    "There are many places to access spatial data. Many datasets are just a quick Google search away!\n",
    "\n",
    "For example, here are the web addresses of some datasets I have used in my work that we will look at below:\n",
    "\n",
    "* Gridded population of the world: https://sedac.ciesin.columbia.edu/gpw-v2/index.html?main.html&2\n",
    "* Global administrative boundaries: https://gadm.org/download_country.html\n",
    "* Global agricultural lands: https://www.earthdata.nasa.gov/data/catalog/sedac-ciesin-sedac-aglands-pas2000-1.00\n",
    "* WorldClim monthly temperature and precipitation data: https://www.worldclim.org/data/monthlywth.html\n",
    "* Global land cover classifications: https://gaez.fao.org/\n",
    "\n",
    "All the datasets are at different spatial and temporal resolutions, so require some work to prepare to merge and work with. \n",
    "\n",
    "Let's have a look at some of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd756b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting my data path\n",
    "#path=\"/Users/pierrebiscaye/Dropbox/Data/\"\n",
    "path=\"C:/Users/pibiscay/Dropbox/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c4024",
   "metadata": {},
   "source": [
    "### Gridded population of the world from CIESIN\n",
    "\n",
    "We've already seen the GPW data. The GPWv4 rev11 population estimates are at a 15 arcminute (0.25 degree) level globally interpolated for every 5 years from 2000-2020. \n",
    "\n",
    "There are a lot of calculations under the hood already to estimate population. We won't get into those here but you should be aware that every data source you find has a variety of calculations under the hood that may affect the quality of the data.\n",
    "\n",
    "Let's have another look, considering 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bab494",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop20=rasterio.open(path+\"Spatial/GPW/2020/gpw_v4_population_count_rev11_2020_15_min.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31843f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [0, 0.33, 0.67, 1]  # positions for each color from 0-1\n",
    "color_scheme = ['bisque', 'orange', 'red', 'purple']  # corresponds to nodes\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\n",
    "    'WhiteYellowRed', list(zip(nodes, color_scheme)))\n",
    "custom_cmap.set_under('gray')  # set values under vmin to gray\n",
    "custom_cmap.set_over('blue')  # set values over vmax to black\n",
    "\n",
    "# plotting starts\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "im = ax.imshow(pop20.read(1),\n",
    "               cmap=custom_cmap,\n",
    "               extent=(-180, 180, -90, 90),\n",
    "               vmin=0, vmax=500000)\n",
    "fig.colorbar(im)\n",
    "# label axes and title\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Gridded Population of the World in 2020 (Source: CIESIN)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6e1be",
   "metadata": {},
   "source": [
    "### Monthly total precipitation from WorldClim\n",
    "\n",
    "Weather-related data are available from a variety of sources. The first source I used was WorldClim. \n",
    "\n",
    "WorldClim has data on monthly total precipitation and maximum temperature available at a 2.5 arcminute resolution (around 0.04 degrees) globally for every month from 1985-2018. The files are provided as TIFs.\n",
    "\n",
    "Let's look at precipitation in January 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28230c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rainpath=path+\"/Spatial/WorldClim/Precipitation/wc2.1_2.5m_prec_2010-2018/\"\n",
    "rain_01=rasterio.open(rainpath+\"wc2.1_2.5m_prec_2018-01.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [0, 0.5, 1]  # positions for each color from 0-1\n",
    "color_scheme = ['gray', 'lightblue', 'blue']  # corresponds to nodes\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\n",
    "    'WhiteYellowRed', list(zip(nodes, color_scheme)))\n",
    "custom_cmap.set_under('white')  # set values under vmin to gray\n",
    "custom_cmap.set_over('purple')  # set values over vmax to black\n",
    "\n",
    "# plotting starts\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "im = ax.imshow(rain_01.read(1),\n",
    "               cmap=custom_cmap,\n",
    "               extent=(-180, 180, -90, 90),\n",
    "               vmin=0, vmax=250)\n",
    "fig.colorbar(im)\n",
    "# label axes and title\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Total Precipitation in January 2018 (Source: WorldClim)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb019a2",
   "metadata": {},
   "source": [
    "### Land cover from CIESIN\n",
    "\n",
    "CIESIN has a global agricultural lands dataset that shows the share of land that is crop or pasture land in 2000, at the 0.0833 degree level. This dataset has no temporal variation, though there are several other land classification datasets with information on changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a076507",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropland=rasterio.open(path+\"Spatial/Land Cover/CIESIN 2000/gl-croplands-geotif/cropland.tif\")\n",
    "pasture=rasterio.open(path+\"Spatial/Land Cover/CIESIN 2000/gl-pastures-geotif/pasture.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [0, 0.5, 1]  # positions for each color from 0-1\n",
    "color_scheme = ['gray', 'gold', 'green']  # corresponds to nodes\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\n",
    "    'WhiteYellowRed', list(zip(nodes, color_scheme)))\n",
    "custom_cmap.set_under('white')  # set values under vmin to gray\n",
    "custom_cmap.set_over('darkgreen')  # set values over vmax to black\n",
    "\n",
    "# plotting starts\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "im = ax.imshow(cropland.read(1),\n",
    "               cmap=custom_cmap,\n",
    "               extent=(-180, 180, -90, 90),\n",
    "               vmin=0, vmax=1)\n",
    "fig.colorbar(im)\n",
    "# label axes and title\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Share of cropland in pixel, 2000 (Source: CIESIN)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9657d17",
   "metadata": {},
   "source": [
    "### Global Administrative Boundaries\n",
    "\n",
    "There are a variety of datasets on administrative boundaries around the world. The GADM dataset we've used before is probably the most useful for subnational boundaries. The below national country boundaries are for the whole world. Let's plot national boundaries in Africa over cropland shares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "world=gpd.read_file(path+\"/Country boundaries/Country raw/\" + \n",
    "                    \"UIA_World_Countries_Boundaries/World_Countries__Generalized_.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting starts\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "im = ax.imshow(cropland.read(1),\n",
    "               cmap=custom_cmap,\n",
    "               extent=(-180, 180, -90, 90),\n",
    "               vmin=0, vmax=1)\n",
    "fig.colorbar(im)\n",
    "world.plot(ax=ax,color='none', edgecolor='k', alpha=0.3)\n",
    "# label axes and title\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Share of cropland in pixel, Africa 2000 (Source: CIESIN)')\n",
    "ax.set_xlim([-25,60])\n",
    "ax.set_ylim([-40,40])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71c7dd",
   "metadata": {},
   "source": [
    "### GAEZ\n",
    "\n",
    "The Global Agro-Ecological Zones data portal has a huge amount of spatial data on land characteristics.\n",
    "\n",
    "Let's look at a map of the share of agricultural land that is irrigated in 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837201e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr=rasterio.open(path+\"/Spatial/GAEZ/LR/wat/faoirr00.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9feeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr.read(1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [0, 0.5, 1]  # positions for each color from 0-1\n",
    "color_scheme = ['lightblue', 'blue', 'darkblue']  # corresponds to nodes\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\n",
    "    'WhiteYellowRed', list(zip(nodes, color_scheme)))\n",
    "custom_cmap.set_under('white')  # set values under vmin to gray\n",
    "custom_cmap.set_over('darkblue')  # set values over vmax to black\n",
    "\n",
    "# plotting starts\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "im = ax.imshow(irr.read(1),\n",
    "               cmap=custom_cmap,\n",
    "               extent=(-180, 180, -90, 90),\n",
    "               vmin=1, vmax=50)\n",
    "fig.colorbar(im)\n",
    "world.plot(ax=ax,color='none', edgecolor='k', alpha=0.3)\n",
    "# label axes and title\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlim([-25,60])\n",
    "ax.set_ylim([-40,40])\n",
    "ax.set_title('Irrigated lands in Africa in 2000 (Source: GAEZ)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3d13b-7a98-4500-a16a-87087860170c",
   "metadata": {},
   "source": [
    "Now France! [for comparison](https://www.researchgate.net/figure/Global-map-of-irrigation-areas-irrigation-intensity-in-the-EU-as-area-equipped-for_fig7_46488610)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "im = ax.imshow(irr.read(1),\n",
    "               cmap=custom_cmap,\n",
    "               extent=(-180, 180, -90, 90),\n",
    "               vmin=1, vmax=50)\n",
    "fig.colorbar(im)\n",
    "world.plot(ax=ax,color='none', edgecolor='k', alpha=0.3)\n",
    "# label axes and title\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlim([-10,10])\n",
    "ax.set_ylim([40,55])\n",
    "ax.set_title('Irrigated lands in France in 2000 (Source: GAEZ)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5dcd8",
   "metadata": {},
   "source": [
    "# 4. Comparing Data Sources\n",
    "\n",
    "There are often many ways to measure the same thing, and many different data sources available.\n",
    "\n",
    "Josephson et al (2024)'s [paper](https://arxiv.org/abs/2409.07506) shows that the choice of data source can affect economic analyses. They combine remotely-sensed data on weather and measures of smallholder agricultural productivity in the LSMS-ISA and show that the results are not robust to the choice of weather data, with differences in both magnitude and sign!\n",
    "\n",
    "Many [other](https://www.aeaweb.org/articles?id=10.1257/pandp.20191064) [papers](https://www.journals.uchicago.edu/doi/full/10.1093/reep/rez023) have reported on issues for using remotely sensed data in economic analyses. \n",
    "\n",
    "Let's go through an ongoing project I am working on about mapping flood exposure.\n",
    "\n",
    "GO TO FLOOD MAPPING SLIDES."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
