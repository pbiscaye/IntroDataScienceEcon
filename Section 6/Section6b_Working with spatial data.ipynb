{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6. Working with spatial data\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n",
    "\n",
    "The content of this notebook draws on material from my own research. \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Think about what manipulations are needed to prepare raw spatial data for analysis\n",
    "* Understand some of the common steps, such as interpolation and rescaling\n",
    "* Be able to visualize point data together with other spatial data\n",
    "* Think about ways to match spatial data to observation units for analysis\n",
    "\n",
    "### Sections\n",
    "\n",
    "1. Prepping spatial data for analysis\n",
    "2. Point-level data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sys\n",
    "import rasterio\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting my data path\n",
    "#path=\"/Users/pierrebiscaye/Dropbox/Data/\"\n",
    "path=\"C:/Users/pibiscay/Dropbox/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepping data for analysis\n",
    "\n",
    "In notebook 6a we walked through how to access and map a variety of remotely sensed and other spatial data products. But there is much more work to do before the data you access can be used for analysis.\n",
    "\n",
    "We'll go through three types of data prep steps today:\n",
    "1. Interpolating spatial data\n",
    "2. Changing data resolution\n",
    "3. Creating derived variables\n",
    "\n",
    "We'll talk about these in the contex of code I used for my paper on the impacts of exposure to locust swarms on the risk of conflict.\n",
    "\n",
    "In this paper, the unit of observation for the analysis was 0.25$^{\\circ}$ cells at the annual level. This means that all my spatial data have to be prepared at this level. \n",
    "\n",
    "I first constructed a grid of 0.25$^{\\circ}$ cells defined by cell centroids. This is what we have done previously in this class when creating meshed grids. I then duplicated this grid for the number of years in my dataset. The next step was to prepare my individual datasets to merge into this grid to create a master dataset for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolating spatial data\n",
    "\n",
    "The GPW data are only available every 5 years. What to do if you want population data for the years in between?\n",
    "\n",
    "One possibility is to **interpolate**, meaning to estimate missing data based on other observations.\n",
    "\n",
    "For the case of the GPW data, the approach I took is **linear interpolation**, estimating population in years between observations as weighted averages of the two surrounding years with data. \n",
    "\n",
    "Below is the code I used to build a population array after loading the available data as above in Section 6a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop01=(4/5)*pop00.read(1) + (1/5)*pop05.read(1)\n",
    "# pop02=(3/5)*pop00.read(1) + (2/5)*pop05.read(1)\n",
    "# pop03=(2/5)*pop00.read(1) + (3/5)*pop05.read(1)\n",
    "# pop04=(1/5)*pop00.read(1) + (4/5)*pop05.read(1)\n",
    "# pop06=(4/5)*pop05.read(1) + (1/5)*pop10.read(1)\n",
    "# pop07=(3/5)*pop05.read(1) + (2/5)*pop10.read(1)\n",
    "# pop08=(2/5)*pop05.read(1) + (3/5)*pop10.read(1)\n",
    "# pop09=(1/5)*pop05.read(1) + (4/5)*pop10.read(1)\n",
    "# pop11=(4/5)*pop10.read(1) + (1/5)*pop15.read(1)\n",
    "# pop12=(3/5)*pop10.read(1) + (2/5)*pop15.read(1)\n",
    "# pop13=(2/5)*pop10.read(1) + (3/5)*pop15.read(1)\n",
    "# pop14=(1/5)*pop10.read(1) + (4/5)*pop15.read(1)\n",
    "# pop16=(4/5)*pop15.read(1) + (1/5)*pop20.read(1)\n",
    "# pop17=(3/5)*pop15.read(1) + (2/5)*pop20.read(1)\n",
    "# pop18=(2/5)*pop15.read(1) + (3/5)*pop20.read(1)\n",
    "# pop19=(1/5)*pop15.read(1) + (4/5)*pop20.read(1)\n",
    "# pop00=pop00.read(1)\n",
    "# pop05=pop05.read(1)\n",
    "# pop10=pop10.read(1)\n",
    "# pop15=pop15.read(1)\n",
    "# pop20=pop20.read(1)\n",
    "\n",
    "# pop=np.array([pop00,pop01,pop02,pop03,pop04,\n",
    "#              pop05,pop06,pop07,pop08,pop09,\n",
    "#              pop10,pop11,pop12,pop13,pop14,\n",
    "#              pop15,pop16,pop17,pop18,pop19,pop20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result (after some additional preparation) was a dataset at the cell-year level that was ready to merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing data resolution\n",
    "\n",
    "#### Spatial resolution\n",
    "\n",
    "When combining data sources it is necessary to match data resolution (as well as CRS!!).\n",
    "\n",
    "Most of the data I used for my locusts and conflict project were at a finer resolution than 0.25$^{\\circ}$ degrees. This means I had multiple pixels for each 0.25$^{\\circ}$ grid cell. The approach here is to **rescale** the original data to match the desired resolution. You could see that I did this in the GEE script we looked at.\n",
    "\n",
    "When the original data is at a higher resolution, you are **collapsing** it to the lower target resolution. You can take a mean, a sum, a max, or other statistics for higher resolution pixels to then generate statistics at the lower target resolution. What you use depends on what you are trying to measure.\n",
    "\n",
    "When the original data is at a lower resolution, you are **spreading** the data across the lower target resolution. One approach is to assign all smaller pixels within the larger pixel the value of the larger pixel. More sophisticated approachs require assumptions about how the values in the larger pixel were generated, and using that as an input to determine how to spread the data. You might not want to do this for something like temperature, but you might for something like GDP or agricultural output.\n",
    "\n",
    "#### Temporal resolution\n",
    "\n",
    "Merging data sources also requires matching the temporal resolution.\n",
    "\n",
    "How you approach this again depends on the relative resolutions of the original and target data sources. You can **collapse** data to larger temporal resolutions (i.e., months to years), or **spread** data across lower resolutions (i.e., years to days).\n",
    "\n",
    "#### Example: Preparing WorldClim weather data\n",
    "\n",
    "WorldClim has data on monthly total precipitation and maximum temperature available at a 2.5 arcminute resolution (around 0.04 degrees) globally for every month from 1985-2018. \n",
    "\n",
    "I needed to merge this 0.04$^{\\circ}$ degree by month data to a 0.25$^{\\circ}$ by year target dataset.\n",
    "\n",
    "**Temporal resolution:** I took the sum of precipitation across months to get an annual measure and took the max of maximum monthly temperature. \n",
    "\n",
    "*Question:* Why did I use different methods for the two measures?\n",
    "\n",
    "**Spatial resolution:** I assigned every original 0.04$^{\\circ}$ degree cell to the larger grid cell it fell in, and took means across smaller cells to get an average value for the larger cells. \n",
    "\n",
    "*Question:* When might taking a sum or a max have been appropriate?\n",
    "\n",
    "#### When to change resolution?\n",
    "\n",
    "High-resolution spatial datasets take up a lot of memory, so it may be best to change the resolution to the desired level before saving the resulting dataset. In this case, you can write the code to change resolution in whatever codebook you are using to load and treat the raw data (EE JavaScript, Python, ...).\n",
    "\n",
    "But saving higher-resolution datasets can be useful if you want to create flexibility for different types of merging later on. In this case, you can coarsen or collapse the data in the process of merging it in whatever your preferred econometric software is (Python, R, Stata, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating derived variables\n",
    "\n",
    "#### In raster data\n",
    "\n",
    "When working with rasters, you may want to create some derived variables in the original resolution as coarsening first and then creating variables in a combined dataset at the target resolution may lead to different values of derived variables.\n",
    "\n",
    "You should think carefully about the math of what exactly the derived variables will be measuring depending on the order of operations and decide how to proceed.\n",
    "\n",
    "*Question*: You have evapotranspiration and precipitation data at a monthly 25km resolution and want to calculate\n",
    "$$ SPEI = norm(Precipitation - Evapotranspiration) $$\n",
    "What would it imply to calculate this first at the monthly level and then derive an annual measure, as opposed to collapsing evapotranspiration and precipitation to the year level first and then trying to calculate SPEI?\n",
    "\n",
    "#### Merging point data to a grid\n",
    "\n",
    "In some cases you want to assign raster data to point locations, which we will discuss below. \n",
    "\n",
    "In other cases, you want to use information from point events to create a raster. This is what I did for the conflict and locust swarms data in my paper. \n",
    "\n",
    "I first identified which grid cell and year each point event was located in.\n",
    "\n",
    "I then assigned values to cell-years based on the distribution of point events. I calculated two statistics for each cell-year:\n",
    "* Count of events\n",
    "* Indicators for any events\n",
    "\n",
    "Finally, I created additional variables for each cell-year based on proximity to events outside of the cell, to consider potential spillovers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did this all look in the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean=\"C:/Users/pibiscay/Dropbox/Locusts/Clean data/\"\n",
    "world=gpd.read_file(path+\"Country boundaries/Country raw/\"+\n",
    "                    \"UIA_World_Countries_Boundaries/World_Countries__Generalized_.shp\")\n",
    "data = pd.read_csv(clean+\"mapping.csv\")\n",
    "x = data['lon']\n",
    "y = data['lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop20=rasterio.open(path+\"Spatial/GPW/2020/gpw_v4_population_count_rev11_2020_15_min.tif\")\n",
    "pop10=rasterio.open(path+\"Spatial/GPW/2010/gpw_v4_population_count_rev11_2010_15_min.tif\")\n",
    "data['population']=data['population']*10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [0, 0.33, 0.67, 1]  # positions for each color from 0-1\n",
    "color_scheme = ['bisque', 'orange', 'red', 'purple']  # corresponds to nodes\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\n",
    "    'WhiteYellowRed', list(zip(nodes, color_scheme)))\n",
    "custom_cmap.set_under('gray')  # set values under vmin to gray\n",
    "custom_cmap.set_over('blue')  # set values over vmax to black\n",
    "\n",
    "# Create a figure with three subplots\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 10))\n",
    "\n",
    "title_fontsize = 20  # Size of the subplot titles\n",
    "label_fontsize = 16  # Size of the colorbar labels\n",
    "tick_fontsize = 12   # Size of the colorbar tick labels\n",
    "\n",
    "im = ax[0].imshow(pop20.read(1),\n",
    "               cmap=custom_cmap,\n",
    "               extent=(-180, 180, -90, 90),\n",
    "               vmin=0, vmax=500000)\n",
    "ax[0].set_title('Population in 2020 (Source: CIESIN)', fontsize=title_fontsize)\n",
    "cbar=fig.colorbar(im, ax=ax[0], label=\"Population\")\n",
    "cbar.ax.tick_params(labelsize=tick_fontsize) \n",
    "cbar.set_label(\"Population\", fontsize=label_fontsize)\n",
    "\n",
    "im = ax[1].imshow(pop10.read(1),\n",
    "               cmap=custom_cmap,\n",
    "               extent=(-180, 180, -90, 90),\n",
    "               vmin=0, vmax=500000)\n",
    "ax[1].set_title('Population in 2010 (Source: CIESIN)', fontsize=title_fontsize)\n",
    "cbar=fig.colorbar(im, ax=ax[1], label=\"Population\")\n",
    "cbar.ax.tick_params(labelsize=tick_fontsize) \n",
    "cbar.set_label(\"Population\", fontsize=label_fontsize)\n",
    "\n",
    "scatter = ax[2].scatter(x, y, marker=\"s\", s=10, c=data['population'], cmap=custom_cmap,\n",
    "                       vmin=0, vmax=500000)\n",
    "ax[2].set_title('Population in 2018 (Interpolated)', fontsize=title_fontsize)\n",
    "cbar=fig.colorbar(scatter, ax=ax[2], label=\"Population\")\n",
    "cbar.ax.tick_params(labelsize=tick_fontsize) \n",
    "cbar.set_label(\"Population\", fontsize=label_fontsize)\n",
    "\n",
    "for i in range(3):\n",
    "    world.plot(ax=ax[i],color='none', edgecolor='k', alpha=0.3)\n",
    "    ax[i].set_xlim([30,50])\n",
    "    ax[i].set_ylim([5,20])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_18=rasterio.open(path+\"/Spatial/WorldClim/Max Temperature/wc2.1_2.5m_tmax_2010-2018/\"\n",
    "                      +\"wc2.1_2.5m_tmax_2018-07.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(10, 10))\n",
    "im = ax[0].imshow(temp_18.read(1),\n",
    "               cmap='Reds',\n",
    "               extent=(-180, 180, -90, 90),\n",
    "               vmin=15, vmax=45)\n",
    "ax[0].set_title('Max Temperature in July 2018 (Source: WorldClim)', fontsize=title_fontsize)\n",
    "cbar=fig.colorbar(im, ax=ax[0], label=\"Temperature\")\n",
    "cbar.ax.tick_params(labelsize=tick_fontsize) \n",
    "cbar.set_label(\"Temperature (C)\", fontsize=label_fontsize)\n",
    "\n",
    "scatter = ax[1].scatter(x, y, marker=\"s\", s=20, \n",
    "                        c=data['temp_max_july'], cmap='Reds', vmin=15, vmax=45)\n",
    "ax[1].set_title(\"Max Temperature in July 2018, rescaled\", fontsize=title_fontsize)\n",
    "cbar=fig.colorbar(scatter, ax=ax[1], label=\"Temperature\")\n",
    "cbar.ax.tick_params(labelsize=tick_fontsize) \n",
    "cbar.set_label(\"Temperature (C)\", fontsize=label_fontsize)\n",
    "\n",
    "for i in range(2):\n",
    "    world.plot(ax=ax[i],color='none', edgecolor='k', alpha=0.3)\n",
    "    ax[i].set_xlim([-15,5])\n",
    "    ax[i].set_ylim([10,25])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarms = pd.read_csv(path+\"/Locust Hub/Retrieved 9.13.20/Swarms_geo_clean.csv\")\n",
    "swarms=swarms[swarms['yr']>1989] # because that is the focus in my paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarms.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color schemes\n",
    "nodes = [0,  1] \n",
    "color_scheme3 = ['white', 'orange']  # corresponds to nodes\n",
    "custom_cmap3 = LinearSegmentedColormap.from_list(\n",
    "    'custom_name3', list(zip(nodes, color_scheme3)))\n",
    "custom_cmap3.set_under('white')  # set values under vmin to white\n",
    "custom_cmap3.set_over('orange')  # set values over vmax to blue\n",
    "\n",
    "# Create a figure with three subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 10))\n",
    "\n",
    "title_fontsize = 20  # Size of the subplot titles\n",
    "label_fontsize = 16  # Size of the colorbar labels\n",
    "tick_fontsize = 12   # Size of the colorbar tick labels\n",
    "\n",
    "\n",
    "scatter = axes[0].scatter(swarms['x'], swarms['y'], marker=\"s\", s=1, c=swarms['yr'], cmap='jet')\n",
    "axes[0].set_title(\"A. Swarm locations and year\", fontsize=title_fontsize)\n",
    "cbar=fig.colorbar(scatter, ax=axes[0], label=\"Year\")\n",
    "cbar.ax.tick_params(labelsize=tick_fontsize) \n",
    "cbar.set_label(\"Year\", fontsize=label_fontsize)\n",
    "\n",
    "scatter = axes[1].scatter(x, y, marker=\"s\", s=1, c=data['treat_yr2'], cmap='jet')\n",
    "axes[1].set_title(\"B. Year of first locust swarm exposure\", fontsize=title_fontsize)\n",
    "cbar=fig.colorbar(scatter, ax=axes[1], label=\"Year\")\n",
    "cbar.ax.tick_params(labelsize=tick_fontsize) \n",
    "cbar.set_label(\"Year\", fontsize=label_fontsize)\n",
    "\n",
    "scatter = axes[2].scatter(x, y, marker=\"s\", s=1, c=data['swarm100max'], cmap=custom_cmap3)\n",
    "axes[2].set_title(\"C. Any swarm within 100km\", fontsize=title_fontsize)\n",
    "cbar=fig.colorbar(scatter, ax=axes[2], ticks=[0, 1], label=\"0=No, 1=Yes\")\n",
    "cbar.ax.tick_params(labelsize=tick_fontsize) \n",
    "cbar.set_label(\"0=No, 1=Yes\", fontsize=label_fontsize)\n",
    "\n",
    "for i in range(3):\n",
    "    world.plot(ax=axes[i],color='none', edgecolor='k', alpha=0.3)\n",
    "    axes[i].set_xlim([-18,60])\n",
    "    axes[i].set_ylim([-2,38])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Point-level analysis\n",
    "\n",
    "We've looked at examples of using point data (on the location and timing of locust swarms) to create rasters. But in many cases we are interested in conducting analyses at the level of points - locations of individuals, cities, businesses, etc. \n",
    "\n",
    "There are a huge number of datasets with geolocated information on events, communities, survey locations, etc. It is often very useful to map these to other spatial data for analysis.\n",
    "\n",
    "Let's look at two examples: locations of locust swarm observations from the [FAO Desert Locust Hub](https://locust-hub-hqfao.hub.arcgis.com/) and of survey communities in the Nigeria General Household Survey Panel ([GHSP](https://microdata.worldbank.org/index.php/catalog/5835)). \n",
    "\n",
    "### Plotting point data over a basemap\n",
    "\n",
    "It can be useful to have a reference map under your data. You might want to plot terrain, streets, etc. This can help you better understand your point data. \n",
    "\n",
    "Here we will use the `Basemap` package to map relief/topography under data on the locations and dates of locust swarm observations. You will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swarms\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "m = Basemap(projection='lcc',  \n",
    "            lat_0=20, lon_0=-325,\n",
    "            width=1.2E7, height=5.5E6)\n",
    "m.drawcountries(color='gray')\n",
    "m.shadedrelief()\n",
    "m.scatter(swarms.x.values, swarms.y.values, latlon=True, \n",
    "          alpha=0.4, s=1, c=swarms.yr.values, cmap=plt.get_cmap(\"jet\"))\n",
    "plt.colorbar(label='Year',ticks=np.arange(1990, 2022, 4), orientation=\"horizontal\",pad=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting point data over a raster\n",
    "\n",
    "You may also want to plot your point data over raster data you have downloaded and that you might want to use for analysis. Let's plot GHSP community locations over a map of GAEZ agroecological zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = \"C:/Users/pibiscay/Dropbox/Class-Data Science/Section 6/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghsp = pd.read_stata(home+\"NGA_HouseholdGeovariables_Y1.dta\", convert_categoricals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghsp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaez=rasterio.open(home+\"faocmb_2010.tif\")\n",
    "nga_adm1 = gpd.read_file(home+\"NGA_adm1.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, figsize=(10,10))\n",
    "nga_adm1.plot(ax=ax, color='none', edgecolor='k', label=\"State boundaries\")\n",
    "ax.scatter(ghsp['lon_dd_mod'],ghsp['lat_dd_mod'], marker='+', c='red')\n",
    "im = ax.imshow(gaez.read(1),\n",
    "               cmap='viridis',\n",
    "               extent=(-180, 180, -90, 90))\n",
    "fig.colorbar(im)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlim([2,15])\n",
    "ax.set_ylim([4,14])\n",
    "ax.set_title('GHSP 2010-11 survey locations and 2000 cell cropland share')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial calculations\n",
    "\n",
    "Now that we've brought together data sources, we can use them to conduct calculations.\n",
    "\n",
    "For example, for a given location we might want to match data on weather and geography.\n",
    "\n",
    "Let's do an example of calculating distance to the nearest body of water for community locations in the GHSP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nga_wat_area = gpd.read_file(home+\"NGA_water_areas_dcw.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1, figsize=(10,10))\n",
    "nga_adm1.plot(ax=ax, color='none', edgecolor='k', label=\"State boundaries\")\n",
    "nga_wat_area.plot(ax=ax, color='blue', edgecolor='b', alpha=0.1, label=\"Bodies of water\")\n",
    "ax.scatter(ghsp['lon_dd_mod'],ghsp['lat_dd_mod'], color='r', marker='+',alpha=0.6)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('GHSP 2010-11 survey locations and Nigeria water bodies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ghsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First turn the survey locations into a geopandas gdf\n",
    "from shapely.geometry import Point\n",
    "ghsp_geo=ghsp\n",
    "ghsp_geo['Coordinates'] = list(zip(ghsp.lon_dd_mod, ghsp.lat_dd_mod))\n",
    "ghsp_geo['Coordinates'] = ghsp_geo['Coordinates'].apply(Point)\n",
    "ghsp_geo = gpd.GeoDataFrame(ghsp_geo, geometry='Coordinates')\n",
    "ghsp_geo=ghsp_geo.set_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ghsp_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nga_wat_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nga_wat_area.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nga_wat_area.crs==ghsp_geo.crs\n",
    "# If this were not true, we could use the geopandas to_crs method \n",
    "# to reproject one GeoDataFrame to match the CRS of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nga_wat_area.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reprojecting\n",
    "\n",
    "In a geographic CRS (e.g., EPSG:4326, WGS 84), distances are measured in degrees, which are not consistent or accurate for calculating real-world distances. For example, 1 degree of longitude covers different distances depending on the latitude.\n",
    "\n",
    "We need to pick a projected CRS, where the units are typically in meters, suitable for your region. A common choice is the Universal Transverse Mercator (UTM) projection.\n",
    "Nigeria is covered by multiple UTM zones: EPSG:32631, EPSG:32632, and EPSG:32633. We can use any of these based on the specific location of the data, or we can use a more general projected CRS such as EPSG:3857 (Web Mercator) for simplicity.\n",
    "\n",
    "To resolve this, we need to re-project both GeoDataFrames to a projected CRS before performing the distance calculation. This requires the `pyproj` library - you will need to install it.\n",
    "\n",
    "You can run the following code in your command interface (not the one running this notebook), or use the Anaconda Navigator to install these libraries to your environment.\n",
    "`conda install anaconda::pyproj`\n",
    "`conda install conda-forge::proj`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_crs = \"EPSG:32633\"\n",
    "ghsp_geo = ghsp_geo.to_crs(projected_crs)\n",
    "nga_wat_area = nga_wat_area.to_crs(projected_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `shapely.distance` method to calculate distances. It will calculate pairwise distances, and we will take the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance to the nearest water body for each point in ghsp_geos\n",
    "def calculate_nearest_distance(point, polygons):\n",
    "    \"\"\"\n",
    "    Calculate the distance from a point to the nearest polygon in a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    return polygons.distance(point).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to calculate distances; this can take a while!\n",
    "ghsp_geo['distance_to_water'] = ghsp_geo['Coordinates'].apply(lambda x: calculate_nearest_distance(x, nga_wat_area['geometry']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the resulting summary stats\n",
    "ghsp_geo['distance_to_water'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject back\n",
    "ghsp_geo = ghsp_geo.to_crs(\"EPSG:4326\")\n",
    "nga_wat_area = nga_wat_area.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's map it!\n",
    "fig, ax = plt.subplots(ncols=1, figsize=(10,10))\n",
    "nga_adm1.plot(ax=ax, color='none', edgecolor='k', label=\"State boundaries\")\n",
    "nga_wat_area.plot(ax=ax, color='blue', edgecolor='b', alpha=0.1, label=\"Bodies of water\")\n",
    "im=ax.scatter(ghsp_geo['lon_dd_mod'],ghsp_geo['lat_dd_mod'], \n",
    "              c=ghsp_geo['distance_to_water'], marker='s', cmap='viridis')\n",
    "fig.colorbar(im)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('GHSP 2010-11 survey locations and distance to nearest water')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis at the point level\n",
    "\n",
    "In my paper on the impacts of the 2012 floods in Nigeria, I determine flooding status of each community according to both survey reports and satellite imagery, as processed by the NASA Near-Real Time Flood Mapping Product.\n",
    "\n",
    "The two definitions disagree. I analyze the spatial characteristics of different categories of community to try to identify predictors of differences in flood identification. This was the impetus for the project I discussed earlier comparing flood mapping sources. \n",
    "\n",
    "SHOW THE SLIDES.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
