{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 2. Data Wrangling\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n",
    "\n",
    "The content of this notebook draws on material from UC Berkeley's D-Lab Python Fundamentals [course](https://github.com/dlab-berkeley/Python-Fundamentals).\n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Load .csv files and other tabular data into a Pandas `DataFrame`.\n",
    "* Learn how to select columns and rows in a Pandas `DataFrame`.\n",
    "* Learn how to combine or merge data frames.\n",
    "* Understand how Pandas can be used for exploratory analysis.\n",
    "\n",
    "### Sections\n",
    "1. Data Frames: Spreadsheets in Python\n",
    "2. Selecting columns\n",
    "3. Selecting rows\n",
    "4. Merging data frames\n",
    "5. Vectorization with data frames\n",
    "\n",
    "### Libraries loaded\n",
    "* pandas\n",
    "* numpy\n",
    "* os\n",
    "\n",
    "### Files loaded\n",
    "* gapminder.csv\n",
    "* gapminder.dta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Frames: Spreadsheets in Python\n",
    "\n",
    "**Tabular data** are everywhere. Think of an Excel sheet: each column corresponds to a different feature of each datapoint, while rows correspond to different observations.\n",
    "\n",
    "In scientific programming, tabular data is often called a **data frame**. In Python, the `pandas` package contains an object called `DataFrame` that implements this data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's import the `pandas` module using the alias `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As good practice going forward you will typically have one block of code at the start of your notebook where you import all the packages you expect to need at once. That makes it explicit what a given notebook script depends on. But you can also import packages as you go, and we will often do this in this course so that we can clearly discuss each package as we import it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a data frame\n",
    "\n",
    "You can use pandas to make your own data frame. The code below shows the structure: You specify a set of column/variable names, and for each one specify a list of values for each row for that column. You must ensure the lists are of the same length for each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an example data frame\n",
    "data_frame = pd.DataFrame({\"A\":[1,4,5,7,9],\n",
    "                           \"B\":[4,7,2,5,2], \n",
    "                           \"Height\":[44,55,77,33,22]}) \n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frames are similar to **arrays** we practiced making using numpy. \n",
    "In both cases, they refer to a table or matrix of data organized into columns and rows. \n",
    "What is unique about Pandas data frames is that the columns are labeled with variable names, and the rows are given explicit indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "array=np.array([[1,4,5,7,9],[4,7,2,5,2],[44,55,77,33,22]])\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do mathematical operations on columns of data frames in the same way that you can for an array of numbers. \n",
    "We will see this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Data\n",
    "\n",
    "Usually we will load pre-existing data. For the rest of this notebook we will work with a dataset from [Gapminder](https://en.wikipedia.org/wiki/Gapminder_Foundation). The dataset contains data for 142 countries, with values for life expectancy, GDP per capita, and population, every five years from 1952 to 2007."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using a relative path\n",
    "df = pd.read_csv('Data/gapminder.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often use `df` as a short placeholder name for a dataframe. This is fine if you are only working with one dataframe in a particular notebook, but is problematic if you have several. Remember our best practices are to have more descriptive object names. \n",
    "\n",
    "The `.head()` method will show the first five rows of a Data Frame by default. You can put an integer in between the parentheses to specify a different number of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of tabular data\n",
    "As data scientists, we'll often be working with **Comma Seperated Values (.csv)** files. \n",
    "\n",
    "Comma separated values files are common because they are relatively small and look good in spreadsheet software. A comma separated values file is just a text file that contains data but that has commas (or other separators) to indicate column breaks.\n",
    "\n",
    "As you see, `pandas` comes with a function `read_csv()` that makes it really easy to import .csv files as Data Frames.\n",
    "\n",
    "You can use other Pandas methods to load **other data types**. For example, `read_stata()` loads .dta files. You can research other methods to load other forms of tabuler data. For non-tabular data, other packages are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stata = pd.read_stata(\"Data/gapminder.dta\")\n",
    "df_stata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Working With Columns\n",
    "Now that we have our `DataFrame`, we can select a single column by selecting the name of that column. This uses bracket notation (like we do when accessing lists).\n",
    "\n",
    "Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type of this column is a `Series`. It's like a list. You can index a `Series` object just like you can with a list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_country = df['country'] # This creates a data frame with just the country variable - only one column, effectively a list\n",
    "gap_country[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select multiple columns, you need to pass a list of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['country','continent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access columns using dot notation, but this is not typically recommending as we like to reserve that for calling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Methods on Columns\n",
    "\n",
    "`DataFrame` objects come with their own methods, many of which operate on a single column of the DataFrame.\n",
    "\n",
    "As we have seen, Jupyter Notebooks allow for tab completion, just like many text editors. If you begin typing the name of something (such as a variable) that already exists, you can simply hit **Tab** and Jupyter will autocomplete it for you. If there is more than one possibility, it will show them to you and you can choose from there. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens when you hit `TAB`?\n",
    "df['co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are selecting a column in our `DataFrame`. What happens when you hit `TAB`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out some methods. For example, we can identify the number of unique values in each column by using the `nunique()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, a package provides **documentation** that explains all of its functionalities. Let's have a look at the documentation for a method called `value_counts()` [online](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html). \n",
    "\n",
    "What does `value_counts()` do in the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some methods allow you to perform calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['lifeExp'].min())\n",
    "print(df['lifeExp'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out some additional methods here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods are available for the full dataframe. The `describe` method can be particularly useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all methods are available for the full dataframe!\n",
    "df.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes \n",
    "\n",
    "Packages like Pandas don't only come with methods, but also with so-called **attributes**.\n",
    "\n",
    "Attributes are like variables: they give you more information about the data that you have. As we saw last time, methods are like functions: they allow you to do something with data.\n",
    "\n",
    "For instance, we can easily check the column names of our data frame using the `columns` **attribute**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular attribute is `shape`. What do you think it tells you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Working With Rows\n",
    "\n",
    "What if we wanted to get some rows in our dataset based on some condition? For example, what if we just wanted a select only the rows for which the country is Egypt? Or only rows from a particular year?\n",
    "\n",
    "We can use so-called **value comparison operators** for this. For instance, to identify the rows that include data points from Egypt, we can use `==`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'] == 'Egypt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above Series is called a **Boolean mask**. It is like a list of True/False labels that we can use to filter our Data Frame for a certain condition! \n",
    "\n",
    "Here, we create a subset of our Data Frame with the Boolean mask we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting only the data points from Egypt\n",
    "df[df['country'] == 'Egypt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of this operation is a **new data frame**! We can assign it to a new variable so we can work with this subsetted data frame. Let's do it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new data frame with data from 2002\n",
    "egypt_df = df[df['country'] == 'Egypt']\n",
    "egypt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select rows with multiple conditions. \n",
    "\n",
    "The symbol `&` computes an element-wise *and* operation, `| `for an *or* operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['country'] == 'Egypt') & (df['year'] == 1967)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['country'] == 'Egypt') & ((df['year'] == 1967) | (df['year'] == 1972))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select rows by index number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 3 rows \n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows 10-19\n",
    "df[9:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting both rows and columns\n",
    "\n",
    "To subset both rows and columns you can use `iloc` if you are referring to indices, or `loc` if you are referring to column/variable names.\n",
    "\n",
    "The syntax for `loc` is `df.loc[<row_filter>, <columns>]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting both rows and columns using iloc[]\n",
    "df.iloc[[3,4],[0,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting using loc[]\n",
    "df.loc[df['country'] == 'Egypt', ['pop','year','lifeExp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The direct route (less clean)\n",
    "df[df['country'] == 'Egypt'][['pop','year','lifeExp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice:** Using the gapminder dataset, \n",
    "1. Create a new subset with observations from after 1990 and the country, year, and lifeExp columns.\n",
    "2. Print the first 10 rows of this subset of data. \n",
    "3. Calculate and print the means of lifeExp for countries observed before 1990 and after 1990. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merging/joining data frames\n",
    "\n",
    "Sometimes we will have multiple datasets with information on the same units of observation, and will want to merge or join these together for analysis purposes. \n",
    "\n",
    "To do this, we can use the `pd.merge()` method. \n",
    "\n",
    "The syntax is `pd.merge([dataset1],[dataset2], on=['varname1','varname2',...],how='inner')`, where you replace the [dataset] arguments with the names of the data frames, and the varname arguments with the variables used to identify how to match observations from the two datasets. \n",
    "\n",
    "The `on` argument indicates what columns/variables to use as '*keys*' for the merge.\n",
    "If the column names in the two datasets differ, you can use the left_on and right_on arguments, where you specify different variable/column names for the left (first) and right (second) datasets, in the order in which they should be matched.\n",
    "\n",
    "The `how` argument specifies the type of merge. Common options are:\n",
    "* 'inner': Keeps only rows where keys match in both datasets.\n",
    "* 'outer': Keeps all rows from both datasets, filling missing values with NaN.\n",
    "* 'left': Keeps all rows from the left dataset (df) and fills missing values from df2 with NaN.\n",
    "* 'right': Keeps all rows from the right dataset (df2) and fills missing values from df with NaN.\n",
    "\n",
    "Let's do an example with the gapminder data. First we will split the data into 2 dataframes with a subset of columns. Then we will merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the dataset\n",
    "df1=df[['country','year','pop','continent']]\n",
    "df2=df[['country','year','lifeExp','gdpPercap']]\n",
    "\n",
    "print(df1.head())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge it back together\n",
    "\n",
    "df_merge=pd.merge(df1,df2,on=['country','year'],how='outer')\n",
    "print(df_merge.head())\n",
    "print(df_merge.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you have **missing data**? How you do the merge will determine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2[df2['year']>1960]\n",
    "\n",
    "df_merge2=pd.merge(df1,df2,on=['country','year'],how='inner')\n",
    "df_merge3=pd.merge(df1,df2,on=['country','year'],how='outer')\n",
    "df_merge4=pd.merge(df1,df2,on=['country','year'],how='right')\n",
    "df_merge5=pd.merge(df1,df2,on=['country','year'],how='left')\n",
    "print(df_merge2.shape)\n",
    "print(df_merge3.shape)\n",
    "print(df_merge4.shape)\n",
    "print(df_merge5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to one or one to many merge\n",
    "\n",
    "In some cases your merging units are not one to one, for example if you have continent-level data you want to merge with a country-level dataset. The choice of merging `on` variables determines how this is approached. \n",
    "\n",
    "Let's first make a continent-year level dataset, taking means of values using the `groupby()` method.\n",
    "We will then merge this to the country-year dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the continent-level data frame\n",
    "grouped_df = (df.groupby(['continent', 'year']) # select the grouping variables\n",
    "            [['pop', 'lifeExp', 'gdpPercap']] # select the variables to manipulate\n",
    "            .mean() # choose the method to apply\n",
    "            .reset_index() # renumber the row indices\n",
    "             )\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stata, this would be similar to doing a `collapse (mean) variables, by(groups)`.\n",
    "\n",
    "Let's rename the new variables to avoid confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df2=grouped_df.rename(columns={'pop': 'pop_contmean', \n",
    "                                      'lifeExp' : 'lifeExp_contmean', \n",
    "                                      'gdpPercap' : 'gdpPercap_contmean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge this with the main `df` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cont=pd.merge(df,grouped_df2,on=['continent','year'],how='inner')\n",
    "df_merge_cont.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we hadn't renamed the variables in `grouped_df`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cont=pd.merge(df,grouped_df,on=['continent','year'],how='inner')\n",
    "df_merge_cont.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending data\n",
    "\n",
    "In some cases you have dataframes with the same information/variables but for different observations, and you want to combine them. \n",
    "\n",
    "We can combine these dataframes using the `pd.concat()` method. The arguments are a list of data frames. You must make sure that the columns are the same to avoid any errors. If the dataframes have columns with the same names but different types of data, information will be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[df['year']<1960]\n",
    "df2=df[df['year']>=1960]\n",
    "print(df1.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb=pd.concat([df1,df2], ignore_index=True)\n",
    "print(df_comb.shape)\n",
    "df_comb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ignore_index=True` argument reindexes the resulting data frame.\n",
    "\n",
    "What happens if the two datasets don't have the same variables and you try to append rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[['country','year','pop','continent']]\n",
    "df2=df[['country','year','lifeExp','gdpPercap']]\n",
    "df_comb=pd.concat([df1,df2], ignore_index=True)\n",
    "print(df_comb.shape)\n",
    "df_comb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `pd.concat` to merge datasets with the exact same observations but different columns.\n",
    "This is less flexible than `pd.merge` which allows different observations in the two datasets, and leads to some issues of repeated columns.\n",
    "To *concatenate columns*, you have to specify `axis=1`, to override the `axis=0` default argument.\n",
    "Here we will not specify `ignore_index=True` to keep the existing column headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[['country','year','pop','continent']]\n",
    "df2=df[['country','year','lifeExp','gdpPercap']]\n",
    "df_comb=pd.concat([df1,df2], axis=1)\n",
    "df_comb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the two variables don't have matching observations? Missing values (`NaN`) are filled in automatically with no rows dropped as in an `outer` merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2[df2['year']<1960]\n",
    "df_comb=pd.concat([df1,df2], axis=1)\n",
    "print(df_comb.shape)\n",
    "df_comb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Iteration and vectorization with data frames\n",
    "\n",
    "The strength of using computers is their speed. We can leverage this through repeated computation, also called iteration. In Python, we can do this using **loops**. \n",
    "\n",
    "Reminder: A **[for loop](https://www.w3schools.com/python/python_for_loops.asp)** executes some statements once *for* each value in an iterable (like a list or a string). It says: \"*for* each thing in this group, *do* these operations\".\n",
    "\n",
    "Let's take a look at the syntax of a for loop using the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a variable containing a list with the values to be iterated through\n",
    "lifeExp_list = [28.801, 30.332, 31.997]\n",
    "\n",
    "# Initialize the loop\n",
    "for lifeExp in lifeExp_list:\n",
    "    rounded = round(lifeExp)\n",
    "    print(rounded)\n",
    "\n",
    "# This will only be printed when the loop has ended!\n",
    "print('The loop has ended.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conditionals and Loops\n",
    "\n",
    "Recall that we can use `if`-statements to check if a condition is `True` or `False`. Also recall that `True` and `False` are called **Boolean values**.\n",
    "\n",
    "Conditionals are particularly useful when we're iterating through a list, and want to perform some operation only on specific components of that list that satisfy a certain condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [12, 20.2, 43, 88.88, 97, 100, 105, 110.9167]\n",
    "\n",
    "for number in numbers:\n",
    "    if number > 100:\n",
    "        print(number, 'is greater than 100.')\n",
    "    else:\n",
    "        print(number, 'is less than 100.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating Values With Loops\n",
    "\n",
    "In the above example, we are operating on each value in `numbers`. However, instead of simply printing the results, we often will want to save them somehow. We can do this with an **accumulator variable**.\n",
    "\n",
    "A common strategy in programs is to:\n",
    "1.  Initialize an accumulator variable appropriate to the datatype of the output:\n",
    "    * `int` : `0`\n",
    "    * `str` : `''`\n",
    "    * `list` : `[]`\n",
    "2.  Update the variable with values from a collection through a `for` loop. Typical update operations are:\n",
    "    * `int` : `+`\n",
    "    * `str` : `+`\n",
    "    * `list` : `.append()`\n",
    "    \n",
    "The result of this is a single list, number, or string with a summary value for the entire collection being looped over.\n",
    "\n",
    "We can make a new list with all of the rounded numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_numbers = []\n",
    "\n",
    "for number in numbers: \n",
    "    rounded = round(number)\n",
    "    rounded_numbers.append(rounded)\n",
    "\n",
    "print('Rounded numbers:', rounded_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration: Vectorization\n",
    "\n",
    "Let's have a look at our Gapminder dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we want to multiply GDP per capita (`gdpPercap`) by population (`pop`) in order to get the total GDP of a country. We could do so using a `for` loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdpTotal = []\n",
    "df_length = len(df) # so we can loop over all rows\n",
    "\n",
    "for each in range(df_length): # going through each row one at a time\n",
    "    gdp = df['gdpPercap'][each]\n",
    "    pop = df['pop'][each]\n",
    "    gdpTotal.append(gdp * pop)\n",
    "    \n",
    "gdpTotal[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this operation is convoluted, slow, and not preferred. In Pandas, we will want to use [**vectorized**](https://www.geeksforgeeks.org/vectorized-operations-in-numpy) operations. \n",
    "\n",
    "We can just multiply two columns, and Pandas will know we want to use vector algebra to multiply each row of both columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdpTotal = df['gdpPercap'] * df['pop']\n",
    "gdpTotal[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output to this operation is not a list, but a `Series` â€“ a data type specific to Pandas. It is like a list, but it is **labeled**, following the row labels from the original data. \n",
    "\n",
    "We can add this as a new column to df!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gdpTotal'] = df['gdpPercap'] * df['pop']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized operations like these are really handy, and they replace much of the use of `for`-loops in a context of Pandas and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "*****\n",
    "# Key Points\n",
    "\n",
    "* Data frames allow you to work with tabular data (think Excel in Python).\n",
    "* Use the `pandas` library to work with data frames.\n",
    "* `DataFrame` columns can be indexed using square brackets - e.g. `df[last_name]` indexes a column called \"last_name\" in `df`.\n",
    "* Rows are indexed similarly but using row numbers.\n",
    "* You can use Boolean operators to subset rows.\n",
    "* Data frames can be joined using the `.merge()` and `.concat()` methods.\n",
    "* You can use vectorization to easily manipulate columns in Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (UCA DS Base)",
   "language": "python",
   "name": "ucads_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
