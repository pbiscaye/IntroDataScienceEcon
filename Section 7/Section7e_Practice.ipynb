{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7. Machine Learning Basics Practice\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: More exploratory data analysis\n",
    "\n",
    "Load the `auto-mpg.csv` data. Using this dataset, create the following plots, or examine the following distributions, to better understand your data:\n",
    "\n",
    "1. A histogram of the displacement.\n",
    "2. A histogram of the horsepower.\n",
    "3. A histogram of the weight.\n",
    "4. A histogram of the acceleration.\n",
    "5. What are the unique model years, and their counts?\n",
    "6. What are the unique origin values, and their counts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Mean Absolute Error\n",
    "\n",
    "Another commonly used metric in regression is the **Mean Absolute Error (MAE)**. As the name suggests, this can be calculated by taking the mean of the absolute errors. \n",
    "\n",
    "Follow the steps in the Section 7a notebook to model `mpg` with the training data variables. Using this trained model, calculate the mean absolute error on the training and test data. We've imported the MAE for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "# Your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Feature Engineering\n",
    "\n",
    "You might notice that the `origin` variable in the auto-mpg dataset has only three values. So, it's really a categorical variable, where each sample has one of three origins. But so far we've treated it like a continuous variable. \n",
    "\n",
    "How can we properly treat this variable as categorical? This is a question of preprocessing and **feature engineering**.\n",
    "\n",
    "What we can do is replace the `origin` feature with two binary variables. The first tells us whether origin is equal to 2. The second tells us whether origin is equal to 3. If both are false, that means origin is equal to 1.\n",
    "\n",
    "By fitting a linear regression with these two binary features rather than treating `origin` as continuous, we can get a better sense for how the origin impacts the MPG.\n",
    "\n",
    "Create two new binary features corresponding to origin, and then recreate the training and test data. Then, fit a linear model to the new data. What do you find about the performance and new coefficients?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can create some features to improve the training fit without sacrificing the ability to generalize to a good test fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Nearest Neighbors\n",
    "\n",
    "Modify the nearest neighbors tuning function from class to save the Test RMSE for each value of $K$. Run the function on all integers from 1 to 20. Store the results and plot them. Identify the minimum value and plot it in a different color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5: Benchmarking\n",
    "\n",
    "Re-run the ordinary least squares prediction model on the data using `LinearRegression`. Then, create a new ridge regression where the `alpha` penalty is set equal to zero. How do the performances of these models compare to each other? How do they compare to a ridge regression model with `alpha` set to 5? Be sure to compare both the training performances and test performances. Which one performs better with the training data? Which one generalizes better to the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# Your code\n",
    "# Create models\n",
    "\n",
    "# Fit models\n",
    "\n",
    "# Run predictions\n",
    "\n",
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: Lasso\n",
    "\n",
    "Write a loop to perform a lasso regression over a large range of penalty hyperparameters. For each model, store the count of features with non-0 coefficients and the test $R^2$. Then plot how these values change as the penalty increases. Identify the hyperparameter value at which no features have non-0 coefficients. Identify the hyperparameter value with the best test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 7: Order of Preprocessing\n",
    "\n",
    "In the preprocessing of penguin data we did the following steps: \n",
    "\n",
    "1) Null values for categorical data\n",
    "2) One-hot-encoding\n",
    "3) Imputation for continuous data\n",
    "4) Normalization\n",
    "\n",
    "Now, consider that we change the order of the steps in the following ways. What effect might that have on the algorithms? Try changing the code from notebook 7c and trying it out!\n",
    "\n",
    "- One-Hot-Encoding before Null Values for categorical data\n",
    "- Normalization before Imputation for continuous data\n",
    "\n",
    "**Bonus:** Are there any other switches in order that might affect preprocessing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 8: Decision Tree Classification\n",
    "\n",
    "Follow the steps notebook 7d on classification for estimating a decision tree model to predict penguin species.\n",
    "\n",
    "First, estimate the model adding culmen_depth_mm as a feaure, and set max_depth=2.\n",
    "\n",
    "Then, estimate this same model setting max_depth=3. \n",
    "\n",
    "Then, estimate this same model adding flipper_length_mm as a feature, with max_depth=3.\n",
    "\n",
    "How does the training and test performance of the models change? What do you conclude about the effects of feature selection and depth choice?\n",
    "\n",
    "Visualize the two decision trees to see how adding two layers changed the split decisions. Do you understand the decision protocols?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 9: Decision Tree Pruning\n",
    "\n",
    "We've looked at some approaches to pre-pruning. Now estimate a decision tree to estimate penguin species using the default hyperparameters, and use cost complexity post-pruning to prune the tree. You can look up specific code for how to do this online. \n",
    "\n",
    "After identifying a range of complexity penalty alphas, estimate the decision tree for each possible alpha value. Which alpha value has the best validation performance in terms of accuracy? How does the accuracy compare to what you were finding in Challenge 8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
