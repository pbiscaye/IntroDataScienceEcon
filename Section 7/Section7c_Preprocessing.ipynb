{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebbc7db",
   "metadata": {},
   "source": [
    "# Section 7. Preprocessing\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n",
    "\n",
    "The content of this notebook draws on material from UC Berkeley D-Lab's Python Machine Learning [course](https://github.com/dlab-berkeley/Python-Machine-Learning).\n",
    "\n",
    "Preprocessing is the process of data cleaning and preparation for analysis. This is an essential step for any data work, and no less for the machine learning workflow and the performance of models. This notebook will introduce the major steps of preprocessing for machine learning. \n",
    "\n",
    "\n",
    "### Sections\n",
    "\n",
    "1. Missing data\n",
    "2. Processing categorical data: dummy encoding\n",
    "3. Processing continuous data: outliers and normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4378b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10590fd",
   "metadata": {},
   "source": [
    "For today, we will be working with the `penguins` data set, a common public data set for teaching visualization and exploration. This data set is from [Kaggle](https://www.kaggle.com/parulpandey/penguin-dataset-the-new-iris) and includes data on penguins of three different species, their location, and some measurements for each penguin.\n",
    "\n",
    "First, let's import some packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c244fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300dbcd1",
   "metadata": {},
   "source": [
    "Now, let's load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de206905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/penguins.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260ae68",
   "metadata": {},
   "source": [
    "Below is the information for each of the columns:\n",
    "1. **species**: Species of penguin [Adelie, Chinstrap, Gentoo]\n",
    "2. **island**: Island where the penguin was found [Torgersen, Biscoe]\n",
    "3. **culmen_length_mm**: Length of upper part of penguin's bill (millimeters)\n",
    "4. **culmen_depth_mm**: Height of upper part of bill (millimeters)\n",
    "5. **flipper_length_mm**: Length of penguin flipper (millimeters)\n",
    "6. **body_mass_g**: Body mass of the penguin (grams)\n",
    "7. **sex**: Biological sex of the penguin [MALE, FEMALE]\n",
    "\n",
    "*Question:* Which of the columns are continuous? Which are categorical?\n",
    "\n",
    "We will need to treat the numeric and categorical data differently in preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b61930",
   "metadata": {},
   "source": [
    "## 1. Missing Data Preprocessing\n",
    "\n",
    "First, let's check to see if there are any missing values in the data set. Missing values are represented by `NaN`. \n",
    "\n",
    "*Question:* In this case, what do missing values stand for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30296f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe1c9f",
   "metadata": {},
   "source": [
    "It is also possible to have non `NaN` missing values. For example, let's take a look at the `sex` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97753d4d",
   "metadata": {},
   "source": [
    "In this case, the `.` represents a missing value, so let's replace those with `np.nan` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('.', np.nan, inplace=True)\n",
    "\n",
    "data['sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c975f09",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "In the case of missing values, we have the option to fill in the missing values with the best guess. `sklearn` has a function `SimpleImputer` that has flexible options for how to approach this.\n",
    "\n",
    "There are many strategies that can be used to impute missing data ([see function documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)), some of which we have discussed previously.\n",
    "\n",
    "Here we'll impute any missing values for two selected variables using the average, or mean, of all the data that does exist for those variables -- this is making a best guess for what the values would have been. We'll then save this as a new dataset, rather than overwriting the original data.\n",
    "\n",
    "Let's see how the `SimpleImputer` works on a subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan,\n",
    "                        strategy='mean', \n",
    "                        copy=True)\n",
    "imputed = imputer.fit_transform(data[['body_mass_g','flipper_length_mm']])\n",
    "imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299fa288",
   "metadata": {},
   "source": [
    "Now let's check that the previously null values have been filled in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212cb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imputed[data[data['body_mass_g'].isna()].index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8458bf",
   "metadata": {},
   "source": [
    "### Dropping Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281496e",
   "metadata": {},
   "source": [
    "Another option option is to use `pd.dropna()` to drop `Null` values from the `DataFrame`. This should almost always be used with the `subset` argument which restricts the function to only dropping values that are null in a certain column(s).\n",
    "\n",
    "Here we will actually overwrite the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c465f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['sex'])\n",
    "\n",
    "# Now this line will return an empty dataframe\n",
    "data[data['sex'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70140ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are now 11 fewer rows\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c8e2b",
   "metadata": {},
   "source": [
    "Note that it is not strictly necessary to drop observations with missing data. Any model that includes a variable with missing data will automatically drop the observations with missing data from the variable from the estimation. It is potentially important if you are running analyses on subsets of variables, and want to make sure you always include the same observations.\n",
    "\n",
    "This also implies that if you do not impute values for missing data, those observations will be dropped from any analyses using the variables with the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9b3c7",
   "metadata": {},
   "source": [
    "## 2. Categorical Data Processing\n",
    "\n",
    "As we saw earlier, the `penguins` dataset contains both categorical and continuous features, which will each need to be preprocessed in different ways. First, we want to transform the categorical variables from strings to **indicator variables**. Indicator variables have one column per level, For example, the island variable will change from Biscoe/Dream/Torgersen --> Biscoe (1/0), Dream (1/0), and Torgerson (1/0). For each set of indicator variables, there should be a 1 in exactly one column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2091d2e",
   "metadata": {},
   "source": [
    " Let's make a list of the categorical variable names to be transformed into indicator variables, and save a dataset of just these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eabc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable names that are categorical for use later\n",
    "cat_var_names = ['species','island', 'sex']\n",
    "data_cat = data[cat_var_names]\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a031b03",
   "metadata": {},
   "source": [
    "### Categorical Variable Encoding (One-hot & Dummy)\n",
    "\n",
    "Many machine learning algorithms require that categorical data be encoded numerically in some fashion. There are two main ways to do so:\n",
    "\n",
    "\n",
    "- **One-hot-encoding**, which creates `k` new variables for a single categorical variable with `k` categories (or levels), where each new variable is coded with a `1` for the observations that contain that category, and a `0` for each observation that doesn't. \n",
    "- **Dummy encoding**, which creates `k-1` new variables for a categorical variable with `k` categories\n",
    "\n",
    "However, when using some machine learning algorithms we can run into the so-called [\"Dummy Variable Trap\"](https://www.algosome.com/articles/dummy-variable-trap-regression.html) when using One-Hot-Encoding on multiple categorical variables within the same set of features. This occurs because each set of one-hot-encoded variables can be added together across columns to create a single column of all `1`s, and so are multi-colinear when multiple one-hot-encoded variables exist within a given model. This can lead to misleading results. \n",
    "\n",
    "To resolve this, we can simply add an intercept term to our model (which is all `1`s) and remove the first one-hot-encoded variable for each categorical variables, resulting in `k-1` so-called \"Dummy Variables\". \n",
    "\n",
    "Luckily the `OneHotEncoder` from `sklearn` can perform both one-hot and dummy encoding simply by setting the `drop` parameter (`drop = 'first'` for Dummy Encoding and `drop = None` for One Hot Encoding). \n",
    "\n",
    "**Question:** How many total columns will there be in the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f97566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "dummy_e = OneHotEncoder(categories='auto', drop='first', sparse_output=False) # sparse_output=False for scikit-learn v> 1.2; else sparse=False\n",
    "dummy_e.fit(data_cat);\n",
    "dummy_e.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b861e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dummy_e.transform(data_cat)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5ec48",
   "metadata": {},
   "source": [
    "We can also create encode categorical variables into dummy variables manually by looping through categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in data_cat['sex'].unique():\n",
    "    data_cat[v]=data_cat['sex']==v\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af05b49",
   "metadata": {},
   "source": [
    "That loop was one-hot encoding. What if we want to do dummy encoding? We want $k-1$ new variables. We can tweak the loop to accomplish this by including an if statement and telling the loop to stop before the last unique categorical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ebba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6768df",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for v in data_cat['species'].unique():\n",
    "    if i < data_cat['species'].nunique():\n",
    "        data_cat[v]=data_cat['species']==v\n",
    "        i += 1\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed0336",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Continuous Data Preprocessing\n",
    "\n",
    "For numeric data, we don't need to create indicator variables, but there are many potential considerations for preparing the data for analysis.\n",
    "\n",
    " Let's subset out the continuous variables to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5dce80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_num = data.drop(columns=cat_var_names + ['species'])\n",
    "data_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6473b0",
   "metadata": {},
   "source": [
    "### Checking for outliers\n",
    "\n",
    "One that we discussed previously is checking for **outliers** and deciding how to treat those values.\n",
    "\n",
    "A simple approach to identifying potential outliers is plotting histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab267ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(8,8))\n",
    "\n",
    "data_num['culmen_length_mm'].hist(grid=False, ax=ax[0,0])\n",
    "data_num['culmen_depth_mm'].hist(grid=False, ax=ax[0,1])\n",
    "data_num['flipper_length_mm'].hist(grid=False, ax=ax[1,0])\n",
    "data_num['body_mass_g'].hist(grid=False, ax=ax[1,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df97111d",
   "metadata": {},
   "source": [
    "There are no obvious outliers for any of these variables. If there were, we would need to determine how to *define* an outlier (i.e., values outside some threshold, which may be a percentile of the distribution), and how to *process* the outliers (i.e., set to missing, set to the threshold value, set to the median, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a79f1",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Another processing approach that is often useful for machine learning is normalizing our variables. This converts all variables into the same units and gives them a similar distribution, which helps improve performance of many machine learning models (see [here](https://en.wikipedia.org/wiki/Feature_scaling)).\n",
    "\n",
    "[Normalization](https://en.wikipedia.org/wiki/Normalization_(statistics)) is a transformation that puts data into some known \"normal\" scale. There are many forms of normalization, but perhaps the most useful to machine learning algorithms is called the \"z-score\" also known as the standard score. This approach is also useful in econometrics, particularly when comparing effect sizes of different variables or on different outcomes. \n",
    "\n",
    "To z-score normalize the data, we simply subtract the mean of the data, and divide by the standard deviation. This results in data with a mean of `0` and a standard deviation of `1`.\n",
    "\n",
    "We'll use the `StandardScaler` from `sklearn` to do normalization, but you can also code this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a5801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "norm_e = StandardScaler()\n",
    "norm_e.fit_transform(data_num,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66993e",
   "metadata": {},
   "source": [
    "To check the normalization works, let's look at the mean and standard variation of the resulting columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a9abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean:',norm_e.fit_transform(data_num,).mean(axis=0))\n",
    "print('std:',norm_e.fit_transform(data_num,).std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b3690",
   "metadata": {},
   "source": [
    "## 4. Combine it all together\n",
    "\n",
    "Now let's combine what we've learned to preprocess the entire dataset.\n",
    "\n",
    "First we will reload the data set to start with a clean copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613debef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "data = pd.read_csv('Data/penguins.csv')\n",
    "# ensure all missing values are coded as np.nan\n",
    "data.replace('.', np.nan, inplace=True)\n",
    "# drop observations with missing sex\n",
    "data = data.dropna(subset=['sex'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065feef",
   "metadata": {},
   "source": [
    "We will now split the data into a training and test set. In the next notebook we will be developing a **classification model** to predict penguin species using other characteristics.\n",
    "\n",
    "We will stratify the split by species to ensure the training and test shares are the same within each species.\n",
    "\n",
    "Why do we do split the data before preprocessing? The best practice is to fit preprocessing methods on the training data. This avoids **data leakage** or influence of test data information on training data. For example, we don't want to impute means that include test data or normalize based on the distribution that includes the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split\n",
    "y = data['species']\n",
    "X = data.drop('species', axis =1, inplace=False)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=.25, stratify=y, random_state=28)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ddc194",
   "metadata": {},
   "source": [
    "We want to train our preprocessing protocols on the training data using the `fit_transform` function, then use the `transform` funtion on the test data. This more closely resembles what the workflow would look like if you are bringing in brand new test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d13650",
   "metadata": {},
   "source": [
    "First, we will subset out the categorical and numerical features separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the categorical and numerical variable column indices\n",
    "cat_var = ['island', 'sex']\n",
    "num_var = ['culmen_length_mm', 'culmen_depth_mm',\n",
    "           'flipper_length_mm', 'body_mass_g']\n",
    "# Splice the training array\n",
    "X_train_cat = X_train[cat_var]\n",
    "X_train_num = X_train[num_var]\n",
    "\n",
    "# Splice the test array\n",
    "X_test_cat = X_test[cat_var]\n",
    "X_test_num = X_test[num_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd850b",
   "metadata": {},
   "source": [
    "Now, let's process the categorical data with **Dummy encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ece48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_cat['island'].nunique())\n",
    "print(X_train_cat['sex'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Categorical feature encoding\n",
    "dummy_e = OneHotEncoder(categories='auto', drop='first', sparse_output=False) # sparse_output=False for scikit-learn v> 1.2; else sparse=False\n",
    "X_train_dummy = dummy_e.fit_transform(X_train_cat)\n",
    "X_test_dummy = dummy_e.transform(X_test_cat)\n",
    "\n",
    "# Check the shape\n",
    "X_train_dummy.shape, X_test_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f521eee",
   "metadata": {},
   "source": [
    "Is this the number of variables we expected?\n",
    "\n",
    "Now, let's process the numerical data by imputing any missing values using the mean in the trianing set and normalizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical feature standardization\n",
    "\n",
    "# Impute means for missing observations\n",
    "imputer = SimpleImputer(missing_values=np.nan,\n",
    "                        strategy='mean', \n",
    "                        copy=True)\n",
    "X_train_imp = imputer.fit_transform(X_train_num)\n",
    "X_test_imp = imputer.transform(X_test_num)\n",
    "\n",
    "# Check for missing values\n",
    "np.isnan(X_train_imp).any(), np.isnan(X_test_imp).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7963ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "norm_e = StandardScaler()\n",
    "X_train_norm = norm_e.fit_transform(X_train_imp)\n",
    "X_test_norm = norm_e.transform(X_test_imp)\n",
    "\n",
    "X_train_norm.shape, X_test_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2847d320",
   "metadata": {},
   "source": [
    "Now that we've processed the numerical and categorical data separately, we can put the two arrays back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147055c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train_dummy, X_train_norm))\n",
    "X_test = np.hstack((X_test_dummy, X_test_norm))\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a362a39e",
   "metadata": {},
   "source": [
    "Finally, let's save our results as separate `.csv` files, so we won't have to run the preprocessing again. We will use these files later.\n",
    "\n",
    "First we will make them DataFrames, then add column names, and then save them as .csv files. Note that the order of column names is based on how we stacked the categorical and continuous data sets together, and how the one-hot encoding generated the dummy variables based on the island and sex categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b45ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = ['Dream','Torgersen', 'Male',\n",
    "                   'culmen_length_mm', 'culmen_depth_mm',\n",
    "                   'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "X_test.columns = ['Dream','Torgersen', 'Male',\n",
    "                   'culmen_length_mm', 'culmen_depth_mm',\n",
    "                   'flipper_length_mm', 'body_mass_g']\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train.columns = ['species']\n",
    "\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test.columns = ['species']\n",
    "\n",
    "X_train.to_csv('Data/penguins_X_train.csv')\n",
    "X_test.to_csv('Data/penguins_X_test.csv')\n",
    "y_train.to_csv('Data/penguins_y_train.csv')\n",
    "y_test.to_csv('Data/penguins_y_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8eba52",
   "metadata": {},
   "source": [
    "Although now we will move on to talk about classification, all of the choices we make in the preprocessing pipeline are extremely important to machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
