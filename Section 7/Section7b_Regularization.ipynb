{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "959b049b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 7. Model Regularization\n",
    "\n",
    "#### Instructor: Pierre Biscaye\n",
    "\n",
    "The content of this notebook draws on material from UC Berkeley D-Lab's Python Machine Learning [course](https://github.com/dlab-berkeley/Python-Machine-Learning).\n",
    "\n",
    "In machine learning, the objective in training a predictive model is **generalization**. We want to have a model perform well on the training set, but we need to make sure that the patterns the model learns can actually generalize to data the model hasn't seen before.\n",
    "\n",
    "So, the scenario we want to avoid is that of **overfitting**. This occurs when our model too strongly learns patterns in the training data, and doesn't generalize well. Overfit models tend to exhibit large generalization gaps: large differences in predictive performance between the training and test data.\n",
    "\n",
    "Overfitting can happen for a variety of reasons, the most well known of which is having a model that's too complicated/with too many features. Luckily, all is not lost. There are a variety of approaches we can use to combat overfitting. In general, these approaches are called **regularization**.\n",
    "\n",
    "### Sections\n",
    "    \n",
    "1. Overfitting and regularization\n",
    "2. Ridge regression\n",
    "3. Choosing hyperparameters and cross-validation\n",
    "4. Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de81c9b",
   "metadata": {},
   "source": [
    "## 1. Overfitting and Regularization\n",
    "\n",
    "In the previous lesson, we discussed feature engineering, the process by which we create new features in order to make our model more expressive. One tradeoff to adding features to the model is that the model becomes more complex, which makes it prone to overfitting. \n",
    "\n",
    "For example, consider a basic regression with the points shown below:\n",
    "\n",
    "![overfitting](Images/overfitting.png)\n",
    "\n",
    "We could fit a simple line to this data, which will exhibit some error. However, we could also fit a more complex model - say, a polynomial - which could perfectly fit to the training data. There will be no error in the training predictions, which seems great!\n",
    "\n",
    "But do we *really* think the polynomial is making good predictions on *all* possible data points? Look at how it behaves in between the training examples. It's very likely on *new* data - that is, when the model needs to generalize - the linear model will perform much better than the polynomial model. This is because the polynomial model overfit to the training data.\n",
    "\n",
    "So, it's common in machine learning to follow a **\"parsimony principle\"**. Specifically, we aim to choose simpler models that can still be predictive, because simpler models are less likely to overfit, and thus generalize decently well.\n",
    "\n",
    "Regularization is often though of in terms of the **bias-variance tradeoff**. Specifically, prediction errors often break down in terms of two components: bias and variance. The linear model exhibits higher bias, since it exhibits large errors on the training example. But the polynomial model has higher variance - it's more likely to give wildly different predictions for training samples close together.\n",
    "\n",
    "We don't always have to use linear regression in the spirit of opting for simpler models. Sometimes, it's good to use the complicated model, particularly if it makes sense in a specific context. This is where **regularization** is useful: a set of techniques and principles we can use to make a model less prone to overfitting during training. It's important to note that regularization is more of a concept than it is a specific, standardized technique. There are many approaches used for regularizing. \n",
    "\n",
    "Today, we're going to cover the usage of **penalty terms** to regularize linear models.\n",
    "\n",
    "Before we get started, let's warm up by importing our data and performing a train test split. We've providing the importing code for you. Go ahead and split the data into train/test sets using an 80/20 split, and a random state of 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d195f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = pd.read_csv('Data/auto-mpg.csv')\n",
    "# Remove the response variable and car name\n",
    "X = data.drop(columns=['car name', 'mpg'])\n",
    "# Assign response variable to its own variable\n",
    "y = data['mpg'].astype(np.float64)\n",
    "# Split the sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bfaa2",
   "metadata": {},
   "source": [
    "## 2. Ridge Regression\n",
    "\n",
    "Recall the formulation of a linear model. We have the parameters we are trying to estimate, given in the model:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_P X_P$$\n",
    "\n",
    "In OLS, we estimate the parameters by minimizing the following objective function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{MSE} = L(\\beta) &= \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2 \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i - \\beta_0 - \\sum_{j=1}^P \\beta_j X_j\\right)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We're going to regularize this model. We're not going to change the actual linear model - that's the top equation - but we will change how we choose the $\\beta$ parameters. Specifically, we're going to do **ridge regression** (also called $\\ell_2$ regularization and Tikhonov regularization). Instead of using the least squares objective function, specified in the second equation, we're going to use the following objective function: \n",
    "\n",
    "$$ L(\\beta) = \\sum_{i=1}^N (y_i - \\hat y_i)^2  + \\alpha \\sum_{j=1}^P \\beta_j^2 $$ \n",
    "\n",
    "What's the difference? There's a second term added on, which is equal to the sum of the squares of the $\\beta$ values. What does this mean?\n",
    "\n",
    "Our goal is for the loss, $L(\\beta)$, to be as small as possible. The first term says we can make that small if we make our errors, $y_i - \\hat y_i$, small. The second term says that we increase the loss if the $\\beta$ values get too large. Effectively, we are adding a *penalty* for including additional strongly correlated predictors to the model.\n",
    "\n",
    "There's a tradeoff here: if we make the $\\beta$ values all zero to accomodate the second term, then the first term will be large. So, in ridge regression, we try and minimize the errors, while trying hard not to make the coefficients too big. In practice, that means driving toward 0 the coefficients on features with little explanatory power. \n",
    "\n",
    "Also, note that ridge regression requires a **hyperparameter**, called $\\alpha$ (sometimes $\\lambda$). This hyperparameter indicates how much regularization should be done. In other words, how much do we care about the coefficient penalty term vs. how much do we care about the sum of squared errors term? The higher the value of $\\alpha$, the more regularization, and the smaller the resulting coefficients will be. On the other hand, if we use an $\\alpha$ value of 0, we get the same solution as the OLS regression done above.\n",
    "\n",
    "Why does ridge regression serve as a good regularizer? The penalty actually does several things, which are beneficial for our model:\n",
    "1. **Multicollinearity:** Ridge regression was devised largely to combat multicollinearity, or when features are highly correlated with each other. Ordinary least squares struggles in these scenarios, because multicollinearity can cause a huge increase in variance: it makes the parameter estimates unstable. Adding the penalty term stabilizes the parameter estimates, at a little cost to bias. This results in better generalization performance.\n",
    "2. **Low Number of Samples:** The most common scenario where you might overfit is when you have many features, but not many samples. Adding the penalty term stabilizes the model in these scenarios. There's not a great intuition for this without diving into the math, so you can just take it at face value. \n",
    "3. **Shrinkage:** The $\\ell_2$ penalty results in shrinkage, or a small reduction in the size of the parameters. This is effectively a bias, but helps regularize by reducing variance that often comes with overfit models.\n",
    "\n",
    "**Question**: What are the tradeoffs in setting a higher or lower alpha value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f6a97",
   "metadata": {},
   "source": [
    "### Ridge Regression in Practice\n",
    "\n",
    "As with linear regression, `scikit-learn` makes it easy to fit a ridge regression. We simply use the `Ridge` class from `scikit-learn`. This time, however, we're going to specify some arguments when we create the ridge regression object. The most important one is the regularization penalty, $\\alpha$, which we need to choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2741a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "# Create models\n",
    "ridge = Ridge(\n",
    "    # Regularization penalty\n",
    "    alpha=10,\n",
    "    random_state=1)\n",
    "# Fit object\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948862e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "y_train_pred_ridge = ridge.predict(X_train)\n",
    "y_test_pred_ridge = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ac73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(f'Training R^2: {ridge.score(X_train, y_train)}')\n",
    "print(f'Test R^2: {ridge.score(X_test, y_test)}')\n",
    "print(f'Train RMSE: {mean_squared_error(y_train, y_train_pred_ridge) ** (1/2)}')\n",
    "print(f'Test RMSE: {mean_squared_error(y_test, y_test_pred_ridge) ** (1/2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bfa7af",
   "metadata": {},
   "source": [
    "What if we used a different sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change random state to 27\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=27)\n",
    "ridge2 = Ridge(\n",
    "    # Regularization penalty\n",
    "    alpha=10,\n",
    "    random_state=1)\n",
    "# Fit object\n",
    "ridge2.fit(X_train2, y_train2)\n",
    "# Evaluate\n",
    "print(f'Training R^2: {ridge2.score(X_train2, y_train2)}')\n",
    "print(f'Test R^2: {ridge2.score(X_test2, y_test2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95441bd9",
   "metadata": {},
   "source": [
    "Results can depend on samples chosen. We'll discuss how to deal with this when we discuss validation.\n",
    "\n",
    "How does this performance compare to the OLS model we estimated previously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4449776",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training R^2: 0.8134')\n",
    "print(f'Test R^2: 0.8437')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8f358",
   "metadata": {},
   "source": [
    "In the practice coding problems, you will run some experiments with different ridge regression specifications and see how it affects the results. \n",
    "\n",
    "In general (though not always), we find that ridge regression typically results in worse training performance, but slightly better generalization performance than OLS. So the regularization can help, particularly in this case where we know the parameters are correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ca727",
   "metadata": {},
   "source": [
    "## 3. Choosing Hyperparameters: Validation Sets\n",
    "\n",
    "The current issue with our analysis thus far is that we don't know what $\\alpha$ value we should use. Since hyperparameters are chosen *before* we fit the model, we can't just choose them based off the training data. So, how should we go about conducting **hyperparameter search**: identifying the best hyperparameter(s) to use?\n",
    "\n",
    "Let's think back to our original goal. We want a model that generalizes to unseen data. So, ideally, the choice of the hyperparameter should be such that the performance on unseen data is the best. We can't use the test set for this, but what if we had another set of held-out data? \n",
    "\n",
    "This is the basis for a **validation set**. If we had hold out a portion of the training data and use this for validation, we could try a bunch of hyperparameters on the training set, and see which one results in a model that performs the best on the validation set. We then would choose that hyperparameter, and use it to refit the model on both the training data and validation data. We could then, finally, evaluate on the test set.\n",
    "\n",
    "![validation](Images/validation.png)\n",
    "\n",
    "So, you'll often see a dataset not only split up into training/test sets, but training/validation/test sets, particularly when you need to choose a hyperparameter.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "We just formulated the process of choosing a hyperparameter with a single validation set. However, there are many ways to perform validation. The most common way is **cross-validation**. Cross-validation is motivated by the concern that we may not choose the best hyperparameter if we're only validating on a small fraction of the data. If the validation sample, just by chance, contains specific data samples, we may bias our model in favor of those samples, and limit its generalizability.\n",
    "\n",
    "So, during cross-validation, we effectively validate on the *entire* dataset, by breaking it up into **folds**. Here's the process:\n",
    "\n",
    "1. Perform a train/test split, as you normally would.\n",
    "2. Choose a number of folds - the most common is $K=5$ - and split up your training data into those equally sized \"folds\".\n",
    "3. For *each* hyperparameter, we're going to fit $K$ models. Let's assume $K=5$. The first model will be fit on Folds 2-5, and validated on Fold 1. The second model will be fit on Folds 1, 3-5, and validated on Fold 2. This process continues for all 5 splits.\n",
    "4. Each hyperparameter's performance is summarized by the average predictive performance on all 5 held-out folds. We then choose the hyperparameter that had the best average performance.\n",
    "5. We can then refit a new model to the entire training set, using our chosen hyperparameter. That's our final model - evaluate it on the test set!\n",
    "\n",
    "![cross-validation](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c948ee73",
   "metadata": {},
   "source": [
    "### Cross-Validation in Practice\n",
    "\n",
    "You guessed it: `scikit-learn` makes it really easy to fit a model with cross-validation. We'll use the `RidgeCV` class. Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) for details about it.\n",
    "\n",
    "`RidgeCV` is going to need to know a few things from us: which hyperparameters do we want? How many folds should we use? We'll specify these when creating the model object.\n",
    "\n",
    "We'll use `np.logspace(start, stop, num, base=10)`, which generates an array of `num` numbers from `base ** start` to `base ** end` evenly spaced on a log scale for a given base. This creates a large number of hyperparameters to search over over a relatively broad range, but with a greater number of candidate parameters at the lower end of the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.logspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-1, 3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "# Create ridge model, with CV\n",
    "ridge_cv = RidgeCV(\n",
    "    # Which alpha values to test for?\n",
    "    alphas=np.logspace(-1, 3, 100),\n",
    "    # Number of folds\n",
    "    cv=5)\n",
    "# Fit model\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "# Evaluate model\n",
    "print(ridge_cv.score(X_train, y_train))\n",
    "print(ridge_cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b4a539",
   "metadata": {},
   "source": [
    "We can also access the best $\\alpha$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceabf7c",
   "metadata": {},
   "source": [
    "As well as the coefficients on the included features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812098c8",
   "metadata": {},
   "source": [
    "Not surprisingly because the optimal $\\alpha$ is similar to the value of 10 we selected earlier, model performance is similar across the baseline ridge regression and the cross-validated model, with only small improvements with the optimal hyperparameter. \n",
    "\n",
    "Test it out with much larger or smaller alpha values to see how the performance would have been different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training R^2: {ridge.score(X_train, y_train)}')\n",
    "print(f'Test R^2: {ridge.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42509cb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Lasso Regression\n",
    "\n",
    "**Lasso regression** (also called $\\ell_1$ regularization) is another form of regularized regression that penalizes the coefficients. Rather than taking a squared penalty of the coefficients as in ridge regression, Lasso uses an absolute value penalty: \n",
    "\n",
    "$$ L(\\beta) = \\sum_{i=1}^N (y_i - \\hat y_i)^2  + \\alpha \\sum_{j=1}^P |\\beta_j| $$ \n",
    "\n",
    "This has a similar effect on making the coefficients smaller, but also has a tendency to force some coefficients to be set *exactly equal to 0*. This leads to what is called **sparser** models, and is another way to reduce overfitting introduced by more complex models.\n",
    "\n",
    "Setting some coefficients exactly equal to zero has the added benefit of performing **feature selection**: it can exactly identify if some features are not worth including in the model, because their coefficients are set exactly to 0 (meaning that their values would have no impact on prediction). \n",
    "\n",
    "This approach is often used in econometrics to create a rationale for the choice of what covariates to include in a regression model. It helps to discipline the choice of covariates, and avoid concerns about selection of specific covariates to generate desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae36b1",
   "metadata": {},
   "source": [
    "### Performing a Lasso Fit\n",
    "\n",
    "Below, we've imported the `Lasso` object from `scikit-learn`. Just like `Ridge`, it needs to know what the strength of the regularization penalty is before fitting to the data. \n",
    "\n",
    "Let's fit several Lasso models, with different regularization strengths, from close to zero to very large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d06b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso1 = Lasso(alpha=0.01)\n",
    "lasso1.fit(X_train, y_train)\n",
    "print(lasso1.coef_)\n",
    "print(lasso1.score(X_train, y_train))\n",
    "print(lasso1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ed530",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso2 = Lasso(alpha=10)\n",
    "lasso2.fit(X_train, y_train)\n",
    "print(lasso2.coef_)\n",
    "print(lasso2.score(X_train, y_train))\n",
    "print(lasso2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e39de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso3 = Lasso(alpha=10000)\n",
    "lasso3.fit(X_train, y_train)\n",
    "print(lasso3.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8805f16",
   "metadata": {},
   "source": [
    "What do we observe about the coefficients?\n",
    "\n",
    "You can try out more variations on the lasso in the practice notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
